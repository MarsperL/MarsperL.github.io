[{"title":"Android手机安装docker","url":"/yyg/46758d12/","content":"## Android手机使用ALPINE-TERM 安装docker\n\nAlpine Term 是一款基于 **Termux** 的安卓应用，集成了 **QEMU 虚拟机** 和 **Alpine Linux**，支持在安卓手机上免 Root 运行完整的 Linux 环境，并可直接使用 Docker 等容器技术。\n\n## 环境要求\n\n手机系统环境必须满足：\n\n- AArch64架构（一般都是） \n- 安卓 7.0+ \n- 内部存储上至少有 500 MB 的空间。\n- 连接网络\n\n## 安装Alpine Term \n\n（1）从 GitHub 下载ALPINE-TERM APK（606MB）。\n\n下载链接：https://github.com/FakeRajbhx/alpine-term/releases/download/New/alpine-term-v16.0-release.apk\n\n（2）在手机上安装完成后，进入app，等待系统加载完成。\n\n（3）输入默认用户alpine和密码alpine，登录系统。\n\n（4）切换至root账号，root用户默认密码为alpine。可输入`passwd`进行修改。\n\n```shell\n#切换至root命令行\nsudo -s\n```\n\n（4）修改sshd服务的配置文件，开启SSH远程连接\n\n```shell\nvi /etc/ssh/sshd_config\n#允许root用户连接登录\nPermitRootLogin yes\n#开启密码登录\nPasswordAuthentication yes\n\n#重启sshd服务\nservice sshd restart\n```\n\n（5）在ALPINE-TERM终端界面的上方中，右划，调出终端菜单，选择`[1] QEMU`,输入以下命令，点击回车，开启ssh远程连接端口。\n\n```shell\n#映射22端口到手机的8034端口\nhostfwd_add tcp::8034-:22\n```\n\n![img](Android手机安装docker/1.png)\n\n> 每次启动ALPINE-TERM时都需要输入该命令才可进行远程连接\n\n（6）修改软件仓库源\n\n```shell\n#注销原有软件仓库源\nmv /etc/apk/repositories /etc/apk/repositories.bak\n#使用清华源\necho \"\nhttps://mirrors.tuna.tsinghua.edu.cn/alpine/v3.13/main\nhttps://mirrors.tuna.tsinghua.edu.cn/alpine/v3.13/community\n\" > /etc/apk/repositories \n\n```\n\n（7）修改DNS\n\n```shell\nsed -i 's/nameserver 94.16.114.254/nameserver 114.114.114.114/g' /etc/resolv.conf \nsed -i 's/nameserver 185.120.22.15/nameserver 223.5.5.5/g' /etc/resolv.conf\n```\n\n## 安装docker\n\n（1）更新软件包\n\n```shell\napk update  && apk upgrade --force-broken-world\n```\n\n（2）系统自带有docker，但没有docker compose\n\n```\n#设置开机自启\nrc-update add docker boot\n#启动docker\nservice docker start\n#查看版本\ndocker version\n```\n\n[可选] 更换软件仓库源，卸载系统自带docker，安装新版docker和docker compose。\n\n```shell\necho \"\nhttps://mirrors.tuna.tsinghua.edu.cn/alpine/latest-stable/main\nhttps://mirrors.tuna.tsinghua.edu.cn/alpine/latest-stable/community\n\" > /etc/apk/repositories \n#更新\napk update \n\n#卸载docker\napk del docker --force-broken-world\n#安装新版docker、docker compose\napk add docker docker-compose  docker-cli-compose --force-broken-world\n```\n\n（3）配置国内镜像源，修改docker默认存储目录\n\n\n```shell\necho '{\n  \"data-root\": \"/data/dockerData\",\n  \"registry-mirrors\": [\"https://docker.1ms.run\"]\n}' > /etc/docker/daemon.json\n```\n\n（4）启动docker，并设置开机自启\n\n```shell\n#重启docker\nservice docker restart\n#设置开机自启\nrc-update add docker boot\n#查看版本\ndocker version\ndocker compose version\n```\n\n## 测试访问\n\n（1）创建nginx容器\n\n```shell\n#拉取nginx镜像并创建容器，映射主机的8080端口\ndocker run -itd --name=nginx -p 8080:80 nginx\n#查看容器运行\ndocker ps\n```\n\n如果容器启动时出现类似报错，重启alpine主机即可\n\n> docker: Error response from daemon: failed to create endpoint nginx on network bridge: failed to add the host (veth3f2f206) <=> sandbox (veth9a8264a) pair interfaces: operation not supported.\n\n（2）在QEMU 中至配置相应端口\n\n```shell\n#将主机的8080端口映射到手机的8081端口\nhostfwd_add tcp::8081-:8080\n```\n\n> 类似 SSH 映射，需要映射容器端口时，也要在QEMU 中执行：\n>\n> hostfwd_add tcp::手机端口-:主机端口 \n\n（3）在浏览器中输入ip+端口访问\n\n![img](Android手机安装docker/2.png)\n\n","tags":["玩机","Android","Android手机安装docker","ALPINE-TERM"],"categories":["漫谈","玩机"]},{"title":"Android手机安装ubuntu","url":"/yyg/c86be486/","content":"## 前言\n\n在闲置的Android旧手机（无需root）安装ubuntu\n\n## 准备\n\n下载termux app，对应链接https://github.com/termux/termux-app/releases\n\n换成国内镜像源\n\n```\ntermux-change-repo\n```\n\n下载软件\n\n```\npkg install vim\n```\n\n创建目录，设置存储权限\n\n在手机的根目录创建目录a，再进入a目录创建file目录\n\n```shell\ntermux-setup-storage\n#设置软链接\nln -s storage/shared/a/file file\n```\n\n## 安装ubuntu\n\n### 下载 proot-distro\n\n```shell\npkg install proot-distro\n```\n\n### 下载ubuntu\n\n有两种方式可下载ubuntu\n\n方式一：直接下载\n\n```\n#查看要安装的系统\nproot-distro list\n#下载安装ubuntu\nproot-distro install ubuntu\n```\n\n方式二：压缩包安装\n\n直接下载太慢的话，可以在github上先把镜像压缩包下载下来，具体链接：https://github.com/termux/proot-distro/releases/download/v4.18.0/ubuntu-noble-aarch64-pd-v4.18.0.tar.xz\n\n将下载的压缩包文件放到手机的`a/file`目录，再移动到`$PREFIX/var/lib/proot-distro/dlcache`目录下，再执行`proot-distro install ubuntu`。具体命令：\n\n```shell\n#移动压缩包\nmv ~/file/ubuntu-noble-aarch64-pd-v4.18.0.tar.xz $PREFIX/var/lib/proot-distro/dlcache\n#执行下载命令\nproot-distro install ubuntu\n```\n\n安装完成后，登录ubuntu\n\n```shell\nproot-distro login ubuntu\n```\n\n### 换国内镜像源\n\n更新现有源\n\n```\napt update -y\n```\n\n下载ca-certificates\n\n```\napt install ca-certificates -y\n```\n\n从 Ubuntu 24.04 开始，Ubuntu 的软件源配置文件变更为 DEB822 格式\n\n```shell\n#将One-Line-Style配置文件注释\nmv /etc/apt/sources.list /etc/apt/sources.list.bak\n#创建配置文件，使用DEB822格式\nvim /etc/apt/sources.list.d/ubuntu.sources\n\nTypes: deb\nURIs: https://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports\nSuites: noble noble-updates noble-backports\nComponents: main restricted universe multiverse\nSigned-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\n# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释\n# Types: deb-src\n# URIs: https://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports\n# Suites: noble noble-updates noble-backports\n# Components: main restricted universe multiverse\n# Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\n# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换\nTypes: deb\nURIs: http://ports.ubuntu.com/ubuntu-ports/\nSuites: noble-security\nComponents: main restricted universe multiverse\nSigned-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\n# Types: deb-src\n# URIs: http://ports.ubuntu.com/ubuntu-ports/\n# Suites: noble-security\n# Components: main restricted universe multiverse\n# Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\n# 预发布软件源，不建议启用\n\n# Types: deb\n# URIs: https://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports\n# Suites: noble-proposed\n# Components: main restricted universe multiverse\n# Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\n# # Types: deb-src\n# # URIs: https://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports\n# # Suites: noble-proposed\n# # Components: main restricted universe multiverse\n# # Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\n# 更新\napt update -y && apt upgrade -y\n```\n\n### 远程连接\n\n```shell\napt install openssh-server -y\n#修改/etc/ssh/sshd_config，取消以下内容的注释并修改\nPort 8253 #使用数字较小的端口不会生效\nPermitRootLogin yes\nPasswordAuthentication yes\n\nservice ssh start \nservice ssh status \n#设置开机自启\nupdate-rc.d ssh enable\n```\n\n若ssh开机自启不生效在`~/.bashrc`文件中追加`service ssh start`。\n\n```shell\necho \"service ssh start\" >> ~/.bashrc\n```\n","tags":["玩机","Android","ubuntu","termux"],"categories":["漫谈","玩机"]},{"title":"Kubernetes知识梳理-核心组件","url":"/yyg/dc35290c/","content":"## pod\n\n在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分：\n\n1. `apiVersion <string>` 版本，由kubernetes内部定义，版本号必须可以用kubectl api-versions查询到\n2. `kind <string>`类型，由kubernetes内部定义，版本号必须可以用kubectl api-resources查询到\n3. `metadata <object>`元数据，主要是资源标识和说明，常用的有name、namespace、labelss等\n4. `spec <object>`描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述\n5. `status <object>`状态信息，里面的内容不需要定义，由kubernetes自动生成\n\n### Pod生命周期\n\n将Pod对象从创建到结束的时间范围称为Pod的生命周期。其生命周期的主要过程如下：\n\n1. pod创建\n2. 运行初始化容器\n3. 运行主容器容器\n   （1）启动钩子、终止钩子\n   （2）存活性探测、就绪性探测\n4. pod终止\n\n![image](Kubernetes知识梳理-核心组件/0.png)\n\n在整个生命周期中，P0d会出现5种状态，分别如下：\n\n1. 挂起(Pending)：apiserveri已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中\n2. 运行中(Running)：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成\n3. 成功(Succeeded)：pod中的所有容器都已经成功终止并且不会被重启\n4. 失败(Failed)：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态\n5. 未知(Unknown)：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致\n\n#### pod的创建过程\n\n通过kubectl将pod配置传输给apiserver，apiserver转化pod信息并存入etcd，再进行”握手“反馈。scheduler监听apiserver中pod信息的变化，使用算法为pod分配主机并更新apiserver的信息，对应node节点主机通过监听更新后的信息，创建容器并更新信息至apiserver，apiserver将最终信息存入etcd，至此pod创建完成。\n\n> 1. 用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer\n> 2. apiServer开始生成pod对象的信息，并将信息存入etcd,然后返回确认信息至客户端\n> 3. apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动\n> 4. scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer\n> 5. node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer\n> 6. apiServer将接收到的pod状态信息存入etcd中\n\n![image](Kubernetes知识梳理-核心组件/1.png)\n\n#### pod的终止过程\n\n用户发送删除pod命令，apiserver接受并更新信息，pod状态变为terminating。kubelet监听收到后，启动pod关闭指令，端点控制器监听到pod关闭指令，删除对应service资源，pod停止运行，kubelet请求apiServer将pod资源的宽限期设置为0从而完成删除操作，apiserver将最终信息存入etcd，至此pod删除完成。\n\n> 1. 用户向apiServer发送删除pod对象的命令\n> 2. apiServer中的pod对象信息会随着时间的推移而更新，在宽限期内(默认30s),pod被视为dead\n> 3. 将pod标记为terminating状态\n> 4. kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程\n> 5. 端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除\n> 6. 如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行\n> 7. pod对象中的容器进程收到停止信号\n> 8. 宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号\n> 9. kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见\n\n#### 初试化容器\n\n初始化容器是在Pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n1、初始化容器必须运行完成直至结束，如果某个初始化容器运行失败，那么kubernetes需要重启它直至成功完成。\n2、初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行。\n初始化容器有很多的应用场景，下面列出的是最常见的几个：\n1、提供主容器镜像中不具备的工具程序或自定义代码。\n2、初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足。\n接下来做一个案例，模拟下面这个需求：\n假设要以主容器来运行Nginx，但是要求在运行Nginx之前要能够连接上MySQL和Redis所在的服务器。\n为了简化测试，事先规定好MySQL和Redis所在的IP地址分别为192.168.18.103和192.168.18.104（注意，这两个IP都不能ping通，因为环境中没有这两个IP）。\n创建pod-initcontainer.yaml文件，内容如下：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\n  labels:\n    user: xudaxian\nspec:\n  containers: # 容器配置\n    - name: nginx\n      image: nginx:1.17.1\n      imagePullPolicy: IfNotPresent\n      ports:\n        - name: nginx-port\n          containerPort: 80\n          protocol: TCP\n      resources:\n        limits:\n          cpu: \"2\"\n          memory: \"10Gi\"\n        requests:\n          cpu: \"1\"\n          memory: \"10Mi\"\n  initContainers: # 初始化容器配置\n    - name: test-mysql\n      image: busybox:1.30\n      command: [\"sh\",\"-c\",\"until ping 192.168.18.103 -c 1;do echo waiting for mysql ...;sleep 2;done;\"]\n      securityContext:\n        privileged: true # 使用特权模式运行容器\n    - name: test-redis\n      image: busybox:1.30\n      command: [\"sh\",\"-c\",\"until ping 192.168.18.104 -c 1;do echo waiting for redis ...;sleep 2;done;\"]\n```\n\n执行命令后，test-mysql未创建成功，之后的容器也无法创建。修改ip为可访问ip后，重新执行命令，会按顺序创建成功\n\n#### 钩子函数\n\nkubernetes在主容器的启动之后和停止之前提供了两个钩子函数：\n\n- post start：容器创建之后执行，如果失败了会重启容器\n\n- pre stop：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作\n\n钩子处理器支持使用下面的三种方式定义动作：\n\n- exec命令：在容器内执行一次命令。\n\n```yaml\n  .......\n    lifecycle:\n       postStart: \n          exec:\n             command:\n               - cat\n               - /tmp/healthy\n  .......\n```\n\n- tcpSocket：在当前容器尝试访问指定的socket。\n\n```yaml\n  .......\n     lifecycle:\n        postStart:\n           tcpSocket:\n              port: 8080\n  .......\n```\n\n- httpGet：在当前容器中向某url发起HTTP请求。\n\n```yaml\n  ....... \n     lifecycle:\n        postStart:\n           httpGet:\n              path: / #URI地址\n              port: 80 #端口号\n              host: 192.168.109.100 #主机地址  \n              scheme: HTTP #支持的协议，http或者https\n  .......\n```\n\n#### 容器探测\n\n容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例“摘除”，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是：\n\n- liveness probes：存活性探测，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器。\n\n- readiness probes：就绪性探测，用于检测应用实例是否可以接受请求，如果不能，k8s不会转发流量。\n\n> livenessProbe：存活性探测，决定是否重启容器。\n> readinessProbe：就绪性探测，决定是否将请求转发给容器。\n\n> k8s在1.16版本之后新增了startupProbe探针，用于判断容器内应用程序是否已经启动。如果配置了startupProbe探针，就会先禁止其他的探针，直到startupProbe探针成功为止，一旦成功将不再进行探测。\n\n上面两种探针目前均支持三种探测方式：\n\n- exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常。\n  \n```yaml\n  ……\n    livenessProbe:\n       exec:\n          command:\n            -    cat\n            -    /tmp/healthy\n  ……\n```\n\n- tcpSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常。\n  \n```yaml\n  ……\n     livenessProbe:\n        tcpSocket:\n           port: 8080\n  ……\n```\n\n- httpGet：调用容器内web应用的URL，如果返回的状态码在200和399之前，则认为程序正常，否则不正常。\n\n```yaml\n……\n   livenessProbe:\n      httpGet:\n         path: / #URI地址\n         port: 80 #端口号\n         host: 127.0.0.1 #主机地址\n         scheme: HTTP #支持的协议，http或者https\n……\n```\n\n#### 重启策略\n\n在容器探测中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由Pod的重启策略决定的，Pod的重启策略有3种，分别如下：\n\n- Always：容器失效时，自动重启该容器，默认值。\n- OnFailure：容器终止运行且退出码不为0时重启。\n- Never：不论状态如何，都不重启该容器。\n\n重启策略适用于Pod对象中的所有容器，首次需要重启的容器，将在其需要的时候立即进行重启，随后再次重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大的延迟时长。\n\n### Pod的调度\n\n在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式。\n\n- 自动调度：运行在哪个Node节点上完全由Scheduler经过一系列的算法计算得出。\n- 定向调度：NodeName、NodeSelector。\n- 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity。\n- 污点（容忍）调度：Taints、Toleration。\n\n#### 定向调度\n\n定向调度，指的是利用在Pod上声明的`nodeName`或`nodeSelector`，以此将Pod调度到期望的Node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过Pod运行失败而已。\n\n##### nodeName\n\nnodeName用于强制约束将Pod调度到指定的name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。\n创建一个pod-nodename.yaml文件，内容如下：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\n  labels:\n    user: xudaxian\nspec:\n  containers: # 容器配置\n    - name: nginx\n      image: nginx:1.17.1\n      imagePullPolicy: IfNotPresent\n      ports:\n        - name: nginx-port\n          containerPort: 80\n          protocol: TCP\n  nodeName: k8s-node1 # 指定调度到k8s-node1节点上\n```\n\n##### nodeSelector\n\nnodeSelector用于将Pod调度到添加了指定标签的Node节点上，它是通过kubernetes的label-selector机制实现的，换言之，在Pod创建之前，会由Scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将Pod调度到目标节点，该匹配规则是强制约束。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers: # 容器配置\n    - name: nginx\n      image: nginx:1.17.1\n      imagePullPolicy: IfNotPresent\n      ports:\n        - name: nginx-port\n          containerPort: 80\n          protocol: TCP\n  nodeSelector:\n    nodeenv: pro # 指定调度到具有nodeenv=pro的Node节点上\n```\n\n#### 亲和性调度\n\n虽然定向调度的两种方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用的Node列表也不行，这就限制了它的使用场景。\n基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在nodeSelector的基础之上进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使得调度更加灵活。Affinity主要分为三类：\n\n- nodeAffinity（node亲和性）：以Node为目标，解决Pod可以调度到那些Node的问题。\n- podAffinity（pod亲和性）：以Pod为目标，解决Pod可以和那些已存在的Pod部署在同一个拓扑域中的问题。\n- podAntiAffinity（pod反亲和性）：以Pod为目标，解决Pod不能和那些已经存在的Pod部署在同一拓扑域中的问题。\n\n关于亲和性和反亲和性的使用场景的说明：\n\n- 亲和性：如果两个应用频繁交互，那么就有必要利用亲和性让两个应用尽可能的靠近，这样可以较少因网络通信而带来的性能损耗。\n- 反亲和性：当应用采用多副本部署的时候，那么就有必要利用反亲和性让各个应用实例打散分布在各个Node上，这样可以提高服务的高可用性。\n\n##### nodeAffinity（node亲和性）\n\n查看nodeAffinity的可选配置项：\n\n```yaml\npod.spec.affinity.nodeAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  Node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeSelectorTerms  节点选择列表\n      matchFields   按节点字段列出的节点选择器要求列表  \n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt\n  preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向)     \n    preference   一个节点选择器项，与相应的权重相关联\n      matchFields 按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key 键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt  \n    weight 倾向权重，在范围1-100。\n```\n\n关系符的使用说明:\n\n```yaml\n- matchExpressions:\n    - key: nodeenv # 匹配存在标签的key为nodeenv的节点\n      operator: Exists   \n    - key: nodeenv # 匹配标签的key为nodeenv,且value是\"xxx\"或\"yyy\"的节点\n      operator: In    \n      values: [\"xxx\",\"yyy\"]\n    - key: nodeenv # 匹配标签的key为nodeenv,且value大于\"xxx\"的节点\n      operator: Gt   \n      values: \"xxx\"\n```\n\n演示requiredDuringSchedulingIgnoredDuringExecution：\n○创建pod-nodeaffinity-required.yaml文件，内容如下：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers: # 容器配置\n    - name: nginx\n      image: nginx:1.17.1\n      imagePullPolicy: IfNotPresent\n      ports:\n        - name: nginx-port\n          containerPort: 80\n          protocol: TCP\n  affinity: # 亲和性配置\n    nodeAffinity: # node亲和性配置\n      requiredDuringSchedulingIgnoredDuringExecution: # Node节点必须满足指定的所有规则才可以，相当于硬规则，类似于定向调度\n        nodeSelectorTerms: # 节点选择列表\n          - matchExpressions:\n              - key: nodeenv # 匹配存在标签的key为nodeenv的节点，并且value是\"xxx\"或\"yyy\"的节点\n                operator: In\n                values:\n                  - \"xxx\"\n                  - \"yyy\"\n```\n\n> nodeAffinity的注意事项：\n> \n> - 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都满足，Pod才能运行在指定的Node上。\n> - 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可。\n> - 如果一个nodeSelectorTerms中有多个matchExpressions，则一个节点必须满足所有的才能匹配成功。\n> - 如果一个Pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的nodeAffinity的要求，则系统将忽略此变化。\n\n##### podAffinity（pod亲和性）\n\npodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod和参照的Pod在一个区域的功能。\nPodAffinity的可选配置项：\n\n```yaml\npod.spec.affinity.podAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  硬限制\n    namespaces 指定参照pod的namespace\n    topologyKey 指定调度作用域\n    labelSelector 标签选择器\n      matchExpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist.\n      matchLabels    指多个matchExpressions映射的内容  \n  preferredDuringSchedulingIgnoredDuringExecution 软限制    \n    podAffinityTerm  选项\n      namespaces\n      topologyKey\n      labelSelector\n         matchExpressions \n            key    键  \n            values 值  \n            operator\n         matchLabels \n    weight 倾向权重，在范围1-1\n```\n\n> topologyKey用于指定调度的作用域，例如:\n> \n> - 如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围。\n> - 如果指定为beta.kubernetes.io/os，则以Node节点的操作系统类型来区分。\n\n演示requiredDuringSchedulingIgnoredDuringExecution。\n创建pod-podaffinity-requred.yaml文件，内容如下：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-requred\n  namespace: dev\nspec:\n  containers: # 容器配置\n    - name: nginx\n      image: nginx:1.17.1\n      imagePullPolicy: IfNotPresent\n      ports:\n        - name: nginx-port\n          containerPort: 80\n          protocol: TCP\n  affinity: # 亲和性配置\n    podAffinity: # Pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        - labelSelector:\n            matchExpressions: # 该Pod必须和拥有标签podenv=xxx或者podenv=yyy的Pod在同一个Node上，显然没有这样的Pod\n              - key: podenv\n                operator: In\n                values:\n                  - \"xxx\"\n                  - \"yyy\"\n          topologyKey: kubernetes.io/hostname\n```\n\n##### podAntiAffinity（pod反亲和性）\n\npodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod和参照的Pod不在一个区域的功能。\n其配置方式和podAffinity一样。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podantiaffinity-requred\n  namespace: dev\nspec:\n  containers: # 容器配置\n    - name: nginx\n      image: nginx:1.17.1\n      imagePullPolicy: IfNotPresent\n      ports:\n        - name: nginx-port\n          containerPort: 80\n          protocol: TCP\n  affinity: # 亲和性配置\n    podAntiAffinity: # Pod反亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        - labelSelector:\n            matchExpressions:\n              - key: podenv\n                operator: In\n                values:\n                  - \"pro\"\n          topologyKey: kubernetes.io/hostname\n```\n\n#### 污点和容忍\n\n##### 污点（Taints）\n\n前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加污点属性，来决定是否运行Pod调度过来。\nNode被设置了污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。\n污点的格式为：key=value:effect，key和value是污点的标签，effect描述污点的作用，支持如下三个选项：\n\n- PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可以调度。\n- NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但是不会影响当前Node上已经存在的Pod。\n- NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已经存在的Pod驱逐。\n\n![污点的三种格式.png](Kubernetes知识梳理-核心组件/2.webp)\n\n##### 容忍（Toleration）\n\n上面介绍了污点的作用，我们可以在Node上添加污点用来拒绝Pod调度上来，但是如果就是想让一个Pod调度到一个有污点的Node上去，这时候应该怎么做？这就需要使用到容忍。\n\n> 污点就是拒绝，容忍就是忽略，Node通过污点拒绝Pod调度上去，Pod通过容忍忽略拒绝。\n\n容忍的详细配置：\n\n```yaml\nkubectl explain pod.spec.tolerations\n......\nFIELDS:\n  key       # 对应着要容忍的污点的键，空意味着匹配所有的键\n  value     # 对应着要容忍的污点的值\n  operator  # key-value的运算符，支持Equal和Exists（默认）\n  effect    # 对应污点的effect，空意味着匹配所有影响\n  tolerationSeconds   # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间\n```\n\n> 当operator为Equal的时候，如果Node节点有多个Taint，那么Pod每个Taint都需要容忍才能部署上去。\n> 当operator为Exists的时候，有如下的三种写法：\n> \n> - 容忍指定的污点，污点带有指定的effect：\n> - 容忍指定的污点，不考虑具体的effect：\n> - 容忍一切污点（慎用）：\n\n```yaml\n  tolerations: # 容忍\n    - key: \"tag\" # 要容忍的污点的key\n      operator: Exists # 操作符\n      effect: NoExecute # 添加容忍的规则，这里必须和标记的污点规则相同\n```\n\n```yaml\n  tolerations: # 容忍\n    - key: \"tag\" # 要容忍的污点的key\n      operator: Exists # 操作符\n```\n\n```yaml\n tolerations: # 容忍\n    - operator: Exists # 操作符\n```\n\n### Pod控制器\n\n在kubernetes中，按照Pod的创建方式可以将其分为两类：\n\n- 自主式Pod：kubernetes直接创建出来的Pod，这种Pod删除后就没有了，也不会重建。\n- 控制器创建Pod：通过Pod控制器创建的Pod，这种Pod删除之后还会自动重建。\n\nPod控制器：Pod控制器是管理Pod的中间层，使用了Pod控制器之后，我们只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它就会创建出满足条件的Pod并确保每一个Pod处于用户期望的状态，如果Pod在运行中出现故障，控制器会基于指定的策略重启或重建Pod。\n在kubernetes中，有很多类型的Pod控制器，每种都有自己的适合的场景，常见的有下面这些：\n\n- ReplicationController：比较原始的Pod控制器，已经被废弃，由ReplicaSet替代。\n- ReplicaSet：保证指定数量的Pod运行，并支持Pod数量变更，镜像版本变更。\n- Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、版本回退。\n- Horizontal Pod Autoscaler：可以根据集群负载自动调整Pod的数量，实现削峰填谷。\n- DaemonSet：在集群中的指定Node上都运行一个副本，一般用于守护进程类的任务。\n- Job：它创建出来的Pod只要完成任务就立即退出，用于执行一次性任务。\n- CronJob：它创建的Pod会周期性的执行，用于执行周期性的任务。\n- StatefulSet：管理有状态的应用。\n\n#### ReplicaSet（RS）\n\nReplicaSet的主要作用是保证一定数量的Pod能够正常运行，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对Pod数量的扩缩容。\n\n![image-20250327184456480](Kubernetes知识梳理-核心组件/3.png)\n\nReplicaSet的资源清单文件：\n\n```yaml\napiVersion: apps/v1 # 版本号 \nkind: ReplicaSet # 类型 \nmetadata: # 元数据 \n  name: # rs名称\n  namespace: # 所属命名空间 \n  labels: #标签 \n    controller: rs \nspec: # 详情描述 \n  replicas: 3 # 副本数量 \n  selector: # 选择器，通过它指定该控制器管理哪些po\n    matchLabels: # Labels匹配规则 \n      app: nginx-pod \n    matchExpressions: # Expressions匹配规则 \n      - {key: app, operator: In, values: [nginx-pod]} \ntemplate: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 \n  metadata: \n    labels: \n      app: nginx-pod \n  spec: \n    containers: \n      - name: nginx \n        image: nginx:1.17.1 \n        ports: \n        - containerPort: 80\n```\n\n在这里，需要新了解的配置项就是spec下面几个选项：\n\n- replicas：指定副本数量，其实就是当然rs创建出来的Pod的数量，默认为1.\n- selector：选择器，它的作用是建立Pod控制器和Pod之间的关联关系，采用了Label Selector机制（在Pod模块上定义Label，在控制器上定义选择器，就可以表明当前控制器能管理哪些Pod了）。\n- template：模板，就是当前控制器创建Pod所使用的模板，里面其实就是前面学过的Pod的定义。\n\n#### Deployment（Deploy）\n\n为了更好的解决服务编排的问题，kubernetes在v1.2版本开始，引入了Deployment控制器。值得一提的是，Deployment控制器并不直接管理Pod，而是通过管理ReplicaSet来间接管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment的功能比ReplicaSet强大。\n\n![image-20250327184720728](Kubernetes知识梳理-核心组件/4.png)\n\nDeployment的主要功能如下：\n\n- 支持ReplicaSet的所有功能。\n- 支持发布的停止、继续。\n- 支持版本滚动更新和版本回退。\n\nDeployment的资源清单：\n\n```yaml\napiVersion: apps/v1 # 版本号 \nkind: Deployment # 类型 \nmetadata: # 元数据 \n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签 \n    controller: deploy \nspec: # 详情描述 \n  replicas: 3 # 副本数量 \n  revisionHistoryLimit: 3 # 保留历史版本，默认为10 \n  paused: false # 暂停部署，默认是false \n  progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600 \n  strategy: # 策略 \n    type: RollingUpdate # 滚动更新策略 \n    rollingUpdate: # 滚动更新 \n      maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 最大不可用状态的    Pod 的最大值，可以为百分比，也可以为整数 \n  selector: # 选择器，通过它指定该控制器管理哪些pod \n    matchLabels: # Labels匹配规则 \n      app: nginx-pod \n    matchExpressions: # Expressions匹配规则 \n      - {key: app, operator: In, values: [nginx-pod]} \n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 \n    metadata: \n      labels: \n        app: nginx-pod \n    spec: \n      containers: \n      - name: nginx \n        image: nginx:1.17.1 \n        ports: \n        - containerPort: 80\n```\n\nDeployment支持两种镜像更新的策略：重建更新和滚动更新（默认），可以通过strategy选项进行配置。\n\n```yaml\nstrategy: 指定新的Pod替代旧的Pod的策略，支持两个属性\n  type: 指定策略类型，支持两种策略\n    Recreate：在创建出新的Pod之前会先杀掉所有已经存在的Pod\n    RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本的Pod\n  rollingUpdate：当type为RollingUpdate的时候生效，用于为rollingUpdate设置参数，支持两个属性：\n    maxUnavailable：用来指定在升级过程中不可用的Pod的最大数量，默认为25%。\n    maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。\n```\n\nDeployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看：\n\n```yaml\n# 版本升级相关功能\nkubetl rollout 参数 deploy xx  # 支持下面的选择\n# status 显示当前升级的状态\n# history 显示升级历史记录\n# pause 暂停版本升级过程\n# resume 继续已经暂停的版本升级过程\n# restart 重启版本升级过程\n# undo 回滚到上一级版本 （可以使用--to-revision回滚到指定的版本）\n```\n\n> deployment之所以能够实现版本的回退，就是通过记录下历史的ReplicaSet来实现的，一旦想回滚到那个版本，只需要将当前版本的Pod数量降为0，然后将回退版本的Pod提升为目标数量即可。\n\n#### 金丝雀发布\n\nDeployment支持更新过程中的控制，如暂停更新操作（pause）或继续更新操作（resume）。\n例如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求到新版本的Pod应用，继续观察能够稳定的按照期望的方式运行，如果没有问题之后再继续完成余下的Pod资源的滚动更新，否则立即回滚操作。\n\n#### Horizontal Pod Autoscaler（HPA）\n\n我们已经可以通过手动执行kubectl scale命令实现Pod的扩缩容，但是这显然不符合kubernetes的定位目标–自动化和智能化。kubernetes期望可以通过监测Pod的使用情况，实现Pod数量的自动调整，于是就产生了HPA这种控制器。\nHPA可以获取每个Pod的利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA和之前的Deployment一样，也属于一种kubernetes资源对象，它通过追踪分析目标Pod的负载变化情况，来确定是否需要针对性的调整目标Pod的副本数。\n\n![image-20250327185417750](Kubernetes知识梳理-核心组件/5.png)\n\n> 若集群中没有收集资源使用情况的程序，可选择安装metrics-server\n\n测试示例：\n\n```yaml\napiVersion: autoscaling/v1 # 版本号\nkind: HorizontalPodAutoscaler # 类型\nmetadata: # 元数据\n  name: pc-hpa # deployment的名称\n  namespace: dev # 命名类型\nspec:\n  minReplicas: 1 # 最小Pod数量\n  maxReplicas: 10 # 最大Pod数量\n  targetCPUUtilizationPercentage: 3 # CPU使用率指标\n  scaleTargetRef:  # 指定要控制的Nginx的信息\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx\n```\n\n#### DaemonSet（DS）\n\nDaemonSet类型的控制器可以保证集群中的每一台（或指定）节点上都运行一个副本，一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。\n\n![image-20250327185800807](Kubernetes知识梳理-核心组件/6.png)\n\nDaemonSet控制器的特点：\n\n- 每向集群中添加一个节点的时候，指定的Pod副本也将添加到该节点上。\n- 当节点从集群中移除的时候，Pod也会被垃圾回收。\n\nDaemonSet的资源清单：\n\n```yaml\napiVersion: apps/v1 # 版本号\nkind: DaemonSet # 类型\nmetadata: # 元数据\n  name: # 名称\n  namespace: #命名空间\n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionHistoryLimit: 3 # 保留历史版本\n  updateStrategy: # 更新策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxUnavailable: 1 # 最大不可用状态的Pod的最大值，可用为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理那些Pod\n    matchLabels: # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - key: app\n        operator: In\n        values:\n          - nginx-pod\n  template: # 模板，当副本数量不足时，会根据下面的模板创建Pod模板\n     metadata:\n       labels:\n         app: nginx-pod\n     spec:\n       containers:\n         - name: nginx\n           image: nginx:1.17.1\n           ports:\n             - containerPort: 80\n```\n\n#### Job\n\nJob主要用于负责批量处理短暂的一次性任务。\nJob的特点：\n\n- 当Job创建的Pod执行成功结束时，Job将记录成功结束的Pod数量。\n- 当成功结束的Pod达到指定的数量时，Job将完成执行。\n\n> Job可以保证指定数量的Pod执行完成。\n\n![image-20250327185958115](Kubernetes知识梳理-核心组件/7.png)\n\nJob的资源清单：\n\n```yaml\napiVersion: batch/v1 # 版本号\nkind: Job # 类型\nmetadata: # 元数据\n  name:  # 名称\n  namespace:  #命名空间\n  labels: # 标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定Job需要成功运行Pod的总次数，默认为1\n  parallelism: 1 # 指定Job在任一时刻应该并发运行Pod的数量，默认为1\n  activeDeadlineSeconds: 30 # 指定Job可以运行的时间期限，超过时间还没结束，系统将会尝试进行终止\n  backoffLimit: 6 # 指定Job失败后进行重试的次数，默认为6\n  manualSelector: true # 是否可以使用selector选择器选择Pod，默认为false\n  selector: # 选择器，通过它指定该控制器管理那些Pod\n    matchLabels: # Labels匹配规则\n      app: counter-pod\n    matchExpressions: # Expressions匹配规则\n      - key: app\n        operator: In\n        values:\n          - counter-pod\n  template: # 模板，当副本数量不足时，会根据下面的模板创建Pod模板\n     metadata:\n       labels:\n         app: counter-pod\n     spec:\n       restartPolicy: Never # 重启策略只能设置为Never或OnFailure\n       containers:\n         - name: counter\n           image: busybox:1.30\n           command: [\"/bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1;do echo $i;sleep 20;done\"]\n```\n\n> 关于模板中的重启策略的说明：\n> \n> - 如果设置为OnFailure，则Job会在Pod出现故障的时候重启容器，而不是创建Pod，failed次数不变。\n> - 如果设置为Never，则Job会在Pod出现故障的时候创建新的Pod，并且故障Pod不会消失，也不会重启，failed次数+1。\n> - 如果指定为Always的话，就意味着一直重启，意味着Pod任务会重复执行，这和Job的定义冲突，所以不能设置为Always。\n\n#### CronJob（CJ）\n\nCronJob控制器以 Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行**时间点**及**重复运行**的方式。也就是说，**CronJob可以在特定的时间点(反复的)去运行job任务**。\n\nCronJob的资源清单文件：\n\n```yaml\napiVersion: batch/v1beta1 # 版本号\nkind: CronJob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingDeadlineSeconds: # 启动作业错误的超时时长\n  jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activeDeadlineSeconds: 30\n      backoffLimit: 6\n      manualSelector: true\n      selector:\n        matchLabels:\n          app: counter-pod\n        matchExpressions: 规则\n          - {key: app, operator: In, values: [counter-pod]}\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartPolicy: Never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done\"]\n需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    <分钟> <小时> <日> <月份> <星期>\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencyPolicy:\n    Allow:   允许Jobs并发运行(默认)\n    Forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    Replace: 替换，取消当前正在运行的作业并用新作业替换它\n```\n\n#### StatefulSet（有状态）\n\n无状态应用：\n\n- 认为Pod都是一样的。\n- 没有顺序要求。\n- 不用考虑在哪个Node节点上运行。\n- 随意进行伸缩和扩展。\n\n有状态应用：\n\n- 有顺序的要求。\n- 认为每个Pod都是不一样的。\n- 需要考虑在哪个Node节点上运行。\n- 需要按照顺序进行伸缩和扩展。\n- 让每个Pod都是独立的，保持Pod启动顺序和唯一性。\n\nStatefulSet是Kubernetes提供的管理有状态应用的负载管理控制器。\nStatefulSet部署需要HeadLinessService（无头服务）。\n\n> 为什么需要HeadLinessService（无头服务）？\n> \n> - 在用Deployment时，每一个Pod名称是没有顺序的，是随机字符串，因此是Pod名称是无序的，但是在StatefulSet中要求必须是有序 ，每一个Pod不能被随意取代，Pod重建后pod名称还是一样的。\n> - 而Pod IP是变化的，所以是以Pod名称来识别。Pod名称是Pod唯一性的标识符，必须持久稳定有效。这时候要用到无头服务，它可以给每个Pod一个唯一的名称 。\n> \n> StatefulSet常用来部署RabbitMQ集群、Zookeeper集群、MySQL集群、Eureka集群等。\n\n演示示例：\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None # 将clusterIP设置为None，即可创建headliness Service\n  type: ClusterIP\n  ports:\n    - port: 80 # Service的端口\n      targetPort: 80 # Pod的端口\n...\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: pc-statefulset\n  namespace: dev\nspec:\n  replicas: 3\n  serviceName: service-headliness\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.17.1\n          ports:\n            - containerPort: 80\n```\n\n## Service\n\n在kubernetes中，Pod是应用程序的载体，我们可以通过Pod的IP来访问应用程序，但是Pod的IP地址不是固定的，这就意味着不方便直接采用Pod的IP对服务进行访问。\n为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个Pod进行聚合，并且提供一个统一的入口地址，通过访问Service的入口地址就能访问到后面的Pod服务。\n\n![image-20250328172653361](Kubernetes知识梳理-核心组件/8.png)\n\nService在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行了一个kube-proxy的服务进程。当创建Service的时候会通过API Server向etcd写入创建的Service的信息，而kube-proxy会基于监听的机制发现这种Service的变化，然后它会将最新的Service信息转换为对应的访问规则。\n\n![image-20250328172807052](Kubernetes知识梳理-核心组件/9.png)\n\nkube-proxy目前支持三种工作模式：\n\n- userspace模式：\n  \n  - userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法（负载均衡算法）选择一个提供服务的Pod并和其建立连接，以便将请求转发到Pod上。\n  \n  - 该模式下，kube-proxy充当了一个四层负载均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理的时候会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率非常低下。\n\n![image-20250328172925938](Kubernetes知识梳理-核心组件/10.png)\n\n- iptables模式：\n  \n  - iptables模式下，kube-proxy为Service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod的IP上。\n  \n  - 该模式下kube-proxy不承担四层负载均衡器的角色，只负责创建iptables规则。该模式的优点在于较userspace模式效率更高，但是不能提供灵活的LB策略，当后端Pod不可用的时候无法进行重试。\n\n![image-20250328173051549](Kubernetes知识梳理-核心组件/11.png)\n\nipvs模式：\n\n- ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高，除此之外，ipvs支持更多的LB（负载均衡）算法。\n\n![image-20250328173219942](Kubernetes知识梳理-核心组件/12.png)\n\n### Service类型\n\nService的资源清单：\n\n```yaml\napiVersion: v1 # 版本\nkind: Service # 类型\nmetadata: # 元数据\n  name: # 资源名称\n  namespace: # 命名空间\nspec:\n  selector: # 标签选择器，用于确定当前Service代理那些Pod\n    app: nginx\n  type: NodePort # Service的类型，指定Service的访问方式\n  clusterIP: # 虚拟服务的IP地址\n  sessionAffinity: # session亲和性，支持ClientIP、None两个选项，默认值为None\n  ports: # 端口信息\n    - port: 8080 # Service端口\n      protocol: TCP # 协议\n      targetPort : # Pod端口\n      nodePort:  # 主机端口\n```\n\n> spec.type的说明：\n> \n> - ClusterIP：默认值，它是kubernetes系统自动分配的虚拟IP，只能在集群内部访问。\n> - NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务。\n> - LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境的支持。\n> - ExternalName：把集群外部的服务引入集群内部，直接使用。\n\n#### ClusterIP类型的Service\n\nEndpoint（实际中使用的不多）\n\n- Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有Pod的访问地址，它是根据service配置文件中的selector描述产生的。\n- 一个service由一组Pod组成，这些Pod通过Endpoints暴露出来，Endpoints是实现实际服务的端点集合。换言之，service和Pod之间的联系是通过Endpoints实现的。\n\n![image-20250328173454822](Kubernetes知识梳理-核心组件/13.png)\n\n 负载分发策略\n\n对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略：\n\n- 如果不定义，默认使用kube-proxy的策略，比如随机、轮询等。\n- 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上，这对于传统基于Session的认证项目来说很友好，此模式可以在spec中添加sessionAffinity: ClusterIP选项。\n\n#### HeadLiness类型的Service\n\n在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLinesss Service，这类Service不会分配Cluster IP，如果想要访问Service，只能通过Service的域名进行查询。\n\n#### NodePort类型的Service\n\n在之前的案例中，创建的Service的IP地址只能在集群内部才可以访问，如果希望Service暴露给集群外部使用，那么就需要使用到另外一种类型的Service，称为NodePort类型的Service。NodePort的工作原理就是将Service的端口映射到Node的一个端口上，然后就可以通过NodeIP:NodePort来访问Service了。\n\n![image-20250328173809010](Kubernetes知识梳理-核心组件/14.png)\n\n#### LoadBalancer类型的Service\n\n![image-20250328173923606](Kubernetes知识梳理-核心组件/15.png)\n\n#### ExternalName类型的Service\n\nExternalName类型的Service用于引入集群外部的服务，它通过externalName属性指定一个服务的地址，然后在集群内部访问此Service就可以访问到外部的服务了。\n\n![image-20250328174018302](Kubernetes知识梳理-核心组件/16.png)\n\n## Ingress\n\n我们已经知道，Service对集群之外暴露服务的主要方式有两种：NodePort和LoadBalancer，但是这两种方式，都有一定的缺点：\n\n- NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显。\n- LoadBalancer的缺点是每个Service都需要一个LB，浪费，麻烦，并且需要kubernetes之外的设备的支持。\n\n基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求，工作机制大致如下图所示：\n\n![image-20250328174120763](Kubernetes知识梳理-核心组件/17.png)\n\n实际上，Ingress相当于一个七层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解为Ingress里面建立了诸多映射规则，Ingress Controller通过监听这些配置规则并转化为Nginx的反向代理配置，然后对外提供服务。\n\n- Ingress：kubernetes中的一个对象，作用是定义请求如何转发到Service的规则。\n- Ingress Controller：具体实现反向代理及负载均衡的程序，对Ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现的方式有很多，比如Nginx，Contour，Haproxy等。\n\nIngress（以Nginx）的工作原理如下：\n\n- 用户编写Ingress规则，说明那个域名对应kubernetes集群中的那个Service。\n- Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx的反向代理配置。\n- Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新。\n- 到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求规则。\n\n![image-20250328174300573](Kubernetes知识梳理-核心组件/18.png)\n\n> Ingress支持Http代理和Https代理\n\n## 数据存储\n\n在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。\n\nVolume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。\n\nkubernetes的Volume支持多种类型，比较常见的有下面几个：\n\n- 简单存储：EmptyDir、HostPath、NFS\n- 高级存储：PV、PVC\n- 配置存储：ConfigMap、Secret\n\n### 基本存储\n\n#### EmptyDir\n\nEmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。\n\nEmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下：\n\n- 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留\n- 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。\n\n在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。\n\n![image-20250328174613499](Kubernetes知识梳理-核心组件/19.png)\n\n#### HostPath\n\n上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。\n\nHostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。\n\n![image-20250328174658800](Kubernetes知识梳理-核心组件/20.png)\n\n#### NFS\n\nHostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。\n\nNFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。\n\n![image-20250328174744542](Kubernetes知识梳理-核心组件/21.png)\n\n### 高级存储\n\n前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。\n\n- PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\n- PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。\n\n![image-20250329114433511](Kubernetes知识梳理-核心组件/22.png)\n\n使用了PV和PVC之后，工作可以得到进一步的细分：\n\n- 存储：存储工程师维护\n- PV： kubernetes管理员维护\n- PVC：kubernetes用户维护\n\n#### PV\n\nPV是存储资源的抽象，下面是资源清单文件:\n\n```yaml\napiVersion: v1  \nkind: PersistentVolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2Gi\n  accessModes:  # 访问模式\n  storageClassName: # 存储类别\n  persistentVolumeReclaimPolicy: # 回收策略\n```\n\nPV 的关键配置参数说明：\n\n- **存储类型**\n  \n  底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异\n\n- **存储能力（capacity）**\n\n目前只支持存储空间的设置( storage=1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置\n\n- **访问模式（accessModes）**\n  \n  用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\n  \n  - ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载\n  - ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载\n  - ReadWriteMany（RWX）：读写权限，可以被多个节点挂载\n  \n  `需要注意的是，底层不同的存储类型可能支持的访问模式不同`\n\n- **回收策略（persistentVolumeReclaimPolicy）**\n  \n  当PV不再被使用了之后，对其的处理方式。目前支持三种策略：\n  \n  - Retain （保留） 保留数据，需要管理员手工清理数据\n  - Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/*\n  - Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务\n  \n  `需要注意的是，底层不同的存储类型可能支持的回收策略不同`\n\n- **存储类别**\n  \n  PV可以通过storageClassName参数指定一个存储类别\n  \n  - 具有特定类别的PV只能与请求了该类别的PVC进行绑定\n  - 未设定类别的PV则只能与不请求任何类别的PVC进行绑定\n\n- **状态（status）**\n  \n  一个 PV 的生命周期中，可能会处于4中不同的阶段：\n  \n  - Available（可用）： 表示可用状态，还未被任何 PVC 绑定\n  - Bound（已绑定）： 表示 PV 已经被 PVC 绑定\n  - Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明\n  - Failed（失败）： 表示该 PV 的自动回收失败\n\n#### PVC\n\nPVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessModes: # 访问模式\n  selector: # 采用标签对PV选择\n  storageClassName: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5Gi\n```\n\nPVC 的关键配置参数说明：\n\n- **访问模式（accessModes）**\n\n用于描述用户应用对存储资源的访问权限\n\n- **选择条件（selector）**\n  \n  通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选\n\n- **存储类别（storageClassName）**\n  \n  PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出\n\n- **资源请求（Resources ）**\n  \n  描述对存储资源的请求\n\n#### 生命周期\n\nPVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期：\n\n- **资源供应**：管理员手动创建底层存储和PV\n\n- **资源绑定**：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定\n  \n  在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的\n  \n  - 一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了\n  - 如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV\n  \n  PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了\n\n- **资源使用**：用户可在pod中像volume一样使用pvc\n  \n  Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。\n\n- **资源释放**：用户删除pvc来释放pv\n  \n  当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。\n\n- **资源回收**：kubernetes根据pv设置的回收策略进行资源的回收\n  \n  对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用\n\n![img](Kubernetes知识梳理-核心组件/23.png)\n\n#### 配置存储\n\n##### ConfigMap\n\nConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n\n创建configmap.yaml，内容如下：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n```\n\n接下来，使用此配置文件创建configmap\n\n```yaml\n# 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nName:         configmap\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\n\nData\n====\ninfo:\n....\nusername:admin\npassword:123456\n\nEvents:  <none>\n```\n\n接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将configmap挂载到目录\n    - name: config\n      mountPath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configMap:\n      name: configmap\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-configmap   1/1     Running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key表示文件     value表示文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n```\n\n##### Secret\n\n在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n\n首先使用base64对数据进行编码\n\n```yaml\n[root@k8s-master01 ~]# echo -n 'admin' | base64 #准备username\nYWRtaW4=\n[root@k8s-master01 ~]# echo -n '123456' | base64 #准备password\nMTIzNDU2\n```\n\n接下来编写secret.yaml，并创建Secret\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MTIzNDU2\n# 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nName:         secret\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\nType:  Opaque\nData\n====\npassword:  6 bytes\nusername:  5 bytes\n```\n\n创建pod-secret.yaml，将上面创建的secret挂载进去：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将secret挂载到目录\n    - name: config\n      mountPath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretName: secret\n# 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-secret      1/1     Running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n```\n\n至此，已经实现了利用secret实现了信息的编码。\n\n## 安全认证\n\nKubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种**客户端**进行**认证和鉴权**操作。\n\n**客户端**\n\n在Kubernetes集群中，客户端通常有两类：\n\n- **User Account**：一般是独立于kubernetes之外的其他服务管理的用户账号。\n- **Service Account**：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。\n\n![img](Kubernetes知识梳理-核心组件/24.png)\n\n**认证、授权与准入控制**\n\nApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程：\n\n- Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证\n- Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作\n- Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。\n\n![img](Kubernetes知识梳理-核心组件/25.png)\n\n### 认证管理\n\nKubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式：\n\n- HTTP Base认证：通过用户名+密码的方式认证\n  \n\n这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n\n- HTTP Token认证：通过一个Token来识别合法用户\n  \n\n这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。\n\n- HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式\n  \n\n这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n\n![img](Kubernetes知识梳理-核心组件/26.png)\n\n**HTTPS认证大体分为3个过程：**\n\n1. 证书申请和下发\n   \n\n​    HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者\n\n2. 客户端和服务端的双向认证\n\n（1） 客户端向服务器端发起请求，服务端下发自己的证书给客户端， 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥， 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n（2）客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n\n3. 服务器端和客户端进行通信\n\n服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n\n> 注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n\n### 授权管理\n\n授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n\n每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\n\nAPI Server目前支持以下几种授权策略：\n\n- AlwaysDeny：表示拒绝所有请求，一般用于测试\n- AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略）\n- ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制\n- Webhook：通过调用外部REST服务对用户进行授权\n- Node：是一种专用模式，用于对kubelet发出的请求进行访问控制\n- RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项）\n\nRBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：**给哪些对象授予了哪些权限**\n\n其中涉及到了下面几个概念：\n\n- 对象：User、Groups、ServiceAccount\n- 角色：代表着一组定义在资源上的可操作动作(权限)的集合\n- 绑定：将定义好的角色跟用户绑定在一起\n\n![img](Kubernetes知识梳理-核心组件/27.png)\n\nRBAC引入了4个顶级资源对象：\n\n- Role、ClusterRole：角色，用于指定一组权限\n- RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象\n\n**Role、ClusterRole**\n\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n\n```yaml\n# Role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apiGroups: [\"\"]  # 支持的API组列表,\"\" 空字符串，表示核心API群\n  resources: [\"pods\"] # 支持的资源对象列表\n  verbs: [\"get\", \"watch\", \"list\"] # 允许的对资源对象的操作方法列表\n# ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n需要详细说明的是，rules中的参数：\n\n- apiGroups: 支持的API组列表\n  \n  \n  \n```yaml\n  \"\",\"apps\", \"autoscaling\", \"batch\"\n```\n\n- resources：支持的资源对象列表\n  \n```yaml\n  \"services\", \"endpoints\", \"pods\",\"secrets\",\"configmaps\",\"crontabs\",\"deployments\",\"jobs\",\n  \"nodes\",\"rolebindings\",\"clusterroles\",\"daemonsets\",\"replicasets\",\"statefulsets\",\n  \"horizontalpodautoscalers\",\"replicationcontrollers\",\"cronjobs\"\n```\n\n- verbs：对资源对象的操作方法列表\n  \n```yaml\n  \"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\", \"exec\"\n```\n\n**RoleBinding、ClusterRoleBinding**\n\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。\n\n```yaml\n# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n - kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: authorization-role\n  apiGroup: rbac.authorization.k8s.io\n# ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n - kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n```\n\n**RoleBinding引用ClusterRole进行授权**\n\nRoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。\n\n一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n\n```yaml\n# 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding\n# 所以heima只能读取dev命名空间中的资源\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n - kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 准入控制\n\n通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。\n\n准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器：\n\n```yaml\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,\n                      DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\n```\n\n只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。\n\n当前可配置的Admission Control准入控制如下：\n\n- AlwaysAdmit：允许所有请求\n- AlwaysDeny：禁止所有请求，一般用于测试\n- AlwaysPullImages：在启动容器之前总去下载镜像\n- DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求\n- ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。\n- Service Account：实现ServiceAccount实现了自动化\n- SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效\n- ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标\n- LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制\n- InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置\n- NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。\n- DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节\n- DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min\n- PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制\n\n> 本篇知识来源于B站视频BV1Qv41167ck\n","tags":["Linux","运维","k8s","知识梳理","pod","Service","Ingress","PV/PVC"],"categories":["容器化","梳理总结","知识梳理","k8s"]},{"title":"Jenkins知识梳理","url":"/yyg/f0979e05/","content":"{% folding cyan,🔜什么是Jenkins？🔚 %}\n\nJenkins是一款基于Java开发的开源 CI&CD 软件，用于自动化各种任务，包括构建、测试和部署软件。Jenkins提供多种安装方式，提供超过1000个插件来满足任何项目的需要。\n\nJenkins的特征：\n\n- 开源的Java语言开发持续集成工具，支持持续集成，持续部署。\n- 易于安装部署配置：可通过yum安装，或下载war包以及通过docker容器等快速实现安装部署，可方便web界面配置管理。\n- 消息通知及测试报告：集成RSS/E-mail通过RSS发布构建结果或当构建完成时通过e-mail通知，生成JUnit/TestNG测试报告。\n- 分布式构建：支持制Jenkins能够让多台计算机一起构建/测试。\n- 文件识别：Jenkins能够跟踪哪次构建生成哪些jar,哪次构建使用哪个版本的jar等。\n- 丰富的插件支持：支持扩展插件，你可以开发适合自己团队使用的工具，如git,svn,maven,docker等。\n\n{% endfolding %}\n\n> 本文通过GitLab+Jenkins+Tomcat，构建CI/CD流程。\n\n## 主机规划\n\n| 主机名 | ip            | 所需软件                 | 用途                            |\n| ------ | ------------- | --------------------------------- | --------------------------------- |\n| 121    | 192.168.1.121 | GitLab-12.4.2 | 代码托管 |\n| 124   | 192.168.1.124 | Jenkins、JDK-21、Maven-3.9.9、<br />Git、SonarQube | Jenkins持续集成 |\n| 125   | 192.168.1.125 | JDK-1.8、Tomcat-8.5 | 发布测试 |\n\n> 服务器操作系统为centos7.9\n\n## 部署步骤\n\n### 部署GitLab\n\n#### 安装GitLab\n\n安装依赖\n\n```\nyum -y install policycoreutils-python policycoreutils postfix\n```\n\n下载并安装gitlab\n\n```shell\n#下载gitlab安装包\ncurl -O  http://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-12.4.2-ce.0.el7.x86_64.rpm\n#安装gitlab\nrpm -i gitlab-ce-12.4.2-ce.0.el7.x86_64.rpm\n```\n\n修改gitlab默认访问端口\n\n```shell\nvi /etc/gitlab/gitlab.rb\n\n#将默认端口80修改为82\nexternal_url \"http://192.168.1.121:82\"\nnginx['listen_port'] = 82\n```\n\n重载配置，启动gitlab\n\n```shell\ngitlab-ctl reconfigure\ngitlab-ctl restart\n```\n\n配置防火墙策略\n\n```shell\nfirewall-cmd --add-port=82/tcp --permanent\nfirewall-cmd --reload\nfirewall-cmd --list-all\n```\n\n> 在浏览器中输入http://192.168.1.121:82/,修改root用户密码，并用root用户登录gitlab\n\n#### 管理GitLab\n\n（1）创建组\n\n使用管理员root创建组，一个组里面可以有多个项目分支，可以将开发添加到组里面进行设置权限，不同的组就是公司不同的开发项目或者服务模块，不同的组添加不同的开发即可实现对开发设置权限的管理。\n\n（2）创建项目\n\n（3）创建用户\n\n在管理中心-概览-用户中，创建一个新用户`test1`，并设置密码，将用户加入刚创建的项目中。\n\n> Gitlab用户在组里面有5种不同权限：\n>\n> - Guest：可以创建issue、发表评论，不能读写版本库\n> - Reporter：可以克隆代码，不能提交，QA、PM可以赋予这个权限\n> - Developer：可以克隆代码、开发、提交、push,普通开发可以赋予这个权限\n> - Maintainer：可以创建项目、添加tag、保护分支、添加项目成员、编辑项目，核心开发可以赋予这个权限\n> - Owner：可以设置项目访问权限Visibility Level、删除项目、迁移项目、管理组成员，卉发组组长可以赋予这个权限\n\n#### 上传项目代码\n\n> 使用idea创建java项目，并上传至gitlab仓库\n>\n> 本文中使用的java项目代码，源自b站up主，''涣沷a靑惷''，原文链接为： https://www.yuque.com/huanfqc/jenkins/jenkins。\n>\n> java代码地址为： https://share.feijipan.com/s/qdEO7czl\n\n将项目导入idea中，并配置gitlab相关设置，推送代码，具体步骤参考上面提到的原文链接。\n\n### 部署Jenkins\n\n> 安装jenkins-2.19*版本时，无法正常安装中文插件。本文使用的jdk为jdk-21、jenkins为2.492.3，且均为rpm方式安装\n\n（1）安装jdk\n\n```shell\nyum install -y wget && wget https://download.oracle.com/java/21/latest/jdk-21_linux-x64_bin.rpm\nrpm -ivh jdk-21_linux-x64_bin.rpm\n#查看版本\njava -version\n```\n\n（2）安装Jenkins\n\n```shell\n#下载jenkins\ncurl -O https://mirrors.tuna.tsinghua.edu.cn/jenkins/redhat-stable/jenkins-2.492.3-1.1.noarch.rpm\n#安装\nrpm -ivh jenkins-2.492.3-1.1.noarch.rpm\n```\n\n（3）修改jenkins的配置文件\n\n```shell\nvi /etc/sysconfig/jenkins\n#修改默认用户为root\nJENKINS_USER=\"root\"\n#修改默认端口为8888\nJENKINS_PORT=8888\n```\n\n（4）配置防火墙策略，并启动jenkins\n\n```shell\nfirewall-cmd --add-port=8888/tcp --permanent\nfirewall-cmd --reload\nfirewall-cmd --list-all\n#启动jenkins\nsystemctl daemon-reload\nsystemctl start jenkins\nsystemctl enable jenkins\n```\n\n（5）查看jenkins的admin初始化密码\n\n```shell\ncat /var/lib/jenkins/secrets/initialAdminPassword\n```\n\n（6）在浏览器中输入http://192.168.1.124:8888/访问，输入admin初始化密码。\n\n（7）点击`选择插件来安装`，选择`无`，跳过安装插件，为之后重新配置下载插件的国内源做准备。\n\n![img](Jenkins知识梳理/1.png)\n\n（8）按引导完成jenkins的安装。\n\n#### 配置国内插件源\n\n（1）选择左侧边栏中的Jenkins -Manage Jenkins-Manage Plugins，选择Available，等待插件相关文件的加载。\n\n（2）进入插件的目录，将地址修改为国内源\n\n```shell\ncd /var/lib/jenkins/updates && ll\nsed -i 's|http://updates.jenkins-ci.org/download|https://mirrors.tuna.tsinghua.edu.cn/jenkins|g' default.json\nsed -i 's|http://www.google.com|https://www.baidu.com|g' default.json\n```\n\n（3）访问jenkins，选择左侧边栏中的Jenkins -Manage Jenkins-Manage Plugins，选择Advanced，把Update Site改为`https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json`（或者`https://mirrors.huaweicloud.com/jenkins/updates/update-center.json`），保存，在浏览器中输入`http://192.168.1.124:8888/restart`，点击并重启jenkins。\n\n#### 设置中文\n\n安装中文插件，选择左侧边栏中的Jenkins -Manage Jenkins-Manage Plugins，选择Advanced，搜索`chinese`，勾选插件，并点击下载并重启。\n\n#### 管理用户权限\n\n安装`Role-based Authorization Strategy`插件，管理用户权限。安装完成后，（新版jenkins）选择Manage Jenkins-Security，授权策略选择Role-Based Strategy。选择Manage Jenkins-Manage and Assign Roles，进行角色的创建和分配管理。\n\n#### 凭证管理\n\n（1）安装`Credentials Binding`插件，进行凭证管理。\n\n凭证类型：\n\n- Username with password：用户名和密码\n- SSH Username with private key：使用ssH用户和密钥\n- Secret file：需要保密的文本文件，使用时Jenkins会将文件复制到一个临时目录中，再将文件路径设置到一个变量中，等构建结束后，所复制的Secret file就会被删除。\n- Secret text：需要保存的一个加密的文本串，如钉钉机器人或Github的api token\n- Certificate：通过上传证书文件的方式\n\n> 常用的凭证类型有：Username with password(用户密码)和SSH Username with private key(SSH密钥)\n\n（2）安装`git`插件，并在服务器上下载git。\n\n```shell\nyum -y install git\n```\n\n##### 密码凭证\n\n（1）创建jenkins项目和密码凭证。选择Manage Jenkins-Credentials，点击`全局`-`Add Credentials`，添加gitlab用户test1的用户名和密码，创建凭证。\n\n![img](Jenkins知识梳理/2.png)\n\n（2）创建一个新任务`test01`，点击配置，设置源码管理为Git，填写gitlab的项目仓库地址（http开头），选择test1的凭证，保存即可。\n\n（3）进入test01，点击Build Now，等待任务构建完成后，点击控制台输出查看详细信息。\n\n![img](Jenkins知识梳理/3.png)\n\n![img](Jenkins知识梳理/4.png)\n\n##### SSH密钥凭证\n\n（1）在jenkins服务器中创建SSH密钥。\n\n```shell\nssh-keygen -t rsa\n#查看公钥内容\ncat ~/.ssh/id_rsa.pub\n# 测试 SSH 连接\nssh -T git@192.168.1.121\n# 查看是否有gitlab主机，确保有该主机，否则使用ssh凭证时会报错\ncat ~/.ssh/known_hosts\n```\n\n（2）使用管理员账户登录Gitlab，选择`设置`-`SSH密钥`，填入SSH公钥内容，并保存。\n\n（3）在服务器中使用命令`cat ~/.ssh/id_rsa`，查看私钥内容。在jenkins中创建SSH密钥凭证，输入用户名root和SSH私钥内容，保存即可。\n\n![img](Jenkins知识梳理/5.png)\n\n（4）创建一个新任务`test02`，点击配置，设置源码管理为Git，填写gitlab的项目仓库地址（git开头），选择SSH凭证，保存即可。\n\n（5）进入test02，点击Build Now，等待任务构建完成后，点击控制台输出查看详细信息。\n\n![img](Jenkins知识梳理/6.png)\n\n### 安装Maven\n\n（1）下载安装Maven\n\n```shell\ncurl -O https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.9.9/binaries/apache-maven-3.9.9-bin.tar.gz\n#创建存储目录\nmkdir -p /opt/maven\ntar -vxzf apache-maven-3.9.9-bin.tar.gz -C /opt/maven/\n#配置环境变量，使环境生效\necho \"\nexport MAVEN_HOME=/opt/maven/apache-maven-3.9.9\nexport PATH=\\$PATH:\\$MAVEN_HOME/bin\" >> /etc/profile\nsource /etc/profile\n#查看maven版本\nmvn -v\n```\n\n（2）配置jenkins与maven关联\n\n在Manage Jenkins-Tools中配置`JDK安装`和`Maven安装`，配置其对应安装路径。\n\n![img](Jenkins知识梳理/8.png)\n\n在Manage Jenkins-System-全局属性中，新增3个`Environment variables`。\n\n![img](Jenkins知识梳理/7.png)\n\n（3）修改Maven的本地仓库位置\n\n```shell\n#创建本地仓库\nmkdir -p /data/jenkins/repo\n#修改Maven的配置文件\nvi /opt/maven/apache-maven-3.9.9/conf/settings.xml\n#将<localRepository>/path/to/local/repo</localRepository>修改为\n  <localRepository>/data/jenkins/repo</localRepository>\n  \n#在<id>maven-default-http-blocker</id>所在的镜像后添加阿里云镜像源\n#    <mirror>\n#      <id>maven-default-http-blocker</id>\n#      <mirrorOf>external:http:*</mirrorOf>\n#      <name>Pseudo repository to mirror external repositories initially using HTTP.</name>\n#      <url>http://0.0.0.0/</url>\n#      <blocked>true</blocked>\n#    </mirror>\n    <mirror>\n      <id>alimaven</id>\n      <name>aliyun maven</name>\n      <url>http://maven.aliyun.com/nexus/content/groups/public</url>\n      <mirrorOf>central</mirrorOf>\n    </mirror>\n\n```\n\n（4）测试Maven配置\n\n在jenkins中选择任务`test02`，选择配置-Environment-Build Steps，选择Execute shell中输入命令`mvn clean package`,保存后，点击Build Now，等待任务构建完成后，点击控制台输出查看详细信息。\n\n![img](Jenkins知识梳理/9.png)\n\n![img](Jenkins知识梳理/10.png)\n\n此时服务器上的test02任务目录中会新增一个`target`目录。\n\n> [root@124 workspace]# ls\n> test01  test01@tmp  test02  test02@tmp\n> [root@124 workspace]# cd test01\n> [root@124 test01]# ls\n> pom.xml  src\n> [root@124 test01]# cd ..\n> [root@124 workspace]# ls\n> test01  test01@tmp  test02  test02@tmp\n> [root@124 workspace]# cd test02\n> [root@124 test02]# ls\n> pom.xml  src  **target**\n> [root@124 test02]# pwd\n> /var/lib/jenkins/workspace/test02\n> [root@124 test02]#\n\n### 安装Tomcat\n\n下载jdk-1.8和tomcat-9\n\n```shell\nyum install -y java-1.8.0-openjdk*\ncurl -O https://mirrors.huaweicloud.com/apache/tomcat/tomcat-9/v9.0.104/bin/apache-tomcat-9.0.104.tar.gz\n#创建存储目录\nmkdir -p /opt/tomcat/\n#解压\ntar -zxvf apache-tomcat-9.0.104.tar.gz -C /opt/tomcat/\n#启动tomcat\n/opt/tomcat/apache-tomcat-9.0.104/bin/startup.sh\n```\n\n配置防火墙策略\n\n```\nfirewall-cmd --add-port=8080/tcp --permanent\nfirewall-cmd --reload\nfirewall-cmd --list-all\n```\n\n在浏览器中访问`http://192.168.1.125:8080/`。\n\n![img](Jenkins知识梳理/11.png)\n\n配置tomcat的用户角色\n\n```xml\nvi  /opt/tomcat/apache-tomcat-9.0.104/conf/tomcat-users.xml\n\n<!--在tomcat-users标签下添加以下内容 -->\n\n<role rolename=\"tomcat\"/>\n<role rolename=\"role1\"/>\n<role rolename=\"manager-script\"/>\n<role rolename=\"manager-gui\"/>\n<role rolename=\"manager-status\"/>\n<role rolename=\"admin-gui\"/>\n<role rolename=\"admin-script\"/>\n<user username=\"tomcat\" password=\"tomcat\" roles=\"manager-gui,manager-script,tomcat,admin-gui,admin-script\"/>\n\n```\n\n配置tomcat的远程访问地址范围\n\n```xml\nvi /opt/tomcat/apache-tomcat-9.0.104/webapps/manager/META-INF/context.xml\n<!-- 将以下内容注释-->\n  <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />\n\n```\n\n重启tomcat，输入用户名和密码进行访问。\n\n```shell\n#停止运行\n/opt/tomcat/apache-tomcat-9.0.104/bin/shutdown.sh\n#启动\n/opt/tomcat/apache-tomcat-9.0.104/bin/startup.sh\n```\n\n### Jenkins构建项目\n\nJenkins中自动构建项目的类型有很多，常用的有以下三种：\n\n- 自由风格软件项目(FreeStyle Project)\n- Maven项目(Maven Project)\n- 流水线项目(Pipeline Project)\n\n> 本文构建项目时使用的war包，源自b站up主，''涣沷a靑惷''，原文链接为： https://www.yuque.com/huanfqc/jenkins/jenkins。\n>\n> war包项目代码地址为：https://share.feijipan.com/s/BWEO9Jad，war包构建具体过程见原文。\n\n#### Jenkins构建自由风格项目\n\n（1）安装 `Deploy to container` 插件\n\n（2）新建项目freestyle_demo，配置并拉取gitlab中test_war代码，该过程除包含步骤《部署Jenkins》的过程外，还需进行构建后操作，具体如下图。\n\n![img](Jenkins知识梳理/12.png)\n\n（3）配置保存后，，点击Build Now，等待任务构建完成后，访问tomcat，可以看到`/websocketChat-1.0-SNAPSHOT`虚拟目录已出现。\n\n![img](Jenkins知识梳理/13.png)\n\n#### Jenkins构建Maven项目\n\n（1）安装`Maven Integration`插件\n\n（2）新建项目maven-demo，配置并拉取gitlab中test_war代码，该过程除包含步骤《Jenkins构建自由风格项目》的过程外，还需要在Build时进行修改，具体如下图。\n\n![img](Jenkins知识梳理/14.png)\n\n> 构建时出现报错可将之前配置的阿里云镜像地址改为https\n\n（3）配置保存后，点击Build Now，等待任务构建完成后，访问tomcat，可以看到`/websocketChat-1.0-SNAPSHOT`虚拟目录已出现。\n\n#### Jenkins构建Pipeline流水线项目\n\nPipeline,简单来说，就是一套运行在Jenkins上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。\n\n如何创建Jenkins Pipeline呢？\n\n- Pipeline脚本是由Groovy语言实现的。\n\n- Pipeline支持两种语法：Declarative(声明式)和Scripted Pipeline(脚本式)语法。\n\n- Pipeline也有两种创建方法：可以直接在Jenkins的Web Ul界面中输入脚本；也可以通过创建一个Jenkinsfile脚本文件放入项目源码库中（一般我们都推荐在Jenkins中直接从源代码控制(SCM)中直接载入Jenkinsfile Pipeline这种方法)。\n\n##### 在Jenkins的Web Ul中创建Pipeline脚本发布项目\n\n（1）安装Pipeline插件\n\n（2）新建任务Pipeline-demo，使用声明式语法，在Jenkins的Web Ul里创建Pipeline脚本；实现从代码拉取、编译构建、到发布项目。\n\n在Pipeline-demo的配置界面，选择构建触发器-流水线，选择一个脚本模板Hello Word。\n\n![img](Jenkins知识梳理/15.png)\n\n点击`流水线语法`，在片段生成器中选择`checkout：Check out from version control`，按引导填写项目地址和凭证，生成流水线脚本，将拉取代码脚本粘贴到修改后的模板中。\n\n在片段生成器中选择`sh：Shell Script`，填写编译命令，生成流水线脚本，将脚本粘贴到修改后的模板中。\n\n在片段生成器中选择`deploy：Deploy war/ear to a container`，按引导填写，生成流水线脚本，将脚本粘贴到修改后的模板中。\n\n```groovy\npipeline {\n    agent any\n\n    stages {\n        stage('拉取代码') {\n            steps {\n                checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'fbf87557-40b6-468a-91c5-2cfa1b2e33e8', url: 'http://192.168.1.121:82/test/test_war_demo.git']])\n            }\n        }\n        stage('编译构建') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('发布项目') {\n            steps {\n                deploy adapters: [tomcat9(credentialsId: '4d9b76aa-078c-462b-81f1-33fe1d9c971c', path: '', url: 'http://192.168.1.125:8080/')], contextPath: null, war: 'target/*.war'\n            }\n        }\n    }\n}\n\n```\n\n（3）配置保存后，点击Build Now，等待任务构建完成后，访问tomcat，可以看到`/websocketChat-1.0-SNAPSHOT`虚拟目录已出现。\n\n##### 使用Jenkinsfile脚本发布项目\n\n（1）在idea中的项目目录下，选择New-File，创建Jenkinsfile文件，将Pipeline脚本内容粘贴进去，随代码一起发布到项目仓库中。\n\n![img](Jenkins知识梳理/16.png)\n\n![img](Jenkins知识梳理/17.png)\n\n（2）在任务Pipeline-demo的配置界面，选择构建触发器-流水线，定义内容选择`Pipeline script from SCM`-`GIt`，填写项目地址和凭证，脚本路径填写`Jenkinsfile`。\n\n![img](Jenkins知识梳理/18.png)\n\n（3）配置保存后，点击Build Now，等待任务构建完成后，访问tomcat，可以看到`/websocketChat-1.0-SNAPSHOT`虚拟目录已出现。\n\n#### 常用构建触发器\n\nJenkins内置4种构建触发器：\n\n- 触发远程构建\n\n- 其他工程构建后触发(Build after other projects are build)\n\n- 定时构建(Build periodically)\n\n  > 定时字符串从左往右分别为：分 时 日 月 周 \n  >\n  > 定时表达式的例子：\n  >\n  > 每30分钟构建一次：H代表形参\n  >\n  > H/30 * * * *\n  >\n  > 每2个小时构建一次：\n  >\n  > H H/2 * * *\n  >\n  > 每天的8点，12点，22点，一天构建3次：（多个时间点中间用逗号隔开）\n  >\n  > 0 8,12,22 * * *\n\n- 轮询SCM(Poll SCM)\n\n> 轮询SCM,是指定时扫描本地代码仓库的代码是否有变更，如果代码有变更就触发项目构建。\n\n（1）触发远程构建\n\n在任务Pipeline-demo的配置界面，选择`构建触发器`-`触发远程构建 (例如,使用脚本)`，自定义身份验证令牌为`test`，保存配置。在浏览器中访问：\n\n> jenkins地址/job/Pipeline-demo/build?token=`TOKEN_NAME`  \n\nhttp://192.168.1.124:8888/job/Pipeline-demo/build?token=test ，返回查看任务状态已经开始构建。\n\n（2）其他工程构建后触发(Build after other projects are build)\n\n当需要多个任务先后执行时可用。比如若希望执行`freestyle_demo`任务后，再执行`Pipeline-demo`，在任务Pipeline-demo的配置界面，选择`构建触发器`-`Build after other projects are built`，中输入`freestyle_demo`，选择`只有构建稳定时触发`，保存配置。freestyle_demo任务构建完成后，freestyle_demo开始进行构建。\n\n（3）定时构建(Build periodically)\n\n在任务Pipeline-demo的配置界面，选择`构建触发器`-`Build periodically`，设置`*/1 * * * *`（一分钟一次），保存配置，freestyle_demo任务会每分钟构建一次。\n\n（4）轮询SCM(Poll SCM)\n\n轮询SCM也要设置时间周期，不过与定时构建不同的是，如果在时间周期内代码仓库未发生变化，任务构建就不会进行。比如：在任务Pipeline-demo的配置界面，选择`构建触发器`-`Poll SCM`，设置`*/1 * * * *`（一分钟一次），保存配置。如果在之后的一分钟内代码变更，Pipeline-demo任务会进行构建，反之，直到代码变更后的那一分钟周期内进行构建。\n\n#### Git hook构建触发器（常用）\n\n> 当GitLab出现代码变更时，会提交构建请求到Jenkins，Jenkins收到请求后，触发构建。\n>\n\n（1）安装GitLab插件和GitLab Hook插件（新版本自带GitLab Hook插件）\n\n（2）在Jenkins的任务Pipeline-demo的配置界面，选择`构建触发器`-`Build when a change is pushed to GitLab. GitLab webhook URL: http://192.168.1.124:8888/project/Pipeline-demo`，会出现以下表格中的选项，默认选择即可。\n\n| **事件类型**              | **触发条件**                                 |\n| ------------------------- | -------------------------------------------- |\n| **Push Events**           | 代码推送到分支或标签，包括新分支创建。       |\n| **Push on Branch Delete** | 分支被删除时。                               |\n| **Opened Merge Request**  | 创建新的合并请求时触发。                     |\n| **New Commits in MR**     | 向已存在的MR推送新提交时触发（更新MR）。     |\n| **Accepted (Merged) MR**  | 合并请求被合并到目标分支（如`main`）时触发。 |\n| **Closed MR**             | 合并请求被关闭（未合并）时触发。             |\n\n（3）在Jenkins中选择Manage Jenkins-System-GitLab中，取消勾选`Enable authentication for '/project' end-point`，保存设置。\n\n（4）在GitLab的管理员账户中选择管理中心-设置-网络-外发请求中勾选`Allow requests to the local network from web hooks and services` ，保存配置。\n\n（5）在GitLab的项目仓库中选择设置-集成，输入webhook URL，选择默认配置，保存。在生成的Webhook右侧，选择Test-Push events，显示`Hook executed successfully: HTTP 200`，即为成功。\n\n#### 参数化构建\n\n（1）在源码中创建分支v1，并修改Jenkinsfile脚本中的分支名称，提交代码到gitlab。\n\n> 修改前： checkout scmGit(branches: [[name: '*/master']]\n>\n>  修改后：checkout scmGit(branches: [[name: '*/${branch}']]\n\n（2）在Jenkins的任务Pipeline-demo的配置界面，选择`This project is parameterized`-`String Parameter`，具体设置如下图。\n\n![img](Jenkins知识梳理/19.png)\n\n（3）保存设置，在任务Pipeline-demo界面选择`Build with Parameters`，输入分支的名称，点击Build，进行构建。\n\n#### 配置邮件服务器\n\n（1）安装Email Extension TemplateVersion插件\n\n（2）选择Manage Jenkins-Credentials，点击`全局`-`Add Credentials`，创建邮件服务器的密码凭证，账户为邮箱地址，密码为获取的邮箱授权码。\n\n（3）选择`System`，在`Jenkins Location``-系统管理员邮件地址`出输入注册邮箱，在`Extended E-mail Notification`处，输入注册邮箱和端口，点击`高级`，选择密码凭证，勾选`Use SSL`，在`Default user e-mail suffix`中邮箱后缀，`Default Content Type`选择`HTML (text/html)`，`Default Recipients`输入注册邮箱，在邮件通知部分输入SMTP服务器和用户默认邮件后缀，点击高级完善信息，勾选`通过发送测试邮件测试配置`测试邮件服务器。\n\n（4）构建邮件服务器模板\n\n在项目根目录下创建email.html模板\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"UTF-8\">\n    <title>${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志</title>\n\n</head>\n\n<body leftmargin=\"8\" marginwidth=\"0\" topmargin=\"8\" marginheight=\"4\"\n      offset=\"0\">\n<table width=\"95%\" cellpadding=\"0\" cellspacing=\"0\"\n       style=\"font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif\">\n    <tr>\n        <td>(本邮件是程序自动下发的，请勿回复！)</td>\n\n    </tr>\n\n    <tr>\n        <td><h2>\n                <font color=\"#0000FF\">构建结果 - ${BUILD_STATUS}</font>\n\n            </h2></td>\n\n    </tr>\n\n    <tr>\n        <td><br />\n            <b><font color=\"#0B610B\">构建信息</font></b>\n\n            <hr size=\"2\" width=\"100%\" align=\"center\" /></td>\n\n    </tr>\n\n    <tr>\n        <td>\n            <ul>\n                <li>项目名称&nbsp;：&nbsp;${PROJECT_NAME}</li>\n\n                <li>构建编号&nbsp;：&nbsp;第${BUILD_NUMBER}次构建</li>\n\n                <li>触发原因：&nbsp;${CAUSE}</li>\n\n                <li>构建日志：&nbsp;<a href=\"${BUILD_URL}console\">${BUILD_URL}console</a></li>\n\n                <li>构建&nbsp;&nbsp;Url&nbsp;：&nbsp;<a href=\"${BUILD_URL}\">${BUILD_URL}</a></li>\n\n                <li>工作目录&nbsp;：&nbsp;<a href=\"${PROJECT_URL}ws\">${PROJECT_URL}ws</a></li>\n\n                <li>项目&nbsp;&nbsp;Url&nbsp;：&nbsp;<a href=\"${PROJECT_URL}\">${PROJECT_URL}</a></li>\n\n            </ul>\n\n        </td>\n\n    </tr>\n\n    <tr>\n        <td><b><font color=\"#0B610B\">Changes Since Last\n                    Successful Build:</font></b>\n\n            <hr size=\"2\" width=\"100%\" align=\"center\" /></td>\n\n    </tr>\n\n    <tr>\n        <td>\n            <ul>\n                <li>历史变更记录 : <a href=\"${PROJECT_URL}changes\">${PROJECT_URL}changes</a></li>\n\n            </ul> ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format=\"Changes for Build #%n:<br />%c<br />\",showPaths=true,changesFormat=\"<pre>[%a]<br />%m</pre>\",pathFormat=\"&nbsp;&nbsp;&nbsp;&nbsp;%p\"}\n        </td>\n\n    </tr>\n\n    <tr>\n        <td><b>Failed Test Results</b>\n\n            <hr size=\"2\" width=\"100%\" align=\"center\" /></td>\n\n    </tr>\n\n    <tr>\n        <td><pre\n                    style=\"font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif\">$FAILED_TESTS</pre>\n\n            <br /></td>\n\n    </tr>\n\n    <tr>\n        <td><b><font color=\"#0B610B\">构建日志 (最后 100行):</font></b>\n\n            <hr size=\"2\" width=\"100%\" align=\"center\" /></td>\n\n    </tr>\n\n    <tr>\n        <td><textarea cols=\"80\" rows=\"30\" readonly=\"readonly\"\n                      style=\"font-family: Courier New\">${BUILD_LOG, maxLines=100}</textarea>\n\n        </td>\n\n    </tr>\n\n</table>\n\n</body>\n\n</html>\n```\n\n（5）修改Jenkinsfile文件，添加与stages同级的post部分，添加构建后发送邮件\n\n```groovy\npipeline {\n    agent any\n\n    stages {\n        stage('拉取代码') {\n            steps {\n                checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'fbf87557-40b6-468a-91c5-2cfa1b2e33e8', url: 'http://192.168.1.121:82/test/test_war_demo.git']])\n            }\n        }\n        stage('编译构建') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('发布项目') {\n            steps {\n                deploy adapters: [tomcat9(credentialsId: '4d9b76aa-078c-462b-81f1-33fe1d9c971c', path: '', url: 'http://192.168.1.125:8080/')], contextPath: null, war: 'target/*.war'\n            }\n        }\n    }\n    post {\n        always {\n            emailext(\n                subject: '构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!',\n                body: '${FILE,path=\"email.html\"}',\n                to: '邮箱地址1,邮箱地址2'\n            )\n        }\n    }\n}\n\n```\n\n### 安装SonarQube\n\nJenkins集成SonarQube进行代码审查。\n\n> SonarQube7.9开始不再支持mysql，SonarQube7.8安装支持mysql >=5.6 && <8.0\n\n（1）安装SonarQube前需要安装mysql数据库\n\n```shell\n#配置mysql5.7的yum源\nvi /etc/yum.repos.d/mysql-community.repo\n\n[mysql-connectors-community]\nname=MySQL Connectors Community\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-connectors-community-el7-$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql-2022\n\n[mysql-tools-community]\nname=MySQL Tools Community\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7-$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql-2022\n\n[mysql-5.7-community]\nname=MySQL 5.7 Community Server\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-5.7-community-el7-$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.mysql.com/RPM-GPG-KEY-mysql-2022\n\n\n#安装mysql\nyum -y install mysql-server\n# 查看版本\nmysql -V\nsystemctl start mysqld\nsystemctl enable mysqld\n# 查看MySQL的初始密码\ngrep \"password\" /var/log/mysqld.log\n# MySQL的安全性配置\nmysql_secure_installation\n#按引导配置完后，登录并创建数据库\nmysql -uroot -p\ncreate database sonar;\n#查看数据库\nshow databases;\n```\n\n（2）安装SonarQube\n\n```shell\n#配置系统相关参数\necho \"vm.max_map_count=524288\nfs.file-max=131072\" >> /etc/sysctl.conf\nsysctl -p\n\n#下载依赖工具\nyum install -y unzip\nwget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.8.zip\nmkdir -p /opt/sonar\nunzip -q sonarqube-7.8.zip -d /opt/sonar\n#创建用户\nuseradd sonar\npasswd sonar\n#修改sonarqube文件权限\nchown -R sonar:sonar /opt/sonar/sonarqube-7.8\n```\n\n（3）修改`conf`目录下的`sonar.properties`配置文件\n\n```shell\nsonar.jdbc.username=root\nsonar.jdbc.password=数据库密码\nsonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true&useConfigs=maxPerformance&useSSL=false\n```\n\n（4）修改`conf`目录下`wrapper.conf`的jdk-1.8的路径\n\n```shell\nwrapper.java.command=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.412.b08-1.el7_9.x86_64/bin/java\n```\n\n（5）进入`bin/linux-x86-64/`目录，使用sonar用户启动sonarqube\n\n```shell\nsu sonar\n./sonar.sh start\n#查看启动日志\ntail ../../logs/sonar.log\n```\n\n> 使用默认账户登录，账号密码都是admin\n\n（6）登录sonarqube后，选择 `Administration` - `Security`- `Global Permissions`，为admin勾选 `Administer System`**、**`Administer`**、**`Execute Analysis`、`Create` 权限。单击右上角头像下拉框，选择 `My Account` - `Security`，随意填写token名称，完成 Token 的创建。\n\n#### **配置Jenkins与sonarqube集成**\n\n（1）在Jenkins上安装`SonarQube Scanner`插件，安装完成后，选择`Manage Jenkins` -`Tools`-`SonarQube Scanner 安装`，点击`新增SonarQube Scanner` ，输入名称，版本选择`SonarQube Scanner 4.2.0.1873`，点击保存。\n\n> **SonarQube 7.8** 对应 **Sonar-Scanner 4.2**\n>\n> **SonarQube 8.9 LTS** 对应 **Sonar-Scanner 4.6**\n\n（2）在Jenkins中配置SonarQube的token凭证。选择类型为`Secret text`，输入token，点击创建。\n\n（3）在Jenkins中选择System-SonarQube installations，点击`Add SonarQube`，输入SonarQube的地址、选择创建的token凭证，保存集成SonarQube。\n\n#### **非流水线任务添加SonarQube代码审查**\n\n在freestyle_demo任务中选择配置-Build Steps，点击增加构建步骤，选择`Execute SonarQube Scanner`，选择JDK，输入分析内容，保存配置后，点击Build Now，等待任务构建完成。在SonarQube里查看代码分析结果。\n\n```json\nsonar.projectKey=websocketChat\nsonar.projectName=websocketChat\nsonar.projectVersion=1.0\nsonar.sources=.\nsonar.java.binaries=./target/classes\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.source=1.8\nsonar.java.target=1.8\nsonar.sourceEncoding=UTF-8\n```\n\n![img](Jenkins知识梳理/20.png)\n\n![img](Jenkins知识梳理/21.png)\n\n#### **流水线任务添加SonarQube代码审查**\n\n（1）在项目根目录下，创建sonar-project.properties，写入以下内容。\n\n```json\nsonar.projectKey=Pipeline-demo\nsonar.projectName=Pipeline-demo\nsonar.projectVersion=1.0\nsonar.sources=.\nsonar.java.binaries=./target/classes\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.source=1.8\nsonar.java.target=1.8\nsonar.sourceEncoding=UTF-8\n```\n\n（2）在Jenkinsfile的stages前，引入在Jenkins中安装的SonarQube Scanner作为预置环境变量，之后在拉取代码步骤后，添加审查代码的步骤。\n\n```groovy\npipeline {\n    agent any\n     environment {\n         //引入在Jenkins中安装的SonarQube Scanner\n        scannerHome = tool 'sonar-scanner-jenkins'\n            }\n    stages {\n        stage('拉取代码') {\n            steps {\n                checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'fbf87557-40b6-468a-91c5-2cfa1b2e33e8', url: 'http://192.168.1.121:82/test/test_war_demo.git']])\n            }\n        }\n        stage('审查代码') {\n            steps { \n                    //引入在Jenkins中配置的SonarQube服务器设置\n                withSonarQubeEnv('SonarQube-jenkins') {\n                    sh '${scannerHome}/bin/sonar-scanner'\n                    }\n            }\n        }\n        stage('编译构建') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('发布项目') {\n            steps {\n                deploy adapters: [tomcat9(credentialsId: '4d9b76aa-078c-462b-81f1-33fe1d9c971c', path: '', url: 'http://192.168.1.125:8080/')], contextPath: null, war: 'target/*.war'\n            }\n        }\n    }\n    post {\n        always {\n            emailext(\n                subject: '构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!',\n                body: '${FILE,path=\"email.html\"}',\n                to: '邮箱地址'\n            )\n        }\n    }\n}\n```\n\n（3）代码完成变更后，在流水线任务执行完成，在SonarQube里查看代码分析结果。\n\n![img](Jenkins知识梳理/22.png)\n\n## 应用案例\n\n> 该应用案例参考来源于b站up主，''涣沷a靑惷''，原文链接为： https://www.yuque.com/huanfqc/jenkins/jenkins。\n\n使用Jenkins+GitLab+Docker+SonarQube，在192.168.1.125上部署若依的springboot前后端分离项目。\n\n### 环境准备\n\n设置Jenkins服务器免密登录192.168.1.125\n\n```shell\nssh-copy-id -i /root/.ssh/id_rsa.pub root@192.168.1.125\n```\n\n#### 安装docker\n\n```shell\nbash <(curl -sSL https://linuxmirrors.cn/docker.sh)\n#修改镜像源和默认配置文件路径\nvi /etc/docker/daemon.json\n{\n  \"data-root\": \"/data/dockerData\",\n  \"registry-mirrors\": [\n        \"https://docker.1ms.run\",\n        \"https://docker.xuanyuan.me\"\n    ]\n}\n#启动docker\nsystemctl start docker\nsystemctl enable docker\n```\n\n#### 获取并修改源码配置\n\n（1）拉取若依项目源码到本地，并将ruoyi-ui目录与RuoYi-Vue项目放置在同一级\n\n```shell\ngit clone https://gitee.com/y_project/RuoYi-Vue.git\n```\n\n![img](Jenkins知识梳理/23.png)\n\n（2）修改`ruoyi-ui`目录中的`vue.config.js`文件，将`localhost`修改为部署主机的ip，即192.168.1.125。\n\n```json\n#修改前\nconst baseUrl = 'http://localhost:8080' // 后端接口\n#修改后\nconst baseUrl = 'http://192.168.1.125:8080' // 后端接口\n```\n\n（3）修改`RuoYi-Vue\\ruoyi-admin\\src\\main\\resources\\application.yml`文件，将redis配置的ip改为192.168.1.125。\n\n```\n  # redis 配置\n  redis:\n    # 地址\n    host: 192.168.1.125\n```\n\n（4）修改`RuoYi-Vue\\ruoyi-admin\\src\\main\\resources\\application-druid.yml`文件，将mysql的链接地址改为192.168.1.125。\n\n```\n # 主库数据源\n            master:\n                url: jdbc:mysql://192.168.1.125:3306/ry-vue?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=true&serverTimezone=GMT%2B8\n              \n```\n\n（5）在ruoyi-ui目录下，创建sonar-project.properties，写入以下内容，添加SonarQube代码审查\n\n```groovy\nsonar.projectKey=ruoyi-ui\nsonar.projectName=ruoyi-ui\nsonar.projectVersion=1.0\nsonar.sources=.\nsonar.sourceEncoding=UTF-8\n```\n\n（6）在ruoyi-ui目录下创建前端项目的Jenkinsfile文件\n\n```groovy\npipeline {\n    agent any\n     environment {\n         //引入在Jenkins中安装的SonarQube Scanner\n        scannerHome = tool 'sonar-scanner-jenkins'\n            }\n    stages {\n        stage('拉取代码') {\n            steps {\n                checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'fbf87557-40b6-468a-91c5-2cfa1b2e33e8', url: 'http://192.168.1.121:82/test/ruoyi-ui.git']])\n            }\n        }\n        stage('审查代码') {\n            steps { \n                    //引入在Jenkins中配置的SonarQube服务器设置\n                withSonarQubeEnv('SonarQube-jenkins') {\n                    sh '${scannerHome}/bin/sonar-scanner'\n                    }\n            }\n        }\n               stage('打包，部署网站') {\n\t\t  steps {\n\t\t    script {\n\t\t\t    nodejs(nodeJSInstallationName: 'nodejs14') {\n                              sh '''\n                npm install\n                # 构建生产环境\n                npm run build:prod\n            '''\n               }\n\t\t\t    // 将构建产物部署到 Nginx 容器挂载的目录\n          sh '''\n            \n            # 将整个 dist 目录复制到挂载目录\n            scp -r dist 192.168.1.125:/data/ruoyi/nginx/html/\n            ssh 192.168.1.125 docker restart nginx\n        '''\n\t\t\t   \n\t\t\t}\n\t\t  }\n       \n        }\n\n    }\n}\n\n```\n\n7）在RuoYi-Vue目录下，创建sonar-project.properties，写入以下内容，添加SonarQube代码审查\n\n```groovy\nsonar.projectKey=ruoyi-api\nsonar.projectName=ruoyi-api\nsonar.projectVersion=1.0\nsonar.sources=.\nsonar.java.binaries=./ruoyi-admin/target/classes\nsonar.exclusions=**/test/**,**/target/**\nsonar.java.source=1.8\nsonar.java.target=1.8\nsonar.sourceEncoding=UTF-8\n```\n\n（8）在RuoYi-Vue目录下创建后端项目的Jenkinsfile文件\n\n```groovy\npipeline {\n    agent any\n    environment {\n         //引入在Jenkins中安装的SonarQube Scanner\n        scannerHome = tool 'sonar-scanner-jenkins'\n            }\n    stages {\n        stage('拉取代码') {\n            steps {\n                checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'fbf87557-40b6-468a-91c5-2cfa1b2e33e8', url: 'http://192.168.1.121:82/test/ruoyi-api.git']])\n            }\n        }\n        stage('编译构建') {\n            steps {\n                sh 'mvn clean package'\n            }\n        }\n        stage('审查代码') {\n            steps { \n                    //引入在Jenkins中配置的SonarQube服务器设置\n                withSonarQubeEnv('SonarQube-jenkins') {\n                    sh '${scannerHome}/bin/sonar-scanner'\n                    }\n            }\n        }\n        stage('发布项目') {\n            steps {\n                script {\n                    sh '''\n                       ssh 192.168.1.125  \"mkdir -p /data/ruoyi-api\"\n                        scp ruoyi-admin/target/*.jar  192.168.1.125:/data/ruoyi-api/ruoyi.jar\n                    '''\n                    // 创建Dockerfile\n                    sh '''\n                             ssh 192.168.1.125 'cat > /data/ruoyi-api/Dockerfile <<-EOF\n\tFROM openjdk:8\n\tMAINTAINER xie\n\tVOLUME /tmp\n\tADD ruoyi.jar ruoyi.jar\n\tENTRYPOINT [\"java\",\"-jar\",\"/ruoyi.jar\"]\n\tEXPOSE 8080\n\tEOF'\n                    '''\n\n                    // 构建镜像\n                    sh '''\n                        ssh 192.168.1.125 \"docker build -t ruoyi:1.0 /data/ruoyi-api\"\n                    '''\n\n                    // 停止并删除现有的容器\n                    sh '''\n                        EXISTING_CONTAINER=$(ssh 192.168.1.125 \"docker ps -a -q -f name=ruoyi\")\n                        if [ -n \"$EXISTING_CONTAINER\" ]; then\n                            echo \"容器 'ruoyi' 已存在，停止并删除旧容器...\"\n                           ssh 192.168.1.125 \"docker rm -f $EXISTING_CONTAINER\"\n                        fi\n                    '''\n\n                    // 运行新的容器\n                    sh '''\n                       ssh 192.168.1.125 \"docker run -d --name ruoyi -p 8080:8080 ruoyi:1.0\"\n                    '''\n                }\n            }\n        }\n    }\n}\n```\n\n#### 部署项目所需容器\n\n该项目中要用到nginx、mysql和redis，需提前部署好容器。对应容器版本为nginx:1.18.0、mysql:8.0.19、redis:6.0.8 \n\n（1）创建所需目录\n\n```shell\nmkdir -p /data/ruoyi\nmkdir -p /data/ruoyi/nginx/conf\nmkdir -p /data/ruoyi/mysql/db\n```\n\n（2）创建nginx临时容器，修改配置文件\n\n```shell\ndocker run --rm -v /data/ruoyi/nginx/conf:/backup nginx:1.18.0 \\\n  sh -c \"cp -r /etc/nginx/. /backup\"\n```\n\n修改`/data/ruoyi/nginx/conf/conf.d`中的default.conf文件\n\n```yaml\nserver {\n    listen 80;\n\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header REMOTE-HOST $remote_addr;\n\n    server_name localhost;\n\n    location / {\n        root /usr/share/nginx/html/dist;\n        index index.html index.htm;\n        try_files $uri /index.html;\n    }\n\n    location /prod-api/ {\n        proxy_set_header Host $http_host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header REMOTE-HOST $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_pass http://192.168.1.125:8080/;   #注意修改ip\n    }\n\n    if ($request_uri ~ \"/actuator\") {\n        return 403;\n    }\n}\n\n```\n\n（3）将`RuoYi-Vue\\sql`中的sql文件，拷贝到192.168.1.125主机的`/data/ruoyi/mysql/db`目录\n\n（4）撰写yml，使用docker compose编排部署容器\n\n```yaml\n#撰写yml文件\nvi ruoyi.yml\n\n\nservices:\n  nginx:\n    image: nginx:1.18.0\n    container_name: nginx\n    restart: always\n    volumes:\n      - /data/ruoyi/nginx/conf:/etc/nginx\n      - /data/ruoyi/nginx/logs:/var/log/nginx\n      - /data/ruoyi/nginx/html:/usr/share/nginx/html\n    environment:\n      TZ: \"Asia/Shanghai\"\n    ports:\n      - \"80:80\"\n    networks:\n      ruoyi:\n        ipv4_address: 172.20.112.11\n  mysql:\n    container_name: mysql\n    image: mysql:8.0.19\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: \"password\"\n      MYSQL_ALLOW_EMPTY_PASSWORD: \"no\"\n      MYSQL_DATABASE: \"ry-vue\"\n      TZ: \"Asia/Shanghai\"\n    ports:\n      - \"3306:3306\"\n    volumes:\n      - /data/ruoyi/mysql/db:/var/lib/mysql\n      - /data/ruoyi/mysql/conf:/etc/my.cnf\n      - /data/ruoyi/mysql/init:/docker-entrypoint-initdb.d\n    command: --default-authentication-plugin=mysql_native_password\n    networks:\n      ruoyi:\n        ipv4_address: 172.20.112.12\n  redis:\n    container_name: redis\n    image: redis:6.0.8\n    restart: always\n    environment:\n      TZ: \"Asia/Shanghai\"\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - /data/ruoyi/redis/conf:/etc/redis/redis.conf\n      - /data/ruoyi/redis/data:/data\n    command: redis-server /etc/redis/redis.conf\n    networks:\n      ruoyi:\n        ipv4_address: 172.20.112.13\n\n\nnetworks:\n ruoyi:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.112.0/24\n```\n\n（5）执行命令创建容器\n\n```shell\ndocker compose -f ruoyi.yml up -d\n#查看容器运行状态\ndocker ps\n#导入数据库\ndocker exec -it mysql bash\nmysql -u root -ppassword ry-vue < /var/lib/mysql/ry_20250417.sql\nmysql -u root -ppassword ry-vue < /var/lib/mysql/quartz.sql\n```\n\n#### 上传代码到GitLab\n\n（1）创建ruoyi-ui和ruoyi-api两个仓库分别存放前端和后端代码\n\n（2）在ruoyi-ui目录下，打开cmd，输入命令上传代码\n\n```shell\ngit init\ngit remote add origin http://192.168.1.121:82/test/ruoyi-ui.git\n#添加该目录下的git权限配置\ngit config  user.name \"test1\"\ngit config  user.email \"test1@gitlab\"\n#继续上传\ngit add .\ngit commit -m \"ruoyi-ui\"\ngit push -u origin master\n```\n\n（3）在`RuoYi-Vue`目录下，打开cmd，输入命令上传代码\n\n```shell\ngit init\ngit remote add origin http://192.168.1.121:82/test/ruoyi-api.git\n#添加该目录下的git权限配置\ngit config  user.name \"test1\"\ngit config  user.email \"test1@gitlab\"\n#继续上传\ngit add .\ngit commit -m \"ruoyi-api\"\ngit push -u origin master\n```\n\n#### 配置Jenkins流水线\n\n（1）安装nodejs和NodeJS插件\n\n在jenkins服务器上安装nodejs\n\n```shell\nwget https://nodejs.org/dist/latest-fermium/node-v14.21.3-linux-x64.tar.gz\ntar -vxzf node-v14.21.3-linux-x64.tar.gz  -C /opt\nmv /opt/node-v14.21.3-linux-x64 /opt/nodejs\n#配置环境变量\necho \"\nexport NODE_HOME=/opt/nodejs\nexport PATH=$NODE_HOME/bin:$PATH\" >> /etc/profile\n#变量生效\n source /etc/profile\n#查看版版本\nnode -v\nnpm -v\n#配置镜像源\nnpm config set registry https://registry.npmmirror.com\nnpm config set sass_binary_site=https://npm.taobao.org/mirrors/node-sass\n```\n\n（2）配置NodeJS设置\n\n在`Manage Jenkins` -`Tools`-NodeJS 安装中，点击`新增NodeJS`，填写nodejs的名称为`nodejs14`,输入在服务器安装的nodejs路径`/opt/nodejs`，保存应用。\n\n（3）创建`ruoyi-ui`流水线任务，在流水线-定义中选择`Pipeline script from SCM`，SCM选择`Git`，填写ruoyi-ui的git仓库地址和登录凭证，其他选项保持默认，保存应用。\n\n（4）点击`Build Now`，等待任务执行完成。在浏览器中访问`http://192.168.1.125/`，查看前端部署情况。\n\n![img](Jenkins知识梳理/24.png)\n\n（5）创建`ruoyi-api`流水线任务，在流水线-定义中选择`Pipeline script from SCM`，SCM选择`Git`，填写ruoyi-api的git仓库地址和登录凭证，其他选项保持默认，保存应用。\n\n（6）点击`Build Now`，等待任务执行完成。在浏览器中访问`http://192.168.1.125/`，可正常登录，即为成功。\n\n![img](Jenkins知识梳理/25.png)\n\n（7）访问sonarQube，分别查看前后端代码的具体质量审查情况。\n\n![img](Jenkins知识梳理/26.png)\n\n\n\n\n\n> 本篇知识来源于B站视频BV1pF411Y7tq\n","tags":["Linux运维","SonarQube","自动化运维","CI/CD","知识梳理","Jenkins","GitLab"],"categories":["梳理总结","知识梳理","自动化运维","Jenkins","GitLab","SonarQube"]},{"title":"Ansible最新版安装","url":"/yyg/10ac2e4/","content":"## pip方式安装\n\n### 基础环境准备\n\n（1）安装软件依赖\n\n```shell\nyum install -y zlib zlib-dev zlib-devel  sqlite-devel openssl-devel  bzip2-devel libffi libffi-devel gcc gcc-c++ perl perl-core perl-CPAN perl-IPC-Cmd make tk-devel readline-devel libpcap-devel gdbm-devel  xz-devel\n```\n\n（2）下载并安装新版openssl\n\n```shell\nwget https://github.com/openssl/openssl/releases/download/openssl-3.5.0/openssl-3.5.0.tar.gz \ntar -zxvf openssl-3.5.0.tar.gz\ncd openssl-3.5.0\n#编译安装\n./config --prefix=/usr/local/openssl-3.5.0 --libdir=lib --openssldir=/etc/ssl\nmake -j1 depend && make -j8 && make install_sw\n```\n\n（3）下载python3安装包\n\n```shell\ncurl -O https://mirrors.huaweicloud.com/python/3.13.3/Python-3.13.3.tgz\ntar -zxvf Python-3.13.3.tgz\ncd Python-3.13.3\n./configure --with-openssl=/usr/local/openssl-3.5.0 --with-openssl-rpath=auto --prefix=/usr/local/python3.13\nmake -j8 && make altinstall\n#设置软链接\nln -s /usr/local/python3.13/bin/python3.13 /usr/bin/python3\nln -s /usr/local/python3.13/bin/pip3.13 /usr/bin/pip3\n\n##验证版本\npython3 --version\npip3 --version\n```\n\n### 安装Ansible\n\n```shell\npython3 -m pip install ansible\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' >> ~/.bashrc\nsource ~/.bashrc\n#查看版本\nansible --version\n```\n\n创建全局配置文件`ansible.cfg`\n\n```\nmkdir -p /etc/ansible\nvi /etc/ansible/ansible.cfg\n[defaults]\n#主机清单存在文件\ninventory=/etc/ansible/hosts\n#使用的 Python解释器路径\ninterpreter_python = /usr/bin/python3\n#查看配置文件生效\nansible --version\n```\n\n> [root@124 ~]# ansible --version\n> ansible [core 2.18.4]\n>   config file = **/etc/ansible/ansible.cfg**\n>   configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n>   ansible python module location = /root/.local/lib/python3.13/site-packages/ansible\n>   ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections\n>   executable location = /root/.local/bin/ansible\n>   python version = 3.13.3 (main, Apr 12 2025, 22:15:40) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] (/usr/bin/python3)\n>   jinja version = 3.1.6\n>   libyaml = True\n\n## 容器方式安装\n\n### 使用官方社区容器\n\n `community-ee-minimal` 镜像：只包含 `ansible-core`\n\n`community-ee-base` 镜像：包含`nsible-core`、`ansible.posix`、`ansible.utils`、`ansible.windows`的多个基础集合\n\n本文中使用`community-ee-base` 镜像进行安装。\n\n> 容器方式安装适用于被控主机python版本为3.13及以上的情况\n\n（1）基本环境准备\n\n```shell\nmkdir  -p /data/code\nmkdir  -p /data/ansible\n#创建全局配置文件\nvi /data/ansible/ansible.cfg\n[defaults]\n#主机清单存在文件\ninventory=/etc/ansible/hosts\ninterpreter_python = /usr/bin/python3\n\n#创建配置文件\nvi /data/code/ansible.cfg\n[defaults]\n#主机清单存在文件\ninventory=/data/code/hosts\ninterpreter_python = /usr/bin/python3\n\n#创建主机清单文件\nvi /data/code/hosts\n[webservers]\n192.168.1.124\n192.168.1.125\n```\n\n在安装ansible容器的主机上设置免密登录，并将密钥文件传至被控主机\n\n```\nssh-keygen \nssh-copy-id -i ~/.ssh/id_rsa.pub root@受控主机ip\n```\n\n（2）安装`ghcr.io/ansible-community/community-ee-base`容器\n\n- 方式一：直接使用docker命令：\n\n```shell\n docker run -itd  --name ansible --user \"0\" -v /root/.ssh:/root/.ssh -v /data/ansible:/etc/ansible -v /data/code:/data/code ghcr.io/ansible-community/community-ee-base\n```\n\n- 方式二：使用docker compose安装\n\n```yaml\n#创建yml文件\nvi ansible.yml\n\nservices:\n  ansible:\n    image: ghcr.io/ansible-community/community-ee-base\n    container_name: ansible\n    restart: always\n    tty: true\n    stdin_open: true\n    volumes:\n      - /root/.ssh:/root/.ssh\n      - /data/ansible:/etc/ansible\n      - /data/code:/data/code\n    user: \"0\"\n    networks:\n      ansible-net:\n        ipv4_address: 172.20.17.11\n\nnetworks:\n ansible-net:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.17.0/24\n    \n#安装\ndocker compose -f ansible.yml up -d\n```\n\n（3）安装成功后，进入容器查看\n\n```\ndocker exec -it ansible bash\n```\n\n查看版本\n\n```\nansible --version\n```\n\n> [root@123 runner]# ansible --version\n> ansible [core 2.18.3]\n>   config file = /etc/ansible/ansible.cfg\n>   configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n>   ansible python module location = /usr/local/lib/python3.13/site-packages/ansible\n>   ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections\n>   executable location = /usr/local/bin/ansible\n>   python version = 3.13.2 (main, Feb  4 2025, 00:00:00) [GCC 14.2.1 20250110 (Red Hat 14.2.1-7)] (/usr/bin/python3)\n>   jinja version = 3.1.5\n>   libyaml = True\n\n执行ansible命令测试\n\n```\nansible all -m ping\n```\n\n> [root@123 code]# ansible all -m ping\n> 192.168.1.125 | SUCCESS => {\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.124 | SUCCESS => {\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n\n### 构建自用Ansible镜像\n\n（1）撰写DockerFile文件\n\n```\nvi DockerFile\n\nFROM centos:7\nRUN curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo && yum install epel-release -y && yum install ansible -y\n\n#创建ansible镜像\ndocker build -f DockerFile -t ansible:1.0 --load .\n```\n\n（2）基本环境准备\n\n```shell\nmkdir  -p /data/code\nmkdir  -p /data/ansible\n#创建全局配置文件\nvi /data/ansible/ansible.cfg\n[defaults]\n#主机清单存在文件\ninventory=/etc/ansible/hosts\ninterpreter_python = /usr/bin/python\n\n#创建配置文件\nvi /data/code/ansible.cfg\n[defaults]\n#主机清单存在文件\ninventory=/data/code/hosts\ninterpreter_python = /usr/bin/python\n\n#创建主机清单文件\nvi /data/code/hosts\n[webservers]\n192.168.1.122\n192.168.1.123\n192.168.1.124\n```\n\n（3）在安装ansible容器的主机上设置免密登录，并将密钥文件传至被控主机\n\n```\nssh-keygen \nssh-copy-id -i ~/.ssh/id_rsa.pub root@受控主机ip\n```\n\n（4）安装容器\n\n- 方式一：直接使用docker命令：\n\n```shell\n docker run -itd  --name ansible -v /root/.ssh:/root/.ssh -v /data/ansible:/etc/ansible -v /data/code:/data/code ansible:1.0\n```\n\n- 方式二：使用docker compose安装\n\n```yaml\n#创建yml文件\nvi ansible.yml\n\nservices:\n  ansible:\n    image: ansible:1.0\n    container_name: ansible\n    restart: always\n    tty: true\n    stdin_open: true\n    volumes:\n      - /root/.ssh:/root/.ssh\n      - /data/ansible:/etc/ansible\n      - /data/code:/data/code\n    user: \"0\"\n    networks:\n      ansible-net:\n        ipv4_address: 172.20.17.11\n\nnetworks:\n ansible-net:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.17.0/24\n    \n    \n#安装\ndocker compose -f ansible.yml up -d\n```\n\n（5）安装成功后，进入容器查看\n\n```\ndocker exec -it ansible bash\n```\n\n查看版本\n\n```shell\nansible --version\n```\n\n> [root@c7668aa8c849 ~]# ansible --version\n> ansible 2.9.27\n>   config file = /etc/ansible/ansible.cfg\n>   configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\n>   ansible python module location = /usr/lib/python2.7/site-packages/ansible\n>   executable location = /usr/bin/ansible\n>   python version = 2.7.5 (default, Oct 14 2020, 14:45:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n\n执行ansible命令测试\n\n```\nansible all -m ping\n```\n\n> [root@c7668aa8c849 code]# ansible all -m ping\n> 192.168.1.122 | SUCCESS => {\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.124 | SUCCESS => {\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.123 | SUCCESS => {\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }","tags":["Linux运维","自动化运维","CI/CD","Ansible"],"categories":["自动化运维","Ansible"]},{"title":"Ansible知识梳理","url":"/yyg/e80721f/","content":"{% folding cyan,🔜什么是Ansible？🔚 %}\n\nAnsible 是一款 IT 自动化工具。主要应用场景有配置系统、软件部署、持续发布及不停服平滑滚动更新的高级任务编排。\n\nAnsible 提供开源自动化，可降低复杂性并在任何地方运行。使用 Ansible 可以自动执行几乎任何任务。\n\n{% endfolding %}\n\n## 架构\n\n> `Ansible`：Ansible核心程序。\n> `HostInventory`：记录由Ansible管理的主机信息，包括端口、密码、ip等。\n> `Playbooks`：“剧本”YAML格式文件，多个任务定义在一个文件中，定义主机需要调用哪些模块来完成的功能。\n> `CoreModules`：**核心模块**，主要操作是通过调用核心模块来完成管理任务。\n> `CustomModules`：自定义模块，完成核心模块无法完成的功能，支持多种语言。\n> `ConnectionPlugins`：连接插件，Ansible和Host通信使用\n\n![image-20250331194718453](Ansible知识梳理/1.png)\n\n## Ansible重要概念\n\n### 控制节点\n\n控制节点是运行 Ansible  Ansible CLI 工具 的机器。它负责与受控节点（被管理节点）进行通信，发出命令并执行任务。\n\n### 受控节点（被管理节点）\n\n也称为“主机”，这些是用 Ansible 管理的目标设备。Ansible控制节点会在这些机器上执行任务。受控节点不需要安装任何代理。\n\n### 库存 (Inventory)\n\n包含要管理的目标主机或节点的清单。它是 Ansible 用来定义目标主机信息的配置文件，支持静态或动态的主机清单。它还用于分配组，这既允许在剧集中选择节点，也允许批量分配变量。\n\n### 剧本（Playbook）\n\nPlaybook 是 Ansible 配置管理的核心部分，是一组用于自动化执行任务的 YAML 文件。Playbook 可以执行多个步骤（例如安装软件、启动服务等），并定义执行顺序、任务等。\n\n### 模块 (Modules)\n\n模块是 Ansible 中执行实际任务的单元。它们是高度可复用的代码块，负责执行特定的操作（如安装软件包、配置文件等）。\n\n### 角色 (Roles)\n\n角色是 Playbook 的一种组织方式，用于把多个任务、文件、模板、变量等组织成一个完整的功能模块。角色帮助将复杂的配置拆分成可复用的组件。可在剧集内部使用的可重用 Ansible 内容。\n\n### 任务 (Tasks)\n\n任务是 Playbook 中定义的具体操作。每个任务通常会调用一个 Ansible 模块来执行特定的操作。是ad-hoc 更适合临时执行命令的执行场景。\n\n### 插件\n\n扩展 Ansible 核心功能的代码片段。插件可以控制您如何连接到被管理节点（连接插件）、操作数据（过滤器插件），甚至控制在控制台中显示的内容（回调插件）。\n\n### 集合\n\nAnsible 内容的分发格式，可以包含剧本、角色、模块和插件。\n\n## Ansible任务执行模式\n\nAnsible 系统由控制主机对被管节点的操作方式可分为两类，即`ad-hoc`和`playbook`：\n\n- ad-hoc模式(点对点模式)\n  　　使用单个模块，支持批量执行单条命令。ad-hoc 命令是一种可以快速输入的命令，而且不需要保存起来的命令。\n- playbook模式(剧本模式)\n  　　是Ansible主要管理方式，也是Ansible功能强大的关键所在。**playbook通过多个task集合完成一类功能**。可以简单地把playbook理解为通过组合多条ad-hoc操作的配置文件。\n\n## Ansible安装（centos 7）\n\n修改系统镜像源为阿里云镜像源\n\n```shell\ncd /etc/yum.repos.d\nmv CentOS-Base.repo CentOS-Base.repo.bak\ncurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n```\n\n### 方式一：使用yum安装\n\n安装`epel-release`包与`Ansible`\n\n```shell\nyum install epel-release -y\nyum install ansible -y\n#查看版本\n ansible --version\n```\n\n### 方式二：使用pip安装\n\n```shell\nyum install python3 python3-devel python3-pip -y\npython3 -m pip install ansible\n#查看版本\n ansible --version\n```\n\n## Ansible 配置文件\n\n配置文件读取的优先级顺序如下：\n\n1. 检查环境变量`ANSIBLE_CONFIG`指向的路径文件(export ANSIBLE_CONFIG=xxxx)；\n2. 检查当前项目目录下的ansible.cfg配置文件；\n3. `~/.ansible.cfg`，检查当前目录下的ansible.cfg配置文件；\n4. `/etc/ansible/ansible.cfg`检查etc目录的配置文件。\n\n## Ansible免密登录\n\n### root用户\n\n控制节点设置ssh免密登录，并将密钥传至受控节点上。\n\n```shell\nssh-keygen -t dsa -f ~/.ssh/id_dsa -P \"\"\nssh-copy-id -i ~/.ssh/id_dsa.pub root@受控主机ip\n```\n\n### 普通用户\n\n控制节点和被控节点都要有普通用户，此处以`test`用户为例。\n\n```shell\n#添加test用户，并设置密码\nuseradd test\npasswd test\n```\n\n在控制节点设置ssh免密登录，并将密钥传至受控节点。\n\n```shell\nssh-keygen -t dsa -f ~/.ssh/id_dsa -P \"\"\nssh-copy-id -i ~/.ssh/id_dsa.pub test@受控主机ip\n```\n\n修改所有主机的`test`用户的sudo权限。\n\n```shell\nvisudo\ntest    ALL=(ALL)       NOPASSWD: ALL\n```\n\n修改控制节点的配置文件`/etc/ansible/ansible.cfg`，设置普通用户权限提升。\n\n```shell\nvi /etc/ansible/ansible.cfg\n#将以下内容注释取消\n[privilege_escalation]\nbecome=True   #启用权限提升\nbecome_method=sudo  #通过 sudo 提升到 root 用户的权限\nbecome_user=root  #作为 root 用户执行任务\nbecome_ask_pass=False  #禁用密码提示\n```\n\n## Ansible Ad-Hoc与常用模块\n\n### Ad-Hoc\n\nad-hoc即为命令行模式，执行完即结束。\n\n**命令语法结构：**\n\n`ansible 'groups' -m command -a 'df -h'`,其含义如下图：\n\n![image-20250407162815663](Ansible知识梳理/2.png)\n\n**执行过程：**\n\n1. 加载配置文件，默认`/etc/ansible/ansible.cfg`;\n2. 查找对应的主机配置文件，找到要执行的主机或者组：\n3. 加载对应的模块文件，如`command`;\n4. 通过`ansible`将模块或命令生成对应的临时`py`文件，并将该文件传输至远程服务器对应执行用户`$HOME/.ansible/tmp/ansible-tmp-number/XXX.PY`;\n5. 远程主机执行该文件；\n6. 执行并返回结果；\n7. 删除临时`py`文件，`sleep 0`退出；\n\n**执行状态：**\n\n使用ad-hoc执行一次远程命令，注意观察返回结果的颜色：\n\n- 绿色：代表受控节点没有被修改\n- 黄色：代表受控节点发现变更\n- 红色：代表出现了故障，注意查看提示\n\n### 常用模块\n\n#### command模块\n\n**基本格式和常用参数**\n\n```shell\nansible '组/IP' -m command -a '[参数] 命令'\n```\n\n| 参数      | 选项                 | 含义                 |\n|:------- |:------------------ |:------------------ |\n| chdir   | chdir /opt         | 执行ansible时，切换到指定目录 |\n| creates | creates /data/file | 如果文件存在，则跳过执行       |\n| removes | removes /data/file | 如果文件存在，则执行         |\n\n> [root@121 ~]# ansible webservers -m command -a 'ls /root'                      \n> \n> 192.168.1.122 | CHANGED | rc=0 >>\n> anaconda-ks.cfg\n> test.txt\n> 192.168.1.121 | CHANGED | rc=0 >>\n> anaconda-ks.cfg\n> test\n> \n> [root@121 ~]# ansible webservers -m **command** -a '**creates**=/root/test.txt hostname'\n> 192.168.1.122 | SUCCESS | rc=0 >>\n> skipped, since /root/test.txt exists\n> 192.168.1.121 | CHANGED | rc=0 >>\n> 121\n> [root@121 ~]# ansible webservers -m **command** -a '**removes**=/root/test.txt hostname'\n> 192.168.1.122 | CHANGED | rc=0 >>\n> 122\n> 192.168.1.121 | SUCCESS | rc=0 >>\n> skipped, since /root/test.txt does not exist\n> [root@121 ~]#\n\n<mark>shell模块支持管道符，command模块不支持</mark>\n\n#### shell模块\n\n**基本格式和常用参数**\n\n```shell\nansible '组/IP/all' -m shell -a '[参数] 命令'\n```\n\n> ansible webservers -m shell -a 'ps aux | grep sshd'\n\n#### yum/apt 模块\n\n| 常用参数                     | 功能                      |\n| ---------------------------- | ------------------------- |\n| name                         | 需要安装的服务名          |\n| state=present(缺省值)/absent | 状态，abasent表示卸载服务 |\n\n> #安装服务\n>\n> ansible webservers -m yum -a 'name=httpd'\t\t\t\n>\n> #卸载服务\n>\n> ansible webservers -m yum -a 'name=httpd state=absent'\t\t\t\t\n\n#### copy模块\n\n**基本格式和常用参数**\n\n```shell\nansible < > -m copy -a 'src=   dest=   [owner= ] [mode=]   '\n```\n\n| 常用参数 | 功能                                 | 注意事项                                                   |\n| -------- | ------------------------------------ | ---------------------------------------------------------- |\n| src      | 指定源文件的路径（支持目录或文件）   | 若源为目录，目标也需为目录                                 |\n| dest     | 指定目标文件的位置（必须为绝对路径） | 1. 若源为目录，目标需为目录2. 目标文件已存在时会覆盖原内容 |\n| mode     | 设置目标文件的权限                   |                                                            |\n| owner    | 设置目标文件的属主                   |                                                            |\n| group    | 设置目标文件的属组                   |                                                            |\n| content  | 直接指定目标文件的内容               | 不可与 `src` 参数同时使用                                  |\n\n> ansible dbservers -m copy -a 'src=/etc/fstab dest=/opt/fstab.bak owner=root mode=640'\n> ansible dbservers -a 'ls -l /opt'\n> ansible dbservers -a 'cat /opt/fstab.bak'\n\n#### systemd 模块\n\n| 常用参数      | 功能                                                         |\n| ------------- | ------------------------------------------------------------ |\n| name          | 指定需要控制的服务名称                                       |\n| state         | 指定服务状态，可选值：`stopped`、`started`、`reloaded`、`restarted`、`status` |\n| enabled       | 设置服务是否开机启动，`yes` 为启动，`no` 为不启动            |\n| daemon_reload | 设为 `yes` 时重启 systemd 服务，使 unit 文件生效             |\n\n>  ansible webservers -m systemd -a 'name=firewalld state=started enabled=yes'\n\n#### file模块\n\n**基本格式和常用参数**\n\n```shell\nansible < > -m file -a ''\n```\n\n| 常用参数 |功能                                                         | 备注                          |\n| :------- | ------------------------------------------------------------ | -------------------------------- |\n| path     | 指定远程服务器的路径                                         | 也可写成 `dest` 或 `name`        |\n| state    | 定义操作类型：- `directory`：创建目录- `touch`：创建文件- `link`：创建软链接- `hard`：创建硬链接- `absent`：删除目录/文件/链接 |                                  |\n| mode     | 设置文件/目录权限                                            | 默认值：- 文件 `644`- 目录 `755` |\n| owner    | 设置属主                                                     | 默认 `root`                      |\n| group    | 设置属组                                                     | 默认 `root`                      |\n| recurse  | 递归修改                                                     | yes或no |\n| src      | 目标主机上的源文件（与 `copy` 模块不同）                     |                                  |\n\n> #创建/data目录，授权test属组\n>\n> ansible  webservers  -m file -a 'owner=test group=test mode=644 path=/data state=directory recurse=yes '\t\t\t\n\n#### group模块\n\n**基本格式和常用参数**\n\n```shell\nansible <组/IP/all> -m group -a ' '\n```\n\n| **参数** | **功能**                   | **可选值/说明**                       |\n| -------- | -------------------------- | ------------------------------------- |\n| name     | 指定用户名（必选参数）     |                                       |\n| state    | 控制账号的创建或删除       | `present`：创建账号`absent`：删除账号 |\n| system   | 指定是否为系统账号         | `yes`：系统账号`no`：普通账号         |\n| gid      | 指定组ID（修改用户所属组） | 数值格式（如 `1001`）                 |\n\n> ansible  webservers  -m group -a 'name=www gid=666 state=present '\n\n#### user模块\n\n**基本格式和常用参数**\n\n```shell\nansible <组/IP/all> -m user -a ' '\n```\n\n| **参数**      | **功能**                                         | **可选值/说明**                                             |\n| ------------- | ------------------------------------------------ | ----------------------------------------------------------- |\n| `name`        | 指定用户名（必选）                               |                                                             |\n| `state`       | 控制账号的创建或删除                             | `present`：创建`absent`：删除                               |\n| `system`      | 指定是否为系统账号                               | `yes`：系统账号（如 `mysql`）`no`：普通账号                 |\n| `uid`         | 指定用户UID                                      | 数值（如 `1001`），需唯一                                   |\n| `group`       | 指定用户基本组                                   | 组名或GID（需提前存在）                                     |\n| `groups`      | 指定用户所属附加组                               | 组名列表（如 `groups=wheel,admin`）                         |\n| `shell`       | 设置用户默认shell                                | 路径（如 `/bin/bash`）                                      |\n| `create_home` | 是否创建家目录                                   | `yes`：自动创建`no`：不创建（默认行为可能依赖系统）         |\n| `password`    | 设置用户密码                                     | **建议使用加密后的字符串**（如 `password=\"$6$加密字符串\"`） |\n| `remove`      | 删除用户时是否同时删除家目录（需`state=absent`） | `yes`：删除家目录`no`：保留                                 |\n\n1、 创建www用户，指定uid666,基本组www\n\n```shell\nansible webservers -m user -a 'name=www uid=666 group=www shell=/sbin/nologin create home=no'\n```\n\n2、 创建db用户，基本组是root,附加组，adm,sys\n\n```shell\nansible webservers -m user -a 'name=db group=root groups=adm,sys append=yes shell=/bin/bash create_home=yes'\n```\n\n3、 创建一个ddd用户，密码123，需要正常登录系统\n\n（1）密码加密\n\n```shell\nansible localhost -m debug -a \"msg={{ '123' | password_hash('sha512', 'salt123') }}\"\n```\n\n| 参数                   | 作用                     | 注意事项                                                     |\n| ---------------------- | ------------------------ | ------------------------------------------------------------ |\n| `sha512`               | 指定哈希算法（推荐）     | 支持 `sha256`/`md5` 等，但 `sha512` 更安全 2                 |\n| `salt123`              | 自定义盐值               | 盐值需为字母/数字，含 `-` 或 `_` 会导致哈希失败（返回 `*0`） 5 |\n| `password_hash` 过滤器 | 将明文密码转为加密字符串 | 需通过 `{{ }}` 调用且与变量用 `                              |\n\n> localhost | SUCCESS => {\n>     \"msg\": \"**$6$salt123$c806NQq6Oqk29MjZdmHxTw7sK3BB1K498o7sZC47UiwQmjx5NJyi5ZhWWGngXf3UfyELHQ1wRJ//oW/Y94azv0**\"\n> }\n\n（2）撰写创建命令，完成创建\n\n```shell\nansible webservers -m user -a \"name=ddd password=第一步获取的加密密码 shell=/bin/bash create_home=yes\"\n```\n\n4、 创建一个dev用户，并为其生成对应的秘钥\n\n```shell\nansible webservers -m user -a 'name=dev generate_ssh_key=yes ssh_key_bits=2048 ssh_key_file=.ssh/id_rsa'\n```\n\n#### mount 模块\n\n| 常用参数 | 功能                                                         |\n| -------- | ------------------------------------------------------------ |\n| src      | 指定要挂载的设备或分区路径。                                 |\n| path     | 指定要挂载到的目标路径。                                     |\n| fstype   | 指定要挂载的文件系统类型。                                   |\n| state    | 指定挂载状态，可选值为 `mounted`、（临时挂载）`present`（永久挂载）、`unmounted` （临时卸载）或 `absent`（卸载）。 |\n| opts     | 指定挂载选项，例如挂载选项或参数。                           |\n\n> #将192.168.1.121上的/nfs目录挂载到192.168.1.122的/nfs目录\n>\n> ansible 192.168.1.122 -m mount -a 'src=192.168.1.121:/nfs path=/nfs fstype=nfs opts=defaults state=present '\n\n#### cron模块\n\n**基本格式和常用参数**\n\n```shell\nansible <组/IP/all> -m cron -a ' '\n```\n\n| 常用参数                      | 功能                                          |\n| ----------------------------- | --------------------------------------------- |\n| minute/hour/day/month/weekday | 分/时/日/月/周                                |\n| job                           | 任务计划要执行的命令                          |\n| name                          | 任务计划的名称                                |\n| user                          | 指定计划任务属于哪个用户，默认是root用户      |\n| state                         | present表示添加（可以省略），absent表示移除。 |\n\n> #添加一个每天19点执行/data/test.sh脚本的定时任务，任务名为script-test。\n>\n> ansible webservers -m cron -a 'name=\"script-test\" hour=19 minute=00 job=\"/bin/bash /data/test.sh\" '\n>\n> #注释该定时任务\n>\n> ansible webservers -m cron -a 'name=\"script-test\" hour=19 minute=00 job=\"/bin/bash /data/test.sh\" disabled=yes '\n>\n> #删除该定时任务\n>\n> ansible webservers -m cron -a 'name=\"script-test\" hour=19 minute=00 job=\"/bin/bash /data/test.sh\" state=absent '\n\n#### archive 模块\n\n| 常用参数 | 功能                                                         |\n| -------- | ------------------------------------------------------------ |\n| path     | 指定要打包的源目录或文件的路径。                             |\n| dest     | 指定打包文件的输出路径。                                     |\n| format   | 指定打包文件的格式，可以是 zip、tar、gz 或 bzip2。默认为 tar 格式。 |\n| remove   | 指定是否在打包文件之后，删除源目录或文件。可选值为 yes 或 no。默认为 no，即不删除源目录或文件。 |\n\n#### unarchive 模块\n\n| 常用参数       | 功能                                                       |\n| -------------- | ---------------------------------------------------------- |\n| **copy**       | 指定是否将打包文件复制到远程节点以进行解压缩。             |\n| **remote_src** | (已弃用) 改用 `copy` 参数。                                |\n| **src**        | 指定要解压缩的打包文件路径，可以是本地路径或远程路径。     |\n| **dest**       | 指定要将文件解压缩到的目标目录。                           |\n| **creates**    | 指定一个文件路径，如果该文件已经存在，则不进行解压缩操作。 |\n| **remote_tmp** | 用于指定远程节点上的临时目录。默认为 `/tmp`。              |\n\n> 1、 将控制端的压缩包，解压到被控端：\n>\n> ansible webservers -m unarchive -a 'src=./test.tar.gz dest=/mnt'\n> 2、 将被控端的压缩包解压到被控端：\n>\n> ansible webservers -m unarchive -a 'src=/tmp/config_vpn_new.zip dest=/mnt remote_src=yes'\n\n#### lineinfile模块\n\n含义：替换|追加|删除\n\n| 参数         | 选项                    | 含义                                          |\n| ------------ | ----------------------- | --------------------------------------------- |\n| path         | 无                      | 指定要操作的目标文件路径                      |\n| regexp       | 无                      | 使用正则表达式匹配文件中的行                  |\n| line         | 无                      | 指定要修改或插入的新文本内容                  |\n| insertafter  | 无                      | 将文本插入到「正则匹配行」之后                |\n| insertbefore | 无                      | 将文本插入到「正则匹配行」之前                |\n| state        | absent / present (默认) | `absent`删除匹配行，`present`确保文本存在     |\n| backrefs     | yes / no                | `yes`启用正则后向引用，`no`未匹配时不操作文件 |\n| backup       | 无                      | 修改前是否创建备份文件（布尔值）              |\n| create       | 无                      | 当目标文件不存在时是否创建新文件（布尔值）    |\n\n> 1、替换httpd.conf文件中，Listen为Listen8080;\n>\n> ansible webservers -m lineinfile -a 'path=/etc/httpd/conf/httpd.conf regexp=\"^Listen\" line=\"Listen 8080\"'\n>\n> 2、 给主机增加一个网关：\n>\n> ansible webservers -m lineinfile -a 'path=/etc/sysconfig/network-scripts/ifcfg-eth1 line=\"GATEWAY=172.16.1.200\"'\n> 3、 删除主机的网关：\n>\n> ansible webservers -m lineinfile -a 'path=/etc/sysconfig/network-scripts/ifcfg-ethl regexp=\"^GATEWAY\" state=absent'\n\n## Ansible Playbook\n\nplaybook是一个由yml语法编写的文本文件，它由`play`和`task`两部分组成。\n`play`：主要定义要操作主机或者主机组。\n\n`task`：主要定义对主机或主机组具体执行的任务，可以是一个任务，也可以是多个任务(模块)。\n\nplaybook是由一个或多个play组成，一个play可以包含多个task任务。\n\n可以理解为：使用多个不同的模块来共同完成一件事情。\n\n### Playbook书写格式\n\nplaybook是由yml语法书写，结构清晰，可读性强。\n\n| 语法   | 描述                                                         |\n| ------ | ------------------------------------------------------------ |\n| 缩进   | **YAML** 使用固定的缩进风格表示层级结构，每个缩进由<mark>两个空格</mark>组成，不能使用 **tabs** |\n| 冒号   | 以冒号结尾的除外，其他所有冒号后面必须有空格                 |\n| 短横线 | 表示列表项，使用一个短横杠加一个空格。多个项使用同样的缩进级别作为同一列表 |\n\n### Playbook应用实例：\n\n#### 安装nignx\n\n（1）撰写playbook文件\n\n```yaml\nvi nginx.yml\n\n- hosts: webservers\n  tasks:\n    - name: 安装nignx\n      yum:\n        name: nginx\n        state: present\n    - name: 启动nignx\n      systemd:\n        name: nginx\n        state: started\n        enabled: yes\n```\n\n（2）检查文件是否有语法错误\n\n```shell\nansible-playbook --syntax nginx.yml\n或\nansible-playbook --syntax-check nginx.yml\n```\n\n> [root@121 data]# ansible-playbook --syntax nginx.yml\n>\n> playbook: nginx.yml\n\n（3）进行模拟执行\n\n```shell\nansible-playbook -C nginx.yml\n```\n\n> [root@121 data]# ansible-playbook -C nginx.yml\n>\n> PLAY [webservers] **************************************************************\n>\n> TASK [Gathering Facts] *********************************************************\n> ok: [192.168.1.122]\n> ok: [192.168.1.121]\n>\n> TASK [安装nignx] *****************************************************************\n> changed: [192.168.1.122]\n> changed: [192.168.1.121]\n>\n> TASK [启动nignx] *****************************************************************\n> changed: [192.168.1.121]\n> changed: [192.168.1.122]\n>\n> PLAY RECAP *********************************************************************\n> 192.168.1.121              : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n> 192.168.1.122              : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n（4）实际执行\n\n```shell\nansible-playbook nginx.yml\n```\n\n> [root@121 data]# ansible-playbook nginx.yml\n>\n> PLAY [webservers] **************************************************************\n>\n> TASK [Gathering Facts] *********************************************************\n> ok: [192.168.1.122]\n> ok: [192.168.1.121]\n>\n> TASK [安装nignx] ***************************************************************                                                   **\n> changed: [192.168.1.121]\n> changed: [192.168.1.122]\n>\n> TASK [启动nignx] *****************************************************************\n> changed: [192.168.1.122]\n> changed: [192.168.1.121]\n>\n> PLAY RECAP *********************************************************************\n> 192.168.1.121              : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n> 192.168.1.122              : ok=3    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0\n\n#### 安装NFS服务\n\n任务顺序：\n\n- 下载nfs程序\n- 挂载nfs配置文件（设置触发器，修改配置文件后，使配置重启生效）\n- 初始化设置nfs（设置用户、组、授权）\n- 启动nfs\n\n> 设置的触发器是handles，在第二步中添加notify信息，当配置文件修改时，handles获取notify中的信息，重启nfs服务，使得配置重新生效\n\n文件准备：\n\n```shell\n#创建目录存放该项目\nmkdir nfs\n#创建nfs配置文件，将以下内容写入\nvi exports.j2\n\n/ansible_test 192.168.1.0/24(rw,all_squash,anonuid=7777,anongid=7777)\n\n```\n\n> 将ansible的配置文件`ansible.cfg`和`hosts`，提前放置在该项目目录下\n\n创建nfs剧本\n\n```yaml\nvi nfs.yml\n\n- hosts: webservers\n  tasks:\n    - name: download nfs\n      yum:\n        name: nfs-utils\n        state: present\n    - name: configure nfs\n      copy:\n        src: ./exports.j2\n        dest: /etc/exports\n      notify: restart nfs  #触发信息\n    - name: init group\n      group:\n        name: bbb\n        gid: 7777\n    - name: init user\n      user:\n        name: bbb\n        uid: 7777\n        group: bbb\n        shell: /sbin/nologin\n        create_home: no\n    - name: init directory\n      file:\n        path: /ansible_test\n        state: directory\n        owner: bbb\n        group: bbb\n        mode: \"0755\"\n    - name: start nfs\n      systemd:\n        name: nfs\n        state: started\n        enabled: yes\n  handlers:  #触发器\n    - name: restart nfs\n      systemd:\n        name: nfs\n        state: restarted\n\n```\n\n执行并验证\n\n```shell\n ansible-playbook --syntax-check nfs.yml\n ansible-playbook -C nfs.yml\n ansible-playbook nfs.yml\n \n #查看nfs服务\n showmount -e 192.168.1.121\n showmount -e 192.168.1.122\n \n #挂载测试\n mount -t nfs 192.168.1.121:/ansible_test /data/test\n mount -t nfs 192.168.1.122:/ansible_test /data/test1\n```\n\n> [root@121 ~]# df -h\n> 文件系统                     容量  已用  可用 已用% 挂载点\n> devtmpfs                     898M     0  898M    0% /dev\n> tmpfs                        910M     0  910M    0% /dev/shm\n> tmpfs                        910M  9.7M  901M    2% /run\n> tmpfs                        910M     0  910M    0% /sys/fs/cgroup\n> /dev/mapper/centos-root       17G  1.7G   16G   10% /\n> /dev/sda1                   1014M  153M  862M   16% /boot\n> tmpfs                        182M     0  182M    0% /run/user/0\n> 192.168.1.121:/ansible_test   17G  1.7G   16G   10% /data/test\n> 192.168.1.122:/ansible_test   17G  2.2G   15G   13% /data/test1\n\n#### 安装rsync服务\n\n##### rsync服务端：\n\n任务顺序：\n\n- 下载rsync程序\n- 挂载rsync配置文件（设置触发器，修改配置文件后，使配置重启生效）\n- 初始化设置rsync（设置用户、组、授权）\n- 创建虚拟用户和密码\n- 启动rsync\n\n> 设置的触发器是handles，当配置文件或虚拟用户密码修改时，handles获取notify中的信息，重启rsync服务，使得配置重新生效\n\n文件准备：\n\n```shell\n#创建目录存放该项目\nmkdir rsync\n#复制rsync配置文件rsyncd.conf并修改（可先手动下载rsync，拷贝配置文件后再卸载，也可直接使用以下内容）\n\nuid = ansible_www\ngid = ansible_www\nport = 873\nfake super = yes\nuse chroot = yes\nmax connections = 4\ntimeout = 900\nread only = false\nlist = false\nauth users = rsync_backup\nsecrets file = /etc/rsync.passwd\nlog file = /var/log/rsyncd.log\n[backup]\npath = /backup\n```\n\n> 将ansible的配置文件`ansible.cfg`和`hosts`，提前放置在该项目目录下\n\n创建rsync剧本\n\n\n```yaml\nvi rsync.yml\n\n- hosts: webservers\n  tasks:\n    - name: download rsync\n      yum:\n        name: rsync\n        state: present\n    - name: configure rsync\n      copy:\n        src: ./rsyncd.conf\n        dest: /etc/rsyncd.conf\n        owner: root\n        group: root\n        mode: \"0644\"\n      notify: restart rsync\n    - name: init group\n      group:\n        name: ansible_www\n        gid: 8888\n    - name: init user\n      user:\n        name: ansible_www\n        uid: 8888\n        group: ansible_www\n        shell: /sbin/nologin\n        create_home: no\n    - name: init directory\n      file:\n        path: /backup\n        state: directory\n        owner: ansible_www\n        group: ansible_www\n        mode: \"0755\"\n        recurse: yes\n    - name: init rsync user passwd\n      copy:\n        content: \"rsync_backup:123456\"\n        dest: /etc/rsync.passwd\n        owner: root\n        group: root\n        mode: 0600\n      notify: restart rsync\n    - name: start rsync\n      systemd:\n        name: rsyncd\n        state: started\n        enabled: yes\n  handlers:\n    - name: restart rsync\n      systemd:\n        name: rsyncd\n        state: restarted\n```\n\n执行并验证\n\n```shell\nansible-playbook --syntax-check rsync.yml \nansible-playbook -C rsync.yml \nansible-playbook rsync.yml  \n\n#验证\nrsync -avz host_group rsync_backup@192.168.1.122::backup\n```\n\n##### rsync客户端：\n\n任务顺序：\n\n- 将脚本推送至被控端指定路径下\n- 配置定时任务\n\n创建rsync-client剧本\n\n```yaml\nvi rsync-client.yml\n\n- hosts: localhost\n  tasks:\n    - name: create scripts path\n      file:\n        path: /scripts\n        owner: root\n        group: root\n        state: directory\n        mode: 0755\n    - name: push scripts\n      copy:\n        src: test.sh\n        dest: /scripts/test.sh\n        owner: root\n        group: root\n        mode: 0755\n    - name: contable job\n      cron:\n        name: \"push backup data\"\n        minute: \"*/1\"\n        job: \"/bin/bash /scripts/test.sh &>/dev/null\"\n\n```\n\n创建test.sh以供测试\n\n```shell\n#!/bin/bash\nRSYNC_PASSWORD=\"123456\"  rsync -avz /backup/ rsync_backup@192.168.1.122::backup\n```\n\n查看是否有定时任务\n\n```shell\ncrontab -l\n```\n\n> #Ansible: push backup data\n>\n> */1 * * * * /bin/bash /scripts/test.sh &>/dev/null\n\n#### 使用Ansible部署多节点phpmyadmin\n\n##### 项目需求\n\n- 使用LNMP部署phpmyadmin\n- nginx（Haproxy）作为负载均衡\n- Redis实现会话保持\n\n##### 主机规划\n\n| 主机名 | ip            | 用途                              |\n| ------ | ------------- | --------------------------------- |\n| 121    | 192.168.1.121 | Ansible控制节点、nignx+PHP        |\n| 122    | 192.168.1.122 | Ansible受控节点、nignx+PHP        |\n| 123    | 192.168.1.123 | Ansible受控节点、nignx（Haproxy） |\n| 124    | 192.168.1.124 | Ansible受控节点、nignx（Haproxy） |\n| 125    | 192.168.1.125 | Ansible受控节点、Redis            |\n\n##### 部署步骤\n\n###### 环境准备\n\n> 121节点上要准备好Ansible，并设置以上5台机器免密登录，具体参照上方的配置过程。\n\n```shell\n#创建存放项目文件目录\nmkdir /data/phpmyadmin\n#将ansible的配置文件`ansible.cfg`和`hosts`，复制到该phpmyadmin目录下，并修改配置\ncp /etc/ansible/ansible.cfg /data/phpmyadmin/ansible.cfg\n#修改配置文件中的路径，并取消注释\ninventory      = /data/phpmyadmin/host_group\n\ncp /etc/ansible/hosts /data/phpmyadmin/host_group\n#修改主机组\n[webservers]\n192.168.1.121\n192.168.1.122\n\n[dbservers]\n192.168.1.125\n\n[lbservers]\n192.168.1.123\n192.168.1.124\n```\n\n确认配置文件的读取路径是否为当前项目下的`/data/phpmyadmin/ansible.cfg`\n\n> [root@121 phpmyadmin]# ansible --version\n> ansible 2.9.27\n>   config file = **/data/phpmyadmin/ansible.cfg**\n>   configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']\n>   ansible python module location = /usr/lib/python2.7/site-packages/ansible\n>   executable location = /usr/bin/ansible\n>   python version = 2.7.5 (default, Jun 28 2022, 15:30:04) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]\n\n验证主机是否都可ping通\n\n> [root@121 phpmyadmin]# ansible all -m ping\n> 192.168.1.122 | SUCCESS => {\n>     \"ansible_facts\": {\n>         \"discovered_interpreter_python\": \"/usr/bin/python\"\n>     },\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.124 | SUCCESS => {\n>     \"ansible_facts\": {\n>         \"discovered_interpreter_python\": \"/usr/bin/python\"\n>     },\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.123 | SUCCESS => {\n>     \"ansible_facts\": {\n>         \"discovered_interpreter_python\": \"/usr/bin/python\"\n>     },\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.125 | SUCCESS => {\n>     \"ansible_facts\": {\n>         \"discovered_interpreter_python\": \"/usr/bin/python\"\n>     },\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n> 192.168.1.121 | SUCCESS => {\n>     \"ansible_facts\": {\n>         \"discovered_interpreter_python\": \"/usr/bin/python\"\n>     },\n>     \"changed\": false,\n>     \"ping\": \"pong\"\n> }\n\n###### 安装Redis\n\n任务顺序：\n\n- 下载redis程序\n- 挂载redis配置文件（设置触发器，修改配置文件后，使配置重启生效）\n- 启动redis\n\n> 设置触发器handles，重启redis服务\n\n（1）将redis的配置文件`redis.conf`，拷贝至`/data/phpmyadmin/files`目录下\n\n```shell\nmkdir /data/phpmyadmin/files\n# 若无redis.conf文件可先在控制节点上使用命令yum -y install redis下载后再拷贝\ncp /etc/redis.conf /data/phpmyadmin/files\n#修改配置文件,将要安装redis的主机ip加入\nbind 127.0.0.1 192.168.1.125\n```\n\n（2）\n\n```yaml\nvi redis.yml\n\n- hosts: dbservers\n  tasks:\n    - name: download redis\n      yum:\n        name: redis\n        state: present\n    - name: configure redis\n      copy:\n        src: ./files/redis.conf\n        dest: /etc/redis.conf\n        owner: redis\n        group: root\n        mode: 0640\n      notify: restart redis\n    - name: start redis\n      systemd:\n        name: redis\n        state: started\n        enabled: yes\n  handlers:\n    - name: restart redis\n      systemd:\n        name: redis\n        state: restarted\n\n```\n\n（3）安装并测试\n\n```shell\nansible-playbook --syntax-check redis.yml\nansible-playbook -C redis.yml\nansible-playbook redis.yml\n#测试，之前控制节点121安装了redis，可直接在121上测试\n#注意防火墙端口\nredis-cli -h 192.168.1.125\n```\n\n> [root@121 phpmyadmin]# redis-cli -h 192.168.1.125\n> 192.168.1.125:6379>\n\n###### 安装Mariadb\n\n在192.168.1.125上安装Mariadb\n\n```shell\n yum -y install mariadb-server\n systemctl start mariadb\n systemctl enable mariadb\n #设置root用户密码\n mysqladmin -uroot password '123456'\n #登录创建用户\n mysql -uroot -p123456\n #创建数据库\n create database php_db;\n #创建数据库用户\n grant all on php_db.* to php_user@'%' identified by '123456';\n```\n\n###### 安装nginx、PHP\n\n任务顺序：\n\n- 下载nginx、PHP程序\n- 挂载nginx、PHP配置文件（设置触发器，修改配置文件后，使配置重启生效）\n- 初始化设置nginx（设置用户、组、授权）\n- 启动nginx、PHP\n\n> 设置触发器handles，重启nginx、PHP服务\n\n（1）设置php7下载源，在121，122上执行\n\n```shell\nrpm -Uvh https://mirror.webtatic.com/yum/el7/epel-release.rpm\nrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm\n```\n\n（2）将nginx、php的配置文件，拷贝至`/data/phpmyadmin/files`目录下，并修改配置\n\n```shell\n#nginx配置文件\ncp /etc/nginx/nginx.conf /data/phpmyadmin/files\n#php配置文件\ncp /etc/php.ini /data/phpmyadmin/files\ncp /etc/php-fpm.d/www.conf /data/phpmyadmin/files\n```\n\n修改nginx配置文件，将用户修改为`www`。\n\n```\nvi nginx.conf\nuser www;\n```\n\n在`/data/phpmyadmin/files`目录下，新增`phpmyadmin`的nginx代理文件\n\n```\nvi phpmyadmin.conf\n\nserver {\n    listen 80;\n    server_name ansible.phpmyadmin.local;\n\n    # 确保此路径与phpMyAdmin实际路径完全一致\n    root /code/phpmyadmin;\n    index index.php;\n\n    location / {\n        try_files $uri $uri/ /index.php$is_args$args;\n    }\n\n    location ~ \\.php$ {\n        # 绝对路径必须完整正确\n        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;\n        fastcgi_pass 127.0.0.1:9000;\n        fastcgi_index index.php;\n        include fastcgi_params;\n\n        # 添加关键参数\n        fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n        fastcgi_intercept_errors on;\n        fastcgi_param PATH_INFO $fastcgi_path_info;\n    }\n}\n\n```\n\n修改php配置文件`php.ini`，设置redis连接。\n\n```\nvi php.ini\n[Session]\n\nsession.save_handler = redis\nsession.save_path = \"tcp://192.168.1.125:6379?weight=1&timeout=2.5\"\n```\n\n修改php配置文件`www.conf`，设置用户和组，并注释部分设置。\n\n```\nvi www.conf\n\n[www]\nuser = www\ngroup = www\n\n;php_value[session.save_handler] = files\n;php_value[session.save_path]    = /var/lib/php/session\n\n```\n\n（3）下载phpmyadmin\n\n```shell\n#phpmyadmin程序下载到/data/phpmyadmin/files目录下\ncurl -O https://files.phpmyadmin.net/phpMyAdmin/5.2.2/phpMyAdmin-5.2.2-all-languages.zip\n\n#解压程序\nunzip phpMyAdmin-5.2.2-all-languages.zip\n#复制并修改配置文件，连接mariadb数据库\ncp phpMyAdmin-5.2.2-all-languages/config.sample.inc.php config.inc.php\n#修改localhost为192.168.1.125\n$cfg['Servers'][$i]['host'] = '192.168.1.125';\n```\n\n（4）创建nignx-php剧本\n\n```yaml\nvi nginx-php.yml\n\n- hosts: webservers\n  tasks:\n    - name: download nginx\n      yum:\n        name: nginx\n        state: present\n    - name: download php\n      yum:\n        name: \"{{ packages }}\"\n      vars:\n        packages:\n        - php72w\n        - php72w-cli\n        - php72w-common\n        - php72w-devel\n        - php72w-embedded\n        - php72w-fpm\n        - php72w-gd\n        - php72w-mbstring\n        - php72w-mysqlnd\n        - php72w-opcache\n        - php72w-pdo\n        - php72w-xml\n        - php72w-ldap\n        - php72w-pecl-redis\n\n    - name: configure nginx\n      copy:\n        src: ./files/nginx.conf\n        dest: /etc/nginx/nginx.conf\n        owner: root\n        group: root\n        mode: 0644\n      notify: restart nginx\n    - name: init group www\n      group:\n        name: www\n        gid: 666\n    - name: init user www\n      user:\n        name: www\n        uid: 666\n        group: www\n        shell: /sbin/nologin\n        create_home: no\n    - name: start nginx\n      systemd:\n        name: nginx\n        state: started        \n        enabled: yes\n\n    - name: configure php.ini\n      copy:\n        src: ./files/php.ini\n        dest: /etc/php.ini\n        owner: root\n        group: root\n        mode: 0644\n      notify: restart php    \n    - name: configure www.conf\n      copy:\n        src: ./files/www.conf\n        dest: /etc/php-fpm.d/www.conf\n        owner: root\n        group: root\n        mode: 0644\n      notify: restart php        \n    - name: start php\n      systemd:\n        name: php-fpm\n        state: started        \n        enabled: yes\n        \n    - name: copy nginx conf \n      copy:\n        src: ./files/phpmyadmin.conf\n        dest: /etc/nginx/conf.d/phpmyadmin.conf\n      notify: restart nginx      \n    - name: create code directory\n      file:\n        path: /code\n        state: directory\n        owner: www\n        group: www\n        mode: \"0755\"\n        recurse: yes    \n    - name: unarchive phpmyadmin   #解压phpmyadmin\n      unarchive: \n        src: ./files/phpMyAdmin-5.2.2-all-languages.zip\n        dest: /code/\n        owner: www\n        group: www\n        creates: /code/phpMyAdmin-5.2.2-all-languages/config.inc.php        \n    - name: create link  #创建软连接\n      file: \n        src: /code/phpMyAdmin-5.2.2-all-languages/\n        dest: /code/phpmyadmin \n        state: link\n    - name: change phpmyadmin configure \n      copy:\n        src: ./files/config.inc.php\n        dest: /code/phpMyAdmin-5.2.2-all-languages/config.inc.php  \n          \n        \n  handlers: \n    - name: restart nginx\n      systemd:\n        name: nginx\n        state: restarted\n    - name: restart php\n      systemd:\n        name: php-fpm\n        state: restarted\n\n```\n\n（5）安装并测试\n\n```shell\nansible-playbook --syntax-check nginx-php.yml\nansible-playbook -C nginx-php.yml\nansible-playbook nginx-php.yml\n```\n\n（6）在本机电脑`C:\\Windows\\System32\\drivers\\etc\\hosts`配置hosts\n\n```shell\n#分别配置121，122的hosts进行访问测试\n192.168.1.121 ansible.phpmyadmin.local\n192.168.1.122 ansible.phpmyadmin.local\n```\n\n（7）浏览器访问`ansible.phpmyadmin.local`，用户密码是安装Mariadb数据库时，创建的用户`php_user`及其密码`123456`。\n\n![image-20250411224722322](Ansible知识梳理/3.png)\n\n![image-20250411224810891](Ansible知识梳理/4.png)\n\n###### 负载均衡方式一：nginx\n\n（1）复制nginx的配置文件`nginx.conf`，至`/data/phpmyadmin/files`目录下并命名为`nginx.conf.lb`\n\n```shell\ncp /etc/nginx.conf /data/phpmyadmin/files/nginx.conf.lb\n```\n\n（2）在`/data/phpmyadmin/files`目录下创建配置文件`proxy.conf`，用作负载均衡\n\n```shell\nvi proxy.conf\n\nupstream ansible {\n    server 192.168.1.121;\n    server 192.168.1.122;\n}\nserver {\n    listen 80;\n    server_name ansible.phpmyadmin.local;\n    location / {\n        proxy_pass http://ansible;\n        proxy_set_header Host $http_host;\n    }\n\n}\n\n```\n\n（3）创建lb-nginx剧本\n\n```yaml\nvi lb-nginx.yml\n\n- hosts: lbservers\n  tasks:\n    - name: download nginx\n      yum:\n        name: nginx\n        state: present\n    - name: configure nginx\n      copy:\n        src: ./files/nginx.conf.lb\n        dest: /etc/nginx/nginx.conf\n      notify: restart nginx\n    - name: copy nginx conf\n      copy:\n        src: ./files/proxy.conf\n        dest: /etc/nginx/conf.d/proxy.conf\n      notify: restart nginx\n    - name: start nginx\n      systemd:\n        name: nginx\n        state: started\n        enabled: yes\n  handlers:\n    - name: restart nginx\n      systemd:\n        name: nginx\n        state: restarted\n```\n\n（4）安装并测试\n\n```shell\nansible-playbook --syntax-check lb-nginx.yml\nansible-playbook -C lb-nginx.yml\nansible-playbook lb-nginx.yml\n```\n\n（5）在本机电脑`C:\\Windows\\System32\\drivers\\etc\\hosts`配置hosts\n\n```shell\n192.168.1.123 ansible.phpmyadmin.local\n192.168.1.124 ansible.phpmyadmin.local\n```\n\n（6）浏览器访问`ansible.phpmyadmin.local`,并观察`数据库服务器`-`用户`，刷新查看用户是否为121与122切换显示。\n\n###### 负载均衡方式二：haproxy\n\n（1）设置haproxy下载源\n\n```shell\nrpm -Uvh https://repo.ius.io/ius-release-el7.rpm\n```\n\n（2）将`/etc/haproxy/haproxy.cfg`，至`/data/phpmyadmin/files`目录下\n\n```shell\ncp /etc/haproxy/haproxy.cfg /data/phpmyadmin/files\n```\n\n> 可先在任意一台服务器安装haproxy，获取/etc/haproxy/haproxy.cfg文件\n\n（3）修改`haproxy.cfg`配置文件，新增以下内容。\n\n```\nfrontend web\n                bind *:8080\n                mode http\n                acl ansible_domain hdr_reg(host) -i ansible.phpmyadmin.local\n                use_backend ansible_cluster if ansible_domain\nbackend ansible_cluster\n                balance roundrobin\n                option httpchk HEAD / HTTP/1.1\\r\\nHost:\\ ansible.phpmyadmin.local\n                server server1 192.168.1.121:80 check port 80 inter 3s rise 2 fall 3\n                server server2 192.168.1.122:80 check port 80 inter 3s rise 2 fall 3\n\n```\n\n（4）创建lb-haproxy剧本\n\n```yaml\nvi lb-haproxy.yml\n\n- hosts: lbservers\n  tasks:\n    - name: download haproxy\n      yum:\n        name: haproxy22\n        state: present\n    - name: configure haproxy\n      copy:\n        src: ./files/haproxy.cfg\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644\n      notify: restart haproxy  \n    - name: start haproxy\n      systemd:\n        name: haproxy\n        state: started        \n        enabled: yes\n  handlers: \n    - name: restart haproxy\n      systemd:\n        name: haproxy\n        state: restarted\n```\n\n（5）安装并测试\n\n```shell\nansible-playbook --syntax-check lb-haproxy.yml\nansible-playbook -C lb-haproxy.yml\nansible-playbook lb-haproxy.yml\n```\n\n（5）在本机电脑`C:\\Windows\\System32\\drivers\\etc\\hosts`配置hosts\n\n```shell\n192.168.1.123 ansible.phpmyadmin.local\n192.168.1.124 ansible.phpmyadmin.local\n```\n\n（6）浏览器访问`ansible.phpmyadmin.local:8080`,并观察`数据库服务器`-`用户`，刷新查看用户是否为121与122切换显示。\n\n\n\n> 本篇知识来源于B站视频BV11JZcYwEDd\n\n","tags":["Linux运维","自动化运维","CI/CD","Ansible","知识梳理"],"categories":["梳理总结","知识梳理","自动化运维","Ansible"]},{"title":"Hexo-Butterfly美化教程-[2]Butterfly主题安装","url":"/yyg/80ffa402/","content":"\n## 拉取主题代码\n\n```shell\ngit clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly\n```\n\n## 配置主题应用\n\n将`blog`根目录下的`_config.yml`文件中的主题配置由`landscape`改为`butterfly`。\n\n```yaml\ntheme: butterfly\n```\n\n将`blog\\themes\\butterfly`目录下的`_config.yml`文件复制一份到`blog`根目录下，并将复制的文件改名为`_config.butterfly.yml`。\n\n## 安装插件\n\n```shell\nnpm install hexo-renderer-pug hexo-renderer-stylus --save\n```\n\n## 测试验证\n\n```\n#本地测试\nhexo clean && hexo g && hexo s\n```\n\n在浏览器中访问 `http://localhost:4000/` ，查看Butterfly主题安装是否成功。\n\n{% folding green open,博客搭建系列文章 %}\n\n{% series 博客搭建教程 %}\n\n{% endfolding %}","tags":["Hexo","Butterfly","博客教程"],"categories":["博客教程","博客搭建"]},{"title":"跨城搬家总结","url":"/yyg/4c3a1bc3/","content":"\n## 写在前面\n\n　　今天终于彻底完成了从A城到B城的跨城搬家，开始在B城开启生活的新篇章。\n\n## 搬家全过程\n\n　　年后从老家回到A城之后，开始正式着手考虑搬到B城的事情，此时离房租到期还有半个多月的时间。\n\n### 找房和看房\n\n　　首先，确定租房的需求，包含租房费用（房租预算价位、押金及退还的要求、水电、天然气、物业费、网费、是否有看房费、中介费等其他费用）；房子的基本条件（采光、大小、家具、空调、阳台、暖气、无线网、环境是否安静），其中房子的这些基本条件又分为刚需和二级需求，毕竟要完全满足条件的少之又少，先考虑满足刚需的情况下，再看是否又符合二级需求的；附近生活设施（超市、菜市场、理发店、公园、公交、地铁距离等）。\n\n　　了解B城市的水、电、天然气的价格，方便之后找房时进行评估和谈判。\n\n　　前期工作完成后，开始进行网上找房。在各大租房app上，优先找房东直租/个人房源（主流短视频平台和问答社区都有相关教程），前期可能会麻烦一点，但可以省下一笔中介费，也能避免一些不必要的麻烦。\n\n　　我花费5天时间，找了6套待看房。并根据需求确定了待看的优先级，先看离车站最近的（目的是降低客观因素影响的限度、更好的留住精力），然后看优先级高的。看的时候，除了要满足之前的需求、还要看下房间内设施的好坏，如门锁、空调、窗口、床、桌椅、柜子、如何晾晒衣服和鞋子，重点关注周围环境噪音程度是否在接受范围内。\n\n　　看完房后，再去附近查看生活设施是否齐全。\n\n　　全部看完后，进行综合对比，确定租房，并和房东约定入住和签合同时间，沟通好所有费用并做好留证。\n\n> 我2天时间看完了6套，最后确定的下来的是在第一天看的。其实，第一天看完之后，心里基本上就有了大致的选择。如果时间上较为紧张，直接在第一天内看3-4套，然后在其中进行选择即可（前提是按之前确定好的优先级进行的）。\n\n### 行李打包\n\n　　确定行李打包和运输方式前，先要确定好准备舍弃的物品、如果较多的话可列一个清单，并做好分类和处理计划。较好的衣服、鞋子可选择捐赠或者选择上门回收。\n\n　　我选择的行李打包方式是将衣物、被褥和日用品等通过物流方式邮寄B城市的新住址，自己再携带一个被褥和电子设备进行搬家。因为我的行李较少，自己携带时也很方便。携带被褥可以在物流未到之时，也能住进新家，随身携带电子设备，也能避免物流途中损毁。\n\n　　确定好打包方式后，了解跨城搬家要做的准备工作，购买打包工具和确定使用的物流商家。我这次准备的打包工具有纸箱（60x40x50，打包时要保证每一个箱子至少要装20kg）、工业pe膜、真空压缩袋、宽胶带。\n\n> 这次工业pe膜差点不够用了，下次再多买点。也可考虑买些热缩膜。\n\n　　在网上找房的期间，我进行了一次模拟打包，通过模拟打包和可以确定两件事：\n\n　　　一是通过物流方式运输行李的大致重量，方便选取物流时进行对比分析。\n\n　　　二是打包所需的大致时间，可更好的规划行程。\n\n　　通过模拟打包后，在进行后实际打包时，也相对轻松些，时间上也比之前更快了一些。\n\n　　确定好租房后，先跟房东要房子的快递地址，之后在根据A城市和B城市的地址和打包行李的重量选取性价比较高的物流，（选取物流时注意类型和重货上楼是否收费）。\n\n　　打包好行李后，下单等待物流人员上门收货即可。\n\n> 使用60x40x50的纸箱装行李时，若重量＜20kg，使用体积计算，行李实际重量＜计算重量，即行李实际重量＜20kg，计算重量却为20kg；若重量≥20kg，使用重量计算，则行李实际重量=计算重量，均≥20kg。所以使用60x40x50的纸箱，要装至少20kg的行李，才不会在行李重量上让物流公司有机可乘\n> \n> 不同物流公司邮寄行李时纸箱（60x40x50）体积与重量换算比不同，分别有6000、8000两种：\n> \n> 模式一：60x40x50÷6000=20kg（我本次使用的）\n> \n> 模式二：60x40x50÷8000=15kg\n\n　　剩余行李可在出发去B城市当天打包，并将准备舍弃的物品处理完。\n\n　　在出发去B城市前，必须和A城市房东将押金退还日期与要求协商好，彻底结清A城租房的事情。\n\n### 搬新家、规划并完成布局\n\n　　到B城市新住址后，先记录下房间之内所有物品的原始面貌，再和B城市房东协商好房间的家具及注意事项，最后确定合同内容，期间提到的所有需求和注意事项要留证。\n\n　　布置房间时，先根据房间规格和自己的喜好，重新规划布局，布置家具。结束后，再根据需要进行物品的添置。\n\n> 从搬入新家到彻底完成房间重新布局，我总计用了2天时间。也正式开始在B城开启生活的新篇章。\n\n## 感受总结\n\n　　第一次跨城搬家，总体感受还不错，没有刚毕业找房时的那种慌乱了，全过程的节奏，也把握的顺畅了一些。\n\n　　虽然回看本次搬家的全过程，相对没预想的那么麻烦，这得益于每一大步的准备工作做的还算明晰。\n\n　　从搬家开始到完全结束，整体花费算下来已经赶上现在房子的房租了😂，不过，凡事都有第一次嘛，这次进行复盘总结下我的“直接经验”，下次肯定可以做的更好😎。\n\n　　加油吧！！！与看到本文的诸位共勉。","tags":["跨城搬家","复盘","乔迁新居"],"categories":["漫谈","搬家"]},{"title":"休闲娱乐-1月","url":"/yyg/64b9e8e0/","content":"\n## 登山-嵩山\n\n​    前两天，去了登封爬了嵩山，在登封两天的总体感觉还不错。\n\n### 人文风情\n\n​    之前做攻略看到登封的炒刀削不错。特地去尝了尝，地址是登封汽车客运总站旁的小店，个人觉得味道不错，分量足，价格适中。\n\n​    登封人民也很热情，找民宿时，遇到的老板都很热心，也都很耐心的解答住宿需求。\n\n​    登封发展节奏舒缓，城市公共交通以公交为主，并且公交的班次之间时间大都在30分钟以上，出租车很多。\n\n### 登山\n\n​    这次去嵩山爬的是太室山，去年去了少室山。\n\n​    爬太室山选择的上山路线是嵩阳书院--峻极峰，这次不同于爬泰山时的“特纵兵式”爬法，走走停停，总路程6.2公里，用时3小时。沿途有很多补给点，冬天风景一般。下山路线是峻极峰--卢崖瀑布，这条路线是太室山3条路线中最长的，也是最险峻的。路上会时不时的碰见几只小猫，一线天台阶很陡，还有两座吊桥，这条路山脚有滑雪场，人很多。总路程9.7公里，用时2个半小时。\n\n​    登山之后需要及时进行肌肉拉伸恢复，不然会出现腿部肌肉酸胀的情况。我这次就是没有进行及时的拉伸，导致爬后的第二天，大腿外侧肌肉和小腿内侧肌肉酸疼。\n\n<mark>登山之后的当天晚上做肌肉拉伸、泡脚、洗热水澡，对缓解肌肉酸胀和恢复很有帮助。</mark>\n","tags":["休闲娱乐","旅游","爬山"],"categories":["漫谈","休闲娱乐"]},{"title":"docker部署SonarQube","url":"/yyg/12643dd2/","content":"## 前言\n\n{% folding cyan,🔜什么是SonarQube？🔚 %}\n\n>  SonarQube是一个开源的代码质量管理平台，通过一系列的规则库对代码的扫描检查，提升代码的质量。\n>\n> 运行 SonarQube 服务器分析需要三个组件：SonarQube Server、存储数据库（本文中使用PostgreSQL）、 scanners扫描程序。\n>\n> 其中UI为<mark>SonarQube Server </mark>：负责提供Web界面、处理代码分析报告并将其保存在 存储数据库中的计算引擎（SonarQube Server 中还集成有Elasticsearch）。\n>\n> <mark>存储数据库 </mark>：SonarQube Server的配置；代码扫描期间生成的代码质量和安全性指标和问题。\n>\n>  <mark>scanners扫描程序</mark>：用于分析项目，根据语言有所不同。\n\n{% endfolding %}\n\n## 环境准备\n\n（1）使用脚本安装docker、docker-compose\n\n```shell\nbash <(curl -sSL https://linuxmirrors.cn/docker.sh)\n```\n\n（2）配置镜像加速\n\n```shell\nvi /etc/docker/daemon.json\n\n{\n  \"data-root\": \"/data/dockerData\",\n  \"registry-mirrors\": [\n    \"https://docker.mirrors.sjtug.sjtu.edu.cn\",\n    \"https://docker.mirrors.ustc.edu.cn\",\n    \"https://mirror.iscas.ac.cn\",\n    \"https://docker.rainbond.cc\",\n    \"https://docker.kubesre.xyz\"],\n    \"log-driver\":\"json-file\",\n    \"log-opts\":{\"max-size\" :\"50m\",\"max-file\":\"3\"}\n}\n```\n\n（3）启动docker服务\n\n```shell\nsystemctl start docker\nsystemctl enable docker\nsystemctl status docker\n```\n\n（4）设置进程可能具有的最大内存映射区域数 （vm.max\\_map\\_count） 大于或等于 524288，打开的文件描述符的最大数量 （fs.file-max） 大于或等于 131072。\n\n```shell\necho \"vm.max_map_count=524288\nfs.file-max=131072\" >> /etc/sysctl.conf\nsysctl -p\n```\n\n（5）配置安全策略\n\n```shell\nfirewall-cmd --add-port=9000/tcp --permanent\nfirewall-cmd --reload\nfirewall-cmd --list-all\n```\n\n## 部署sonarqube\n\n（1）创建所需目录\n\n```shell\nmkdir -p /data/sonarqube/data /data/sonarqube/extensions /data/sonarqube/logs /data/sonarqube/temp\nchmod -R 777 /data/sonarqube/\nmkdir -p /data/sonarqube/sonarqube-sql /data/sonarqube/sonarqube-sql/data\n```\n\n（2）创建docker compose文件，编排创建sonarqube、数据库容器。\n\n```shell\nvi sonarqube.yml\n*****************************************************\nservices:\n  sonarqube:\n    image: sonarqube:community\n    container_name: sonarqube\n    restart: always\n    volumes:\n      - /data/sonarqube/data:/opt/sonarqube/data\n      - /data/sonarqube/extensions:/opt/sonarqube/extensions\n      - /data/sonarqube/logs:/opt/sonarqube/logs\n      - /data/sonarqube/temp:/opt/sonarqube/temp\n    environment:\n      SONAR_JDBC_URL: jdbc:postgresql://sonarqube-sql:5432/postgres\n      SONAR_JDBC_USERNAME: postgres\n      SONAR_JDBC_PASSWORD: Qwer#1234\n    ports:\n      - \"9000:9000\"\n    depends_on:\n      sonarqube-sql:\n        condition: service_healthy\n    networks:\n      net:\n        ipv4_address: 172.20.112.11\n  sonarqube-sql:\n    image: postgres:15\n    hostname: postgresql\n    container_name: sonarqube-sql\n    restart: always\n    volumes:\n      - /data/sonarqube/sonarqube-sql:/var/lib/postgresql\n      - /data/sonarqube/sonarqube-sql/data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_DB: postgres\n      POSTGRES_PASSWORD: Qwer#1234\n      TZ: \"Asia/Shanghai\"\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    networks:\n      net:\n        ipv4_address: 172.20.112.12\n\n\nnetworks:\n net:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.112.0/24\n*****************************************************\n\n```\n\n（3）执行命令，创建容器\n\n```shell\ndocker compose -f sonarqube.yml up -d\n```\n\n## **部署**SonarScanner集成VUE\n\n（1）在VUE项目的根目录下创建sonar-project.properties文件，并添加配置项\n\n```shell\nvi sonar-project.properties\n*****************************************************\nsonar.projectKey=sonarqube中创建的项目名称\nsonar.projectName=VUE项目名\nsonar.projectVersion=VUE项目版本\nsonar.sources=.  #VUE项目源文件的目录\nsonar.language=vue   #项目语言\nsonar.sourceEncoding=UTF-8  #项目编码\nsonar.host.url=http://192.168.32.12:9000/ #sonarqube服务器端的地址\nsonar.token=sonarqube中创建项目的token\n*****************************************************\n#修改文件权限\nchmod -R 777 sonar-project.properties\n\n```\n\n（2）创建SonarScanner容器扫描项目代码\n\n```shell\n docker run  --rm \\\n -v \"/home/code/vue-test:/usr/src\"  \\\n sonarsource/sonar-scanner-cli\n```\n\n根据项目代码的多少时间会有所不同，以下是扫描完成后的提示。\n\n> 09:13:23.107 WARN  This may lead to missing/broken features in SonarQube\n\n> 09:13:23.324 INFO  CPD Executor 61 files had no CPD blocks\n\n> 09:13:23.325 INFO  CPD Executor Calculating CPD for 325 files\n\n> 09:13:23.634 INFO  CPD Executor CPD calculation finished (done) | time=309ms\n\n> 09:13:23.641 INFO  SCM revision ID '4941a614714697243a5a8f7824fc921ff5f84345'\n\n> 09:13:24.131 INFO  Analysis report generated in 462ms, dir size=18.8 MB\n\n> 09:13:25.560 INFO  Analysis report compressed in 1429ms, zip size=8.3 MB\n\n> 09:14:10.518 INFO  Analysis report uploaded in 44955ms\n\n> 09:14:10.519 INFO  ANALYSIS SUCCESSFUL, you can find the results at: [http://192.168.32.12:9000/](http://192.168.32.12:9000/)[dashboard?id=test4](http://116.63.39.38:9000/dashboard?id=test4)\n\n> 09:14:10.519 INFO  Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report\n\n> 09:14:10.519 INFO  More about the report processing at [http://192.168.32.12:9000/api/ce/task?id=acfc4bc2-7b20-4166-bde6-93d970ac62b3](http://116.63.39.38:9000/api/ce/task?id=acfc4bc2-7b20-4166-bde6-93d970ac62b3)\n\n> 09:14:10.601 INFO  Analysis total time: 3:49.166 s\n\n> 09:14:10.602 INFO  SonarScanner Engine completed successfully\n\n> 09:14:10.636 INFO  EXECUTION SUCCESS\n\n> 09:14:10.637 INFO  Total time: 17:51.245s\n\n## **部署**SonarScanner集成PHP\n\n（1）在PHP项目的根目录下创建sonar-project.properties文件，并添加配置项\n\n```shell\nvi sonar-project.properties\n*****************************************************\nsonar.projectKey=sonarqube中创建的项目名称\nsonar.projectName=PHP项目名\nsonar.projectVersion=PHP项目版本\nsonar.sources=.  #PHP项目源文件的目录\nsonar.language=php   #项目语言\nsonar.sourceEncoding=UTF-8  #项目编码\nsonar.host.url=http://192.168.32.12:9000/ #sonarqube服务器端的地址\nsonar.token=sonarqube中创建项目的token\n*****************************************************\n#修改文件权限\nchmod -R 777 sonar-project.properties\n\n```\n\n（2）创建SonarScanner容器扫描项目代码\n\n```shell\n docker run  --rm \\\n -v \"/home/code/php-test:/usr/src\"  \\\n sonarsource/sonar-scanner-cli\n```\n\n根据项目代码的多少时间会有所不同，以下是扫描完成后的提示。\n\n> 09:13:23.107 WARN  This may lead to missing/broken features in SonarQube\n\n> 09:13:23.324 INFO  CPD Executor 61 files had no CPD blocks\n\n> 09:13:23.325 INFO  CPD Executor Calculating CPD for 325 files\n\n> 09:13:23.634 INFO  CPD Executor CPD calculation finished (done) | time=309ms\n\n> 09:13:23.641 INFO  SCM revision ID '4941a614714697243a5a8f7824fc921ff5f84345'\n\n> 09:13:24.131 INFO  Analysis report generated in 462ms, dir size=18.8 MB\n\n> 09:13:25.560 INFO  Analysis report compressed in 1429ms, zip size=8.3 MB\n\n> 09:14:10.518 INFO  Analysis report uploaded in 44955ms\n\n> 09:14:10.519 INFO  ANALYSIS SUCCESSFUL, you can find the results at: [http://192.168.32.12:9000/](http://192.168.32.12:9000/)[dashboard?id=test4](http://116.63.39.38:9000/dashboard?id=test4)\n\n> 09:14:10.519 INFO  Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report\n\n> 09:14:10.519 INFO  More about the report processing at [http://192.168.32.12:9000/api/ce/task?id=acfc4bc2-7b20-4166-bde6-93d970ac62b3](http://116.63.39.38:9000/api/ce/task?id=acfc4bc2-7b20-4166-bde6-93d970ac62b3)\n\n> 09:14:10.601 INFO  Analysis total time: 3:49.166 s\n\n> 09:14:10.602 INFO  SonarScanner Engine completed successfully\n\n> 09:14:10.636 INFO  EXECUTION SUCCESS\n\n> 09:14:10.637 INFO  Total time: 17:51.245s\n","tags":["Linux","docker","运维","DevOps","SonarQube","代码分析","SonarScanner"],"categories":["容器化","docker"]},{"title":"项目漏洞问题记录","url":"/yyg/8a0457d3/","content":"## **SSL 2.0 和 3.0 的漏洞修复：**\n\nSSL (Secure Sockets Layer) 是一种加密协议，曾被广泛用于网络通信中以确保数据的安全传输。SSL 2.0 和 3.0 是早期的版本，但它们已经被认为不再安全，因此被现代协议（如 TLS 1.2 和 TLS 1.3）所取代。\n\nSSL 2.0 和 3.0 的漏洞：\n\n- SSL 2.0：存在多个安全漏洞，包括较弱的加密算法、缺乏安全性验证等。它已经被广泛弃用。\n- SSL 3.0：虽然相较于 SSL 2.0 改进了加密算法，但仍然存在诸如 POODLE 攻击（Padding Oracle On Downgraded Legacy Encryption）等严重的安全问题，因此它也已经不再推荐使用。\n\n如何检测 SSL 2.0 和 SSL 3.0：\n\n1. 使用工具扫描：许多安全扫描工具（如 OpenSSL）可以帮助检测网站是否支持 SSL 2.0 和 SSL 3.0。\n\n```\nopenssl s_client -connect <hostname>:443 -ssl2\nopenssl s_client -connect <hostname>:443 -ssl3\n```\n\n如果连接成功，表示该协议被服务器支持。\n\n1. 服务器配置检查：你可以检查服务器上的 SSL/TLS 配置，确保不再支持 SSL 2.0 或 SSL 3.0。\n\n- 对于 Apache，检查 `ssl.conf` 或 `httpd.conf` 文件中的 `SSLProtocol` 配置：\n\n```\nSSLProtocol all -SSLv2 -SSLv3\n```\n\n- 对于 Nginx，检查 `nginx.conf` 文件中的 `ssl_protocols` 配置：\n\n```\nssl_protocols TLSv1.2 TLSv1.3;\n```\n\n1. 在浏览器开发者工具中检查：你也可以通过浏览器的开发者工具（如 Chrome 或 Firefox）来查看网络请求的加密协议版本。在开发者工具的“网络”选项卡中，查看 HTTPS 请求的详细信息，协议版本会在响应头部显示。\n2. 自动化扫描工具：可以使用自动化工具（如 Nmap）来扫描服务器支持的 SSL/TLS 协议。例如：\n\n```\nnmap --script ssl-enum-ciphers -p 443 <hostname>\n```\n\n## **\"SWEET32\" ：**\n\n\"SWEET32\" 是一种针对 TLS 和 SSL 协议中的 中等强度加密套件（如 3DES）的攻击方式，主要用于 2-块碰撞攻击（birthday bound attack）。具体来说，它影响的是 使用 64 位块大小的加密算法，比如 3DES 和一些较老的加密套件。\n\n### 为什么中等强度加密套件不安全？\n\n- SWEET32 攻击原理： 由于 3DES（和其他基于 64 位块的加密算法）的设计缺陷，攻击者可以在长时间的数据流量中找到加密数据块的碰撞，从而泄露信息。攻击的成功概率随着加密数据量的增加而增加。\n- 推荐的做法： 由于这种攻击在大数据传输中更容易成功（比如长时间的 HTTPS 会话），现代的最佳实践是禁用所有 64 位块大小的加密算法，如 3DES。\n\n查看服务器支持的所有加密套件：\n\n```\nopenssl s_client -connect example.com:443 -cipher 'ALL'\n```\n\n### 解决方法\n\n1. 禁用中等强度的加密套件（如 3DES）： 服务器应该配置为只支持 强加密套件，例如 AES 加密和 ChaCha20，同时禁用 3DES、RC4 等不安全的套件。\n\n如果你有访问服务器配置的权限，可以根据使用的 Web 服务器（如 Apache、Nginx 等）进行相应配置。\n\n例如，在 Apache 中，你可以在 `ssl.conf` 文件中禁用 3DES 和其他弱套件：\n\n```\nSSLCipherSuite HIGH:!aNULL:!MD5:!3DES\n```\n\n对于 Nginx，可以使用类似的配置：\n\n```\nssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:!3DES';\n```\n\n1. 启用现代的加密套件： 推荐启用 AES 或 ChaCha20 等现代加密套件。确保启用的套件符合 TLS 1.2 或 TLS 1.3 的标准。\n\n例如，下面是一个推荐的加密套件配置，它仅启用了强加密算法：\n\n```\nSSLProtocol TLSv1.2 TLSv1.3\nSSLCipherSuite ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384\n```\n\n1. 升级到 TLS 1.2 或 TLS 1.3： 确保服务器仅支持 TLS 1.2 或 TLS 1.3，而 SSLv3 和 TLS 1.0/1.1 应该被禁用。现代的加密套件和协议版本可以有效地降低 SWEET32 攻击的风险。\n\n在服务器的配置文件ssl.conf中，确保禁用了旧的协议版本：\n\n```\nSSLProtocol TLSv1.2 TLSv1.3\n```","tags":["运维","项目问题总结","漏洞扫描"],"categories":["梳理总结","项目问题总结"]},{"title":"Windows 10（x86_64）使用QEMU安装ARM虚拟机","url":"/yyg/a785f75e/","content":"\n## 基础环境\n\n系统：Windows 10\n\n硬件：双网卡（本篇中使用的是两个无线网卡）\n\n所需软件：QEMU、tap-windows\n\n所需软件固件：QEMU\\_EFI.fd\n\nARM镜像：openEuler-22.03-LTS-SP4-aarch64-dvd.iso\n\n## 环境准备\n\n1、进入QEMU官网[https://qemu.weilnetz.de/](https://qemu.weilnetz.de/)，下载exe程序，本篇中使用的是`qemu-w64-setup-20240903.exe`， 按引导进行安装。（最新版无需配置环境变量）\n\n打开命令提示符输入`qemu-system-aarch64 -version`，查看到其详细版本号即为安装成功。\n\n> QEMU emulator version 9.1.0 (v9.1.0-12064-gc658eebf44)\n\n> Copyright (c) 2003-2024 Fabrice Bellard and the QEMU Project developers\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/1.png)\n\n2、下载QEMU\\_EFI.fd固件，下载地址：[https://releases.linaro.org/components/kernel/uefi-linaro/16.02/release/qemu64/](https://releases.linaro.org/components/kernel/uefi-linaro/16.02/release/qemu64/)。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/2.png)\n\n3、下载ARM镜像，进入OpenEuler官网，在镜像仓列表中找到并下载openEuler-22.03-LTS-SP4-aarch64-dvd.iso镜像。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/3.png)\n\n4、安装tap-windows，安装完成后在本机的网卡适配器中会出现一个为`TAP-Windows Adapter V9`的网卡，并将其名改为`tap0`。（刚安装时网卡为未连接状态）\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/4.png)\n\n5、共享`本机无线网卡（本篇中名为WLAN 1）`的网络。右击本机无线网卡（WLAN 1）的属性，在WLAN属性-共享中将网络共享给`tap0`。如下图。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/5.png)\n\n## 安装过程\n\n1、将QEMU\\_EFI.fd和openEuler-22.03-LTS-SP4-aarch64-dvd.iso镜像放在同一文件夹中，在该文件夹下打开`命令指示符`。\n\n2、输入命令为虚拟机创建一个虚拟磁盘文件。\n\n```yaml\nqemu-img.exe create -f qcow2 \"F:\\arm-os-test\\openeuler.qcow2\" 60G\n```\n\n> * `qemu-img.exe`: 调用 QEMU 的图像管理工具。\n> \n> * `create`: 指令用于创建一个新的磁盘映像文件。\n> \n> * `-f qcow2`: 指定要创建的文件格式为 QCOW2，这是一种支持压缩和快照的格式。\n> \n> * `\"F:\\arm-os-test\\openeuler.qcow2\"`: 指定虚拟硬盘的文件路径及名称。\n> \n> * `60G`: 设置虚拟硬盘的大小为 60 GB。\n\n命令执行后会在该文件夹下生成一个名为`openeuler.qcow2`的磁盘文件。\n\n3、执行命令，安装openEuler。\n\n```yaml\n qemu-system-aarch64.exe -m 8192 -cpu cortex-a72 -smp 4,sockets=2,cores=2 -M virt -bios \"F:\\arm-os-test\\QEMU_EFI.fd\" -net nic -net tap,ifname=tap0 -device VGA -device nec-usb-xhci -device usb-mouse -device usb-kbd -drive if=none,file=\"F:\\arm-os-test\\openeuler.qcow2\",id=hd0 -device virtio-blk-device,drive=hd0 -drive if=none,file=\"F:\\arm-os-test\\openEuler-22.03-LTS-SP4-aarch64-dvd.iso\",id=cdrom,media=cdrom -device virtio-scsi-device -device scsi-cd,drive=cdrom\n```\n\n> * `qemu-system-aarch64.exe`: 启动 AArch64 架构的 QEMU 模拟器。\n> \n> * `-m 8192`: 分配 8 GB 的内存给虚拟机。\n> \n> * `-cpu cortex-a72`: 指定使用 Cortex-A72 CPU 模型。\n> \n> * `-smp 4,sockets=2,cores=2`: 配置 4 个 CPU 核心，分为 2 个插槽，每个插槽 2 个核心。\n> \n> * `-M virt`: 使用“virt”机器类型，适合虚拟化。\n> \n> * `-bios \"F:\\arm-os-test\\QEMU_EFI.fd\"`: 指定 EFI BIOS 文件。\n> \n> * `-net nic -net tap,ifname=tap0`: 设置网络，使用 NIC 和 tap 接口。\n> \n> * `-device VGA`: 添加 VGA 显示设备。\n> \n> * `-device nec-usb-xhci`: 添加 USB 控制器。\n> \n> * `-device usb-mouse -device usb-kbd`: 添加 USB 鼠标和键盘设备。\n> \n> * `-drive if=none,file=\"F:\\arm-os-test\\openeuler.qcow2\",id=hd0 -device virtio-blk-device,drive=hd0`: 配置一个 VirtIO 块设备作为主磁盘。\n> \n> * `-drive if=none,file=\"F:\\arm-os-test\\openEuler-22.03-LTS-SP4-aarch64-dvd.iso\",id=cdrom,media=cdrom`: 设置 CD-ROM 驱动器，使用指定的 ISO。\n> \n> * `-device virtio-scsi-device -device scsi-cd,drive=cdrom`: 添加 SCSI 控制器，并连接 CD-ROM 驱动器。\n\n执行命令后，在弹出的系统安装界面，按引导进行系统的配置和安装。\n\n## 连接互联网\n\n1、系统安装完成后，在tap0网卡的属性中查看ip地址，将ip地址改为与主机同一网段的ip，DNS和网关与主机一致。执行命令启动系统。\n\n```yaml\nqemu-system-aarch64.exe -m 8192 -cpu cortex-a72 -smp 4,sockets=2,cores=2 -M virt -bios \"F:\\arm-os-test\\QEMU_EFI.fd\" -net nic -net tap,ifname=tap0 -device VGA -device nec-usb-xhci -device usb-mouse -device usb-kbd -drive if=none,file=\"F:\\arm-os-test\\openeuler.qcow2\",id=hd0 -device virtio-blk-device,drive=hd0\n```\n\n> 启动系统命令是将安装命令中去除iso挂载部分去除得到的\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/6.png)\n\n2、配置系统IP地址。使用tap0网卡的地址作为网关，配置完成并启用后，使用ssh工具连接。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/7.png)\n\n> <mark>若出现测试网络联通性失败的情况，进行以下操作：</mark>\n> \n> <mark>1、取消共享无线网卡的网络</mark>\n> \n> <mark>2、重新共享网络</mark>\n> \n> <mark>3、启动虚拟机</mark>\n\n8、测试网络的连通性，可成功访问网络。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/8.png)\n\n此时安装的ARM架构的虚拟机可正常访问网络，但进行发布的业务无法在本地主机访问。即虚拟机可访问互联网，但无法访问本地主机。\n\n## 本地主机连通虚拟机业务\n\n要成功访问虚拟机中发布的业务，需要使用一个新的无线网卡（本篇中名为WLAN 2），用来和tap0进行`桥接`。\n\n1、将两张无线网卡连接同一网络中，在控制面板-网络连接中，按住`Ctrl`,分别选中网卡`tap0`和`WLAN 2` ，右键选择桥接，创建一个网桥。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/9.png)\n\n> <mark>当tap0和WLAN 2都显示“已启用，桥接的”，且网桥处显示连接到的无线网络名称时，在本地主机和虚拟机中均可ping通对方地址。（此时本地主机的ip应使用WLAN分配的）</mark>\n\n2、右键查看网桥属性，在`Internet协议版本 4(TCP/IPv4)属性`中可查看到网关地址与本机一致。\n\n![image.png](Windows 10（x86_64）使用QEMU安装ARM虚拟机/10.png)\n\n3、将虚拟机的网关修改为网关对应地址，关闭虚拟机。重新使用命令启动虚拟机。\n\n若虚拟机和本地主机均可ping通互联网且可相互ping通，此时在本地主机可正常访问虚拟机发布业务。","tags":["x86环境安装ARM虚拟机","QEMU","x86-ARM"],"categories":["Linux运维","梳理总结","知识梳理","QEMU"]},{"title":"eNSP模拟网络设备访问真实网络","url":"/yyg/f5b762ea/","content":"## 添加Loopback环回网卡\n\n（1）使用Win+ R调出”运行“，输入`hdwwiz`，确定后开始添加新硬件向导。\n\n（2）按向导操作，选择`安装我手动从列表选择的硬件(高级)(M)`。\n\n![image.png](eNSP模拟网络设备访问真实网络/1.png)\n\n（3）选择安装的硬件类型为`网络适配器`，选定设备驱动程序的厂商为`Microsoft`，型号为`Microsoft KM-TEST环回适配器`。\n\n![image.png](eNSP模拟网络设备访问真实网络/2.png)\n\n（4）按向导安装，点击完成即可。\n\n## 共享主机网络\n\n（1）打开控制面板--网络和Internet--网络连接，找到新创建的环回网卡，本主机中为`以太网 8`。选择本机连接网络，此外为WLAN，在属性中选择共享，勾选`允许其他网络用户通过此计算机的Internet连接来连接(N)`，选择新创建的环回网卡`以太网 8`，保存即可。\n\n![image.png](eNSP模拟网络设备访问真实网络/3.png)\n\n（2）保存后，会默认给环回网卡分配ip地址`192.168.137.1`。重启网卡使得配置生效。\n\n![image.png](eNSP模拟网络设备访问真实网络/4.png)\n\n（3）打开eNSP软件，打开Cloud的设置，分别创建一个绑定信息为`UDP`和环回网卡`以太网 8 -- IP：192.168.137.1`的端口，并设置端口映射的入端口和出端口编号，勾选双向通道，点击增加。\n\n![image.png](eNSP模拟网络设备访问真实网络/5.png)\n\n（4）开启网络拓扑，设置主机为DHCP，等待主机分配出ip后，使用ping命令验证是否连接外部真实网络。\n\n![image.png](eNSP模拟网络设备访问真实网络/6.png)\n\n![image.png](eNSP模拟网络设备访问真实网络/7.png)","tags":["ensp模拟仿真","网络设备"],"categories":["Linux运维","梳理总结","知识梳理","网络分析"]},{"title":"Linux实现模拟IPMI","url":"/yyg/9ec93abd/","content":"## 环境准备：\n\n（1）使用脚本安装docker、docker-compose\n\n```shell\nbash <(curl -sSL https://linuxmirrors.cn/docker.sh)\n```\n\n（2）配置镜像加速\n\n```shell\nvi /etc/docker/daemon.json\n\n{\n  \"data-root\": \"/data/dockerData\",\n  \"registry-mirrors\": [\"https://registry.cn-hangzhou.aliyuncs.com\",\n    \"https://huecker.io\",\n    \"https://docker.rainbond.cc\",\n    \"https://dockerhub.timeweb.cloud\",\n    \"https://dockerhub.icu\",\n    \"https://docker.registry.cyou\",\n    \"https://docker-cf.registry.cyou\",\n    \"https://dockercf.jsdelivr.fyi\",\n    \"https://docker.jsdelivr.fyi\",\n    \"https://dockertest.jsdelivr.fyi\",\n    \"https://mirror.aliyuncs.com\",\n    \"https://dockerproxy.com\",\n    \"https://mirror.baidubce.com\",\n    \"https://docker.m.daocloud.io\",\n    \"https://docker.nju.edu.cn\",\n    \"https://docker.mirrors.sjtug.sjtu.edu.cn\",\n    \"https://docker.mirrors.ustc.edu.cn\",\n    \"https://mirror.iscas.ac.cn\",\n    \"https://docker.rainbond.cc\",\n    \"https://docker.kubesre.xyz\"],\n    \"log-driver\":\"json-file\",\n    \"log-opts\":{\"max-size\" :\"50m\",\"max-file\":\"3\"}\n}\n```\n\n（3）启动docker服务\n\n```shell\nsystemctl start docker\nsystemctl enable docker\nsystemctl status docker\n```\n\n## 部署IPMI Simulator容器\n\n（1）在IPMI Simulator官方仓库中下载源码包\n\nhttps://github.com/vapor-ware/ipmi-simulator\n\n（2）下载完成并解压后，修改Dockerfile文件\n\n```dockerfile\nFROM alpine\nLABEL maintainer=\"vapor@vapor.io\"\n\nRUN apk --update --no-cache add openipmi-lanserv\n\n# Create the directories that will be used to persist state information\n# for the IPMI simulator instance.\nRUN mkdir -p /tmp/chassis\n\nCOPY . /tmp/ipmisim\n\n# 新加部分\nRUN chmod +x /tmp/ipmisim/bin/chassis_control.sh\n# 新加部分\nEXPOSE 623\n\nEXPOSE 623/udp\n\nCMD [\"ipmi_sim\", \"-n\", \"-c\", \"/tmp/ipmisim/lan.conf\", \"-f\", \"/tmp/ipmisim/sim.emu\"]\n```\n\n（3）生成镜像文件，运行容器\n\n```shell\ndocker build -f Dockerfile -t vaporio/ipmi-simulator .\n```\n\n```shell\ndocker run -d --name ipmi-simulator --restart always -p 623:623/udp -p 623:623/tcp vaporio/ipmi-simulator\n```\n\n（4） 配置防火墙策略\n\n```shell\nfirewall-cmd --add-port=623/tcp --permanent\nfirewall-cmd --add-port=623/udp --permanent\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all\n```\n\n（5）在主机上查看模拟IPMI的底盘和电源状态\n\n```shell\n ipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN chassis status\n```\n\n> System Power         : on\n> Power Overload       : false\n> Power Interlock      : inactive\n> Main Power Fault     : false\n> Power Control Fault  : false\n> Power Restore Policy : always-off\n> Last Power Event     :\n> Chassis Intrusion    : inactive\n> Front-Panel Lockout  : inactive\n> Drive Fault          : false\n> Cooling/Fan Fault    : false\n\n扩展命令：\n\nIPMI设备的电源状态\n\n```shell\n#打开\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN chassis power on\n#关闭\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN chassis power off\n#重启\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN chassis power reset\n#标识机箱（使指示灯闪烁）\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN chassis identify 5\n```\n\n读取系统状态\n\n```shell\n#显示系统传感器\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN sensor list\n#显示系统现有可替代器件列表\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN fru list\n#显示系统SDR Repository设备列表\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN sdr list\n#显示系统平台事件过滤的列表\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN pef filter list\n#显示系统平台策略列表\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN pef policy list\n```\n\n系统日志\n\n```shell\n#显示所有系统事件日志\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN sel elist\n#显示当前BMC的时间\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN sel time get\n#修改当前BMC的时间\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN sel time set \"09/25/2024 18:29:20\"\n\n\n```\n\n系统相关\n\n```shell\n#查看BMC信息\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN bmc info\n#BMC冷启动\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN bmc reset cold\n#BMC启动\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN bmc reset warm\n#显示系统默认通道\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN channel info\n#显示所有用户\nipmitool -I lan -H 主机ip -p 623 -U ADMIN -P ADMIN user list\n```\n","tags":["Linux","docker","IPMI","运维"],"categories":["容器化","docker"]},{"title":"Slim缩小容器镜像大小","url":"/yyg/fd02fe22/","content":"## 前言\n\n{% folding cyan,🔜Slim🔚 %}\n\n[GitHub - slimtoolkit/slim： Slim（toolkit）：不要更改容器镜像中的任何内容，并将其缩小多达 30 倍（对于编译语言甚至更多），使其也安全！（免费和开源）](https://github.com/slimtoolkit/slim)\n\nSlim是一个在不更改容器镜像的情况下，将其大小缩小多大30倍的容器工具。并且该工具兼容多种平台和架构。本文主要介绍Linux平台X86_64架构的Slim的使用。\n\ngithub上官方给出了以下容器镜像优化前后大小的示例：\n\n> Node.js应用程序映像：\n> \n> - 从 ubuntu：14.04 - 432MB => 14MB （缩小 **30.85X**)\n> - 来自 debian：jessie - 406MB => 25.1MB （缩小 **16.21X**）)\n> - 来自 node：alpine - 66.7MB => 34.7MB（缩小 **1.92 倍**）)\n> - 来自 node：distroless - 72.7MB => 39.7MB （缩小 **1.83 倍）**)\n> \n> Python 应用程序映像：\n> \n> - 从 ubuntu：14.04 - 438MB => 16.8MB （缩小 **25.99X**)\n> - 来自 python：2.7-alpine - 84.3MB => 23.1MB （缩小 **3.65 倍）**)\n> - 从 python：2.7.15 - 916MB => 27.5MB （缩小 **33.29X**)\n> - 从 centos：7 - 647MB => 23MB （缩小 **28.57 倍）**)\n> - 从 centos/python-27-centos7 - 700MB => 24MB（缩小 **29.01X**）)\n> - 从 python2.7：distroless 开始 - 60.7MB => 18.3MB （缩小 **3.32 倍）**)\n> \n> Ruby 应用程序映像：\n> \n> - 从 ubuntu：14.04 - 433MB => 13.8MB （缩小 **31.31X**)\n> - 来自 ruby：2.2-alpine - 319MB => 27MB （缩小 **11.88X**)\n> - 从 ruby：2.5.3 - 978MB => 30MB （缩小 **32.74X**)\n> \n> Go 应用程序映像：\n> \n> - 来自 golang：latest - 700MB => 1.56MB （缩小 **448.76X**)\n> - 从 ubuntu：14.04 - 531MB => 1.87MB （缩小 **284.10X**)\n> - 来自 golang：alpine - 258MB => 1.56MB （缩小 **165.61X**)\n> - 从 centos：7 - 615MB => 1.87MB （缩小 **329.14X**)\n> \n> Rust 应用程序镜像：\n> \n> - 从 rust：1.31 - 2GB => 14MB （缩小 **147.16X**)\n> \n> Java 应用程序映像：\n> \n> - 从 ubuntu：14.04 - 743.6 MB => 100.3 MB\n> \n> PHP 应用程序映像：\n> \n> - 从 php：7.0-cli - 368MB => 26.6MB （缩小 **13.85 倍**）)\n> \n> Haskell 应用程序图像：\n> \n> - （Scotty 服务） 从 haskell 开始：8 - 2.09GB => 16.6MB （缩小 **125.32X**)\n> - （Scotty 服务） 从 haskell 开始：7 - 1.5GB => 21MB （缩小 71X）\n> \n> Elixir 应用程序图片：\n> \n> - （Phoenix 服务） 从 elixir：1.6 - 1.1 GB => 37 MB （缩小 **29.25 倍）**)\n\n{% endfolding %}\n\n## 安装并使用Slim\n\nSlim提供了多种安装方式，包括解二进制文件解压安装、使用脚本一键安装、拉取镜像安装。此处以nginx、tomcat、httpd镜像为例，进行镜像缩小。\n\n### 二进制文件解压安装\n\n```shell\n# 下载二进制文件\ncurl -L -o ds.tar.gz https://github.com/slimtoolkit/slim/releases/download/1.40.11/dist_linux.tar.gz\n#解压文件并移动安装程序\ntar -xvf ds.tar.gz\nmv  dist_linux/slim /usr/local/bin/\nmv  dist_linux/slim-sensor /usr/local/bin/\n#查看版块\nslim -v\n\n\n#执行缩小命令\nslim build 镜像ID \n```\n\n### 脚本一键安装\n\n此方式可在 Linux（x86 和 ARM）和 macOS（x86 和 Apple Silicon）上使用。\n\n```shell\ncurl -sL https://raw.githubusercontent.com/slimtoolkit/slim/master/scripts/install-slim.sh | sudo -E bash -\n\n#执行缩小命令\nslim build 镜像ID\n```\n\n镜像缩小前大小\n\n> docker images\n> REPOSITORY   TAG       IMAGE ID       CREATED       SIZE\n> <mark>nginx</mark>        latest    39286ab8a5e1   3 weeks ago   <mark>188MB</mark>\n> <mark>tomcat</mark>       latest    c2a444ea6cd7   4 weeks ago   <mark>508MB</mark>\n\n执行命令缩小\n\n> <mark> slim build 39286ab8a5e1 </mark>\n> cmd=build info=param.http.probe message='using default probe'\n> cmd=build state=started\n> cmd=build info=params image-build-engine='internal' target.type='image' target.image='39286ab8a5e1' continue.mode='probe' rt.as.user='true' keep.perms='true' tags=''\n> cmd=build state=image.inspection.start\n> cmd=build info=image id='sha256:39286ab8a5e14aeaf5fdd6e2fac76e0c8d31a0c07224f0ee5e6be502f12e93f3' size.bytes='187706879' size.human='188 MB'\n> cmd=build info=image.stack id='sha256:39286ab8a5e14aeaf5fdd6e2fac76e0c8d31a0c07224f0ee5e6be502f12e93f3' index='0' name='nginx:latest'\n> ......\n> \n> <mark>slim build c2a444ea6cd7</mark>\n> cmd=build info=param.http.probe message='using default probe'\n> cmd=build state=started\n> cmd=build info=params continue.mode='probe' rt.as.user='true' keep.perms='true' tags='' image-build-engine='internal' target.type='image' target.image='c2a444ea6cd7'\n> cmd=build state=image.inspection.start\n> cmd=build info=image size.bytes='507583233' size.human='508 MB' id='sha256:c2a444ea6cd7a0df0600cbd7ed249e1b5f8635ec7f55420a07444703f0aab70e'\n> cmd=build info=image.stack id='sha256:c2a444ea6cd7a0df0600cbd7ed249e1b5f8635ec7f55420a07444703f0aab70e' index='0' name='tomcat:latest'\n> cmd=build info=image.exposed_ports list='8080/tcp'\n> cmd=build state=image.inspection.done\n> cmd=build state=container.inspection.start\n> cmd=build info=container status='created' name='slimk_5648_20240905085045' id='a62eca6f410e99620c4050b063ab6d8ebe1d0b60340e0ac47da88dce4902f97c'\n> ......\n\n镜像缩小后大小\n\n>  docker images\n> REPOSITORY    TAG       IMAGE ID       CREATED          SIZE\n> <mark>tomcat.slim</mark>   latest    97901079a904   21 seconds ago   <mark>212MB</mark>\n> <mark>nginx.slim</mark>    latest    9131fc2c397d   3 minutes ago    <mark>13.3MB</mark>\n> <mark>nginx </mark>        latest    39286ab8a5e1   3 weeks ago      <mark>188MB</mark>\n> <mark>tomcat</mark>        latest    c2a444ea6cd7   4 weeks ago      <mark>508MB</mark>\n\n### 容器安装\n\n```shell\n#拉取镜像\ndocker pull dslim/slim\n#执行缩小命令\ndocker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock dslim/slim build 镜像ID\n```\n\n镜像缩小前大小\n\n> docker images\n> REPOSITORY    TAG       IMAGE ID       CREATED          SIZE\n> tomcat.slim   latest    97901079a904   7 minutes ago    212MB\n> nginx.slim    latest    9131fc2c397d   10 minutes ago   13.3MB\n> nginx         latest    39286ab8a5e1   3 weeks ago      188MB\n> tomcat        latest    c2a444ea6cd7   4 weeks ago      508MB\n> <mark>httpd </mark>        latest    9cb0a2315602   7 weeks ago      <mark>148MB</mark>\n> dslim/slim    latest    6205a57fba8b   7 months ago     56.3MB\n\n执行镜像缩小命令\n\n> <mark>docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock dslim/slim</mark> <mark>build  9cb0a2315602</mark>\n> \n> cmd=build info=param.http.probe message='using default probe'\n> cmd=build state=started\n> cmd=build info=params continue.mode='probe' rt.as.user='true' keep.perms='true' tags='' image-build-engine='internal' target.type='image' target.image='9cb0a2315602'\n> cmd=build state=image.inspection.start\n> cmd=build info=image id='sha256:9cb0a231560203a9b46325ef6dfe0d21d524813acb74447fd276b9813a9fdd44' size.bytes='148177949' size.human='148 MB'\n> cmd=build info=image.stack index='0' name='httpd:latest' id='sha256:9cb0a231560203a9b46325ef6dfe0d21d524813acb74447fd276b9813a9fdd44'\n> cmd=build info=image.exposed_ports list='80/tcp'\n> cmd=build state=image.inspection.done\n> cmd=build state=container.inspection.start\n> cmd=build info=container status='created' name='slimk_1_20240905090155' id='6d058318cd7729d4e3831a4aecb5d2884853bc2efbf1d6a6d233f04ade3225a5'\n> cmd=build info=container status='running' name='slimk_1_20240905090155' id='6d058318cd7729d4e3831a4aecb5d2884853bc2efbf1d6a6d233f04ade3225a5'\n> cmd=build info=container message='obtained IP address' ip='172.17.0.3'\n> cmd=build info=cmd.startmonitor status='sent'\n> cmd=build info=event.startmonitor.done status='received'\n> cmd=build info=container name='slimk_1_20240905090155' id='6d058318cd7729d4e3831a4aecb5d2884853bc2efbf1d6a6d233f04ade3225\n> \n> ......\n\n镜像缩小后大小\n\n>  docker images\n> REPOSITORY    TAG       IMAGE ID       CREATED          SIZE\n> <mark>httpd.slim</mark>    latest    692af09a91c2   57 seconds ago  <mark> 8.13MB</mark>\n> tomcat.slim   latest    97901079a904   9 minutes ago    212MB\n> nginx.slim    latest    9131fc2c397d   12 minutes ago   13.3MB\n> nginx         latest    39286ab8a5e1   3 weeks ago      188MB\n> tomcat        latest    c2a444ea6cd7   4 weeks ago      508MB\n> <mark>httpd </mark>        latest    9cb0a2315602   7 weeks ago     <mark> 148MB</mark>\n> dslim/slim    latest    6205a57fba8b   7 months ago     56.3MB\n","tags":["Linux","docker","运维","Slim","缩小容器镜像大小"],"categories":["容器化","docker"]},{"title":"docker 部署 SkyWalking","url":"/yyg/f5933c9f/","content":"\n## 前言\n\n{% folding cyan,🔜什么是SkyWalking？🔚 %}\n\n> 是一款优秀的国产 APM 工具。\n> \n> 分布式系统的应用程序性能监视工具，专为微服务、云原生架构和基于容器（Docker、K8s、Mesos）架构而设计。\n> \n> 提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。\n> \n> SkyWalking架构分为UI、OAP、存储、探针4部分。\n> \n> 其中UI为<mark>SkyWalking UI </mark>：负责提供控台、查看链路等等；<mark>（可视化显示）</mark>\n> \n> OAP为<mark>SkyWalking OAP </mark>：负责接收 Agent（探针） 发送的 Tracing 数据信息，然后进行<mark>分析</mark>(Analysis Core) ，存储到外部存储器( Storage )，最终提供查询( Query )功能。<mark>（数据分析）</mark>\n> \n> <mark>存储</mark>为Tracing 数据存储。目前支持 ES、MySQL、Sharding Sphere、TiDB、H2 多种存储器。而我们目前采用的是 ES ，主要考虑是 SkyWalking 开发团队自己的生产环境采用 ES 为主。<mark>（存储数据）</mark>\n> \n> 探针为 <mark>Agent </mark>：负责从应用中，收集链路信息，发送给 SkyWalking OAP 服务器。目前支持 SkyWalking、Zikpin、Jaeger 等提供的 Tracing 数据信息。而我们目前采用的是，SkyWalking Agent 收集 SkyWalking Tracing 数据，传递给服务器。<mark>（收集数据）</mark>\n\n{% endfolding %}\n\n## 环境准备\n\n（1）使用脚本安装docker、docker-compose\n\n```shell\nbash <(curl -sSL https://linuxmirrors.cn/docker.sh)\n```\n\n（2）配置镜像加速\n\n```json\nvi /etc/docker/daemon.json\n\n{\n  \"data-root\": \"/data/dockerData\",\n  \"registry-mirrors\": [\"https://registry.cn-hangzhou.aliyuncs.com\",\n    \"https://huecker.io\",\n    \"https://docker.rainbond.cc\",\n    \"https://dockerhub.timeweb.cloud\",\n    \"https://dockerhub.icu\",\n    \"https://docker.registry.cyou\",\n    \"https://docker-cf.registry.cyou\",\n    \"https://dockercf.jsdelivr.fyi\",\n    \"https://docker.jsdelivr.fyi\",\n    \"https://dockertest.jsdelivr.fyi\",\n    \"https://mirror.aliyuncs.com\",\n    \"https://dockerproxy.com\",\n    \"https://mirror.baidubce.com\",\n    \"https://docker.m.daocloud.io\",\n    \"https://docker.nju.edu.cn\",\n    \"https://docker.mirrors.sjtug.sjtu.edu.cn\",\n    \"https://docker.mirrors.ustc.edu.cn\",\n    \"https://mirror.iscas.ac.cn\",\n    \"https://docker.rainbond.cc\",\n    \"https://docker.kubesre.xyz\"],\n    \"log-driver\":\"json-file\",\n    \"log-opts\":{\"max-size\" :\"50m\",\"max-file\":\"3\"}\n}\n```\n\n（3）启动docker服务\n\n```shell\nsystemctl start docker\nsystemctl enable docker\nsystemctl status docker\n```\n\n（4）设置与内存映射相关的内核参数为262144，查看应用到系统的内核参数。\n\n```shell\necho \"vm.max_map_count=262144\" >> /etc/sysctl.conf\nsysctl -p\n```\n\n## 部署步骤\n\n（1）创建部署文件所需的存储目录\n\n```shell\nmkdir -p /data/elasticsearch/data /data/elasticsearch/logs /data/skywalking/oap\nchmod -R 777 /data/elasticsearch\n```\n\n（2）创建临时skywalking-oap-server容器，将skywalking配置文件拷贝到映射目录中。\n\n```shell\ncd /data/skywalking/oap\n# 创建临时skywalking-oap-server容器，拷贝skywalking配置文件到主机目录\ndocker run -itd --name=oap-temp apache/skywalking-oap-server:9.5.0\ndocker cp  oap-temp:/skywalking/config/. .\ndocker rm -f oap-temp\n```\n\n（3）修改skywalking的配置文件application.yml，将elasticsearch作为数据存储。\n\n```shell\nvi application.yml\n\nstorage:\n  selector: ${SW_STORAGE:elasticsearch} #将h2修改为elasticsearch\n  elasticsearch:\n    namespace: ${SW_NAMESPACE:\"\"}\n    clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:本机ip:9200} #localhost修改为主机ip\n    protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\"http\"}\n    connectTimeout: ${SW_STORAGE_ES_CONNECT_TIMEOUT:3000}\n    socketTimeout: ${SW_STORAGE_ES_SOCKET_TIMEOUT:30000}\n    responseTimeout: ${SW_STORAGE_ES_RESPONSE_TIMEOUT:15000}\n    numHttpClientThread: ${SW_STORAGE_ES_NUM_HTTP_CLIENT_THREAD:0}\n    user: ${SW_ES_USER:\"elastic\"} #填写es的账号\n    password: ${SW_ES_PASSWORD:\"elastic\"} #填写es密码\n    trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\"\"}\n    trustStorePass: ${SW_STORAGE_ES_SSL_JKS_PASS:\"\"}\n```\n\n（4）创建docker-compose文件，编排部署skywalking、es、skywalking-ui。\n\n```yaml\nvi skywalking.yml\n\nservices:\n  elasticsearch:\n    image: elasticsearch:8.15.0\n    container_name: elasticsearch\n    restart: always\n    environment:\n      - discovery.type=single-node\n      - ES_JAVA_OPTS=-Xms1g -Xmx1g\n      - ELASTIC_PASSWORD=elastic\n      - TZ=Asia/Shanghai\n    ports:\n      - \"9200:9200\"\n      - \"9300:9300\"\n    healthcheck:\n      test: [ \"CMD-SHELL\", \"curl --silent --fail -u elastic:elasitc localhost:9200/_cluster/health || exit 1\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 10s\n  logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"50m\"\n        max-file: \"3\"\n    volumes:\n      - /data/elasticsearch/data:/usr/share/elasticsearch/data\n      - /data/elasticsearch/logs:/usr/share/elasticsearch/logs\n      - /data/elasticsearch/plugins:/usr/share/elasticsearch/plugins\n    networks:\n      skywalking-network:\n        ipv4_address: 172.20.110.11\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n\n  skywalking-oap:\n    image: apache/skywalking-oap-server:9.5.0\n    container_name: skywalking-oap\n    restart: always\n    ports:\n      - \"11800:11800\"\n      - \"12800:12800\"\n      - \"1234:1234\"  \n    healthcheck:\n      test: [ \"CMD-SHELL\", \"/skywalking/bin/swctl health\" ]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 10s\n    depends_on:\n      elasticsearch:\n        condition: service_healthy\n    environment:\n      - SW_STORAGE=elasticsearch\n      - SW_HEALTH_CHECKER=default\n      - TZ=Asia/Shanghai\n      - JVM_Xms=512M\n      - JVM_Xmx=1024M\n      - SW_STORAGE_ES_CLUSTER_NODES=本机ip:9200\n    volumes:\n      - /data/skywalking/oap:/skywalking/config\n    networks:\n      skywalking-network:\n        ipv4_address: 172.20.110.12\n\n\n  skywalking-ui:\n    image: apache/skywalking-ui:9.5.0\n    container_name: skywalking-ui\n    restart: always\n    environment:\n      - SW_OAP_ADDRESS=http://本机ip:12800\n      - SW_ZIPKIN_ADDRESS=http://本机ip:9412\n      - TZ=Asia/Shanghai\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      skywalking-oap:\n        condition: service_healthy\n    networks:\n      skywalking-network:\n        ipv4_address: 172.20.110.13\n\nnetworks:\n  skywalking-network:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.110.0/24\n```\n```shell\ndocker compose -f skywalking.yml up -d\n```\n\nSkywalking可通过以下两种方式连接es，作为存储。（使用其中一种即可）\n\n**Skywalking 通过 HTTP认证连接 Elasticsearch**\n\n```shell\n#关闭es的ssl证书认证\ndocker exec -it elasticsearch bash -c ' sed -i \"s/  enabled: true/  enabled: false/g\" /usr/share/elasticsearch/config/elasticsearch.yml'\ndocker exec -it elasticsearch bash -c 'cat /usr/share/elasticsearch/config/elasticsearch.yml'\ndocker restart elasticsearch\ndocker restart skywalking-oap\n```\n\n**Skywalking 通过 HTTPS SSL 认证连接 Elasticsearch**\n\n(1)将es的crt和key证书文件，转化为p12格式\n\n```shell\nopenssl pkcs12 -export -in ca.crt -inkey ca.key -out es.p12 -name esca -CAfile es.crt\n```\n\n输入两次keypass，其中-name参数为别名。\n\n> openssl pkcs12 -export -in ca.crt -inkey ca.key -out es.p12 -name esca -CAfile es.crt\n> Enter Export Password:\n> Verifying - Enter Export Password: \n\n（2）将p12格式证书转化为jks证书\n\n安装JDK。`keytool` 是 JDK 中的一部分，需要安装JDK，进行证书转化操作。\n\n```shell\n yum install - java-11-openjdk-devel\n```\n\nstorepass 参数为jks证书密码，srcstorepass参数为p12证书密码。\n\n```shell\nkeytool -importkeystore -v -srckeystore es.p12 -srcstoretype PKCS12  -srcstorepass wasd2345  -deststoretype JKS -destkeystore es.jks -storepass qiswasd2345\n```\n\n> storage:\n> selector: ${SW_STORAGE:elasticsearch}\n> elasticsearch:\n>  namespace: ${SW_NAMESPACE:\"\"}\n>  clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:<mark>es所在服务器地址:443</mark>}\n>  protocol: 443${SW_STORAGE_ES_HTTP_PROTOCOL:\"<mark>https</mark>\"}\n>  connectTimeout: ${SW_STORAGE_ES_CONNECT_TIMEOUT:3000}\n>  socketTimeout: ${SW_STORAGE_ES_SOCKET_TIMEOUT:30000}\n>  responseTimeout: ${SW_STORAGE_ES_RESPONSE_TIMEOUT:15000}\n>  numHttpClientThread: ${SW_STORAGE_ES_NUM_HTTP_CLIENT_THREAD:0}\n>  user: ${SW_ES_USER:\"<mark>es用户名</mark>\"}\n>  password: ${SW_ES_PASSWORD:\"<mark>es密码</mark>\"}\n>  trustStorePath: ${SW_STORAGE_ES_SSL_JKS_PATH:\"<mark>jks证书地址</mark>\"}\n>  trustStorePass: <mark></mark>${SW_STORAGE_ES_SSL_JKS_PASS:\"<mark>jks证书密码</mark>\"}\n\n### 开启Linux监控\n\n#### 安装Prometheus node-exporter从 VM 收集指标数据。（源码方式）\n\n```shell\nyum install -y wget\nwget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz\ntar -xvzf node_exporter-1.8.2.linux-amd64.tar.gz\ncd \nmv node_exporter-1.8.2.linux-amd64/node_exporter /usr/sbin/\ncd /usr/sbin/\n./node_exporter\n```\n\n验证是否运行\n\n```shell\ncurl http://localhost:9100/metrics\n```\n\n创建node_exporter服务文件\n\n```shell\nvi /usr/lib/systemd/system/node_exporter.service\n\n[Unit]\nDescription=node exporter service\nDocumentation=https://prometheus.io\nAfter=network.target\n\n[Service]\nType=simple\nUser=root\nGroup=root\n#node_exporter的存放位置\nExecStart=/usr/sbin/node_exporter  \nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n```\n\n重新加载系统管理器配置文件，启动node_exporter服务并设置开机自启\n\n```shell\nsystemctl daemon-reload\nsystemctl start node_exporter\nsystemctl enable node_exporter\nsystemctl status node_exporter\n```\n\n#### 安装Prometheus node-exporter（容器方式）\n\n```yaml\nservices:\n  node-exporter:\n    image: quay.io/prometheus/node-exporter\n    container_name: node-exporter\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n    command:\n      - '--path.procfs=/host/proc'\n      - '--path.rootfs=/rootfs'\n      - '--path.sysfs=/host/sys'\n    restart: always\n    environment:\n      - TZ=Asia/Shanghai\n    ports:\n      - 9100:9100\n    networks:\n       linux_exporter:\n        ipv4_address: 172.20.104.11\n\nnetworks:\n  linux_exporter:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.104.0/24\n```\n\n#### 安装 OpenTelemetry Collector\n\n创建OpenTelemetry Collector配置文件\n\n```shell\nmkdir /data/opentelemetry-collector\nvi /data/opentelemetry-collector/config.yaml\n\nreceivers:\n  prometheus:\n    config:\n     scrape_configs:\n       - job_name: 'vm-monitoring' #要与skywalking-oap中的otel-rules的vm.yaml中的名称保持一致\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9100']\n             labels:\n               host_name: 10.10.2.145\n               service: oap-server\nprocessors:\n  batch:\n\nexporters:\n  otlp:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging:\n    loglevel: debug\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [batch]\n      exporters: [otlp, logging]\n```\n\n```yaml\nservices:\n  otelcol:\n    image: otel/opentelemetry-collector\n    container_name: otelcol\n    restart: always\n    environment:\n      - TZ=Asia/Shanghai\n    volumes:\n      - /data/opentelemetry-collector/config.yaml:/etc/otelcol/config.yaml\n    networks:\n       opentelemetry:\n        ipv4_address: 172.20.101.11\n\nnetworks:\n  opentelemetry:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.101.0/24\n```\n\n> opentelemetry-collector最新镜像配置文件变更\n>\n> 在最新版的容器镜像中（0.113.0）中使用导出器exporters中使用debug代替logging。\n>\n> 即在v0.86.0版本之前使用logging\n>\n> ```\n> exporters:\n>   otlp:\n>     endpoint: ip+端口\n>     tls:\n>       insecure: true\n>   logging:\n>     loglevel: debug\n> ```\n>\n> 之后使用debug\n\n> ```\n> exporters:\n>   otlp:\n>     endpoint: ip+端口\n>     tls:\n>       insecure: true\n>   debug:\n>     verbosity: detailed\n> ```\n\n### SkyWalking服务自监控开启\n\n开启后端遥测，在skywalking-oap的配置文件application.yml中，找到promethus的部分，修改参数。\n\n```shell\ntelemetry:\n  selector: ${SW_TELEMETRY:prometheus} #将none修改为prometheus\n  none:\n  prometheus:\n    host: ${SW_TELEMETRY_PROMETHEUS_HOST:0.0.0.0}\n    port: ${SW_TELEMETRY_PROMETHEUS_PORT:1234}\n    sslEnabled: ${SW_TELEMETRY_PROMETHEUS_SSL_ENABLED:false}\n    sslKeyPath: ${SW_TELEMETRY_PROMETHEUS_SSL_KEY_PATH:\"\"}\n    sslCertChainPath: ${SW_TELEMETRY_PROMETHEUS_SSL_CERT_CHAIN_PATH:\"\"}\n```\n\n 在OpenTelemetry Collector配置文件中加入自监控的参数\n\n```yaml\nreceivers:\n  prometheus:\n    config:\n     scrape_configs:\n       - job_name: 'vm-monitoring'\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9100']\n             labels:\n               host_name: 10.10.2.145\n               service: skywalking-oap-server\n  prometheus/2:  #新增\n    config:\n     scrape_configs:\n       - job_name: 'skywalking-so11y'  #要与skywalking-oap中的otel-rules的oap.yaml中的名称保持一致\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:1234']   #端口为1234\n             labels:\n               host_name: 10.10.2.145_self\n               service: skywalking-oap\n\n\nprocessors:\n  batch:\n  batch/2:\n\nexporters:\n  otlp:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging:\n    loglevel: debug\n  otlp/2:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging/2:\n    loglevel: debug\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [prometheus, prometheus/2]\n      processors: [batch, batch/2]\n      exporters: [otlp, otlp/2, logging, logging/2]\n```\n\n依次重启otelcol容器和skywalking-oap，查看是否生成自监控。\n\n### 开启数据库MySQL/MariaDB监控\n\n（1）部署mysqld_exporter\n\n```yaml\nservices:\n  mysqld_exporter:\n    image: prom/mysqld-exporter\n    container_name: mysqld_exporter\n    restart: always\n    environment:\n      - TZ=Asia/Shanghai\n    ports:\n      - \"9104:9104\"\n    command:\n      - \"--mysqld.username=user:password\"   #用户名和密码\n      - \"--mysqld.address=10.10.2.145:3306\"   #ip和端口号\n    networks:\n       sw-mysql:\n        ipv4_address: 172.20.102.11\n\nnetworks:\n  sw-mysql:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.102.0/24\n```\n\n（2） 在OpenTelemetry Collector配置文件中mysql的监控参数\n\n```yaml\nreceivers:\n  prometheus:\n    config:\n     scrape_configs:\n       - job_name: 'vm-monitoring'\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9100']\n             labels:\n               host_name: 10.10.2.145\n               service: skywalking-oap-server\n  prometheus/2:\n    config:\n     scrape_configs:\n       - job_name: 'skywalking-so11y'\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:1234']\n             labels:\n               host_name: 10.10.2.145_self\n               service: skywalking-oap\n\n  prometheus/3:  #mysql、mariadb的监控部分\n    config:\n     scrape_configs:\n       - job_name: 'mysql-monitoring' #要与skywalking-oap中的otel-rules/mysql目录中的yaml文件中的名称保持一致\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9104']\n             labels:\n               host_name: mariadb-monitoring\n\nprocessors:\n  batch:\n  batch/2:\n  batch/3:\n\nexporters:\n  otlp:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging:\n    loglevel: debug\n  otlp/2:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging/2:\n    loglevel: debug\n  otlp/3:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging/3:\n    loglevel: debug\n\nservice:\n  pipelines:\n    metrics:\n      receivers:\n      - prometheus\n      - prometheus/2\n      - prometheus/3\n      processors:\n      - batch\n      - batch/2\n      - batch/3\n      exporters:\n      - otlp\n      - otlp/2\n      - otlp/3\n      - logging\n      - logging/2\n      - logging/3\n```\n\n依次重启otelcol容器和skywalking-oap，查看是否生成mysql或mariadb的监控数据。\n\n### 开启Elasticsearch监控\n\n（1）部署elasticsearch_exporter\n\n```yaml\nservices:\n  elasticsearch_exporter:\n    image: quay.io/prometheuscommunity/elasticsearch-exporter:latest\n    container_name: elasticsearch_exporter\n    restart: always\n    environment:\n      - TZ=Asia/Shanghai\n    ports:\n      - \"9114:9114\"\n    command:\n      #- '--es.uri=http://elastic:elastic@10.10.2.145:9200'   #es使用http协议\n      - '--es.uri=https://elastic:elastic@10.10.2.145:9200' \n      - \"--es.ssl-skip-verify\"   #连接到es时跳过SSL验证\n    networks:\n       es_exporter:\n        ipv4_address: 172.20.103.11\n\nnetworks:\n  es_exporter:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.103.0/24\n```\n\n（2）在OpenTelemetry Collector配置文件中es的监控参数\n\n```yaml\nreceivers:\n  prometheus:\n    config:\n     scrape_configs:\n       - job_name: 'elasticsearch-monitoring'\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9114']\n             labels:\n               host_name: elasticsearch-monitoring\n\n\nprocessors:\n  batch:\n\nexporters:\n  otlp:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging:\n    loglevel: debug\n\nservice:\n  pipelines:\n    metrics:\n      receivers:\n      - prometheus\n      processors:\n      - batch\n      exporters:\n      - otlp\n      - logging\n```\n\n依次重启otelcol容器和skywalking-oap，查看是否生成es的监控数据。\n\n### 开启数据库PostgreSQL监控\n\n（1）部署postgres-exporter\n\n```yaml\nservices:\n  postgres-exporter:\n    image: quay.io/prometheuscommunity/postgres-exporter\n    container_name: postgres-exporter\n    restart: always\n    environment:\n      TZ: \"Asia/Shanghai\"\n      DATA_SOURCE_URI: \"localhost:5432/postgres?sslmode=disable\"\n      DATA_SOURCE_USER: \"postgres\"\n      DATA_SOURCE_PASS: \"password\"\n    ports:\n      - \"9187:9187\"\n    networks:\n       sw-pgsql:\n        ipv4_address: 172.20.105.11\n\nnetworks:\n  sw-pgsql:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.105.0/24\n```\n\n（2）在OpenTelemetry Collector配置文件中postgres sql的监控参数\n\n```yaml\nreceivers:\n  prometheus:\n    config:\n     scrape_configs:\n       - job_name: 'postgresql-monitoring'\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9187']\n             labels:\n               host_name: postgresql-monitoring\n\n\nprocessors:\n  batch:\n\nexporters:\n  otlp:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging:\n    loglevel: debug\n\nservice:\n  pipelines:\n    metrics:\n      receivers:\n      - prometheus\n      processors:\n      - batch\n      exporters:\n      - otlp\n      - logging\n```\n\n依次重启skywalking-oap容器和otelcol，查看是否生成postgres sql的监控数据。\n\n### 开启数据库MongoDB监控\n\n（1）部署mongodb_exporter\n\n```yaml\nservices:\n  mongodb_exporter:\n    image: percona/mongodb_exporter:0.40\n    container_name: mongodb_exporter\n    restart: always\n    ports:\n      - \"9216:9216\"\n    environment:\n      - TZ=Asia/Shanghai\n      - MONGODB_URI=mongodb://user:password@192.168.1.23:27017/?authSource=admin\n    command:\n      - --collect-all\n      - --web.listen-address=:9216\n    networks:\n       sw-mongodb:\n        ipv4_address: 172.20.106.11\n\nnetworks:\n  sw-mongodb:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.106.0/24\n```\n\n(2)在OpenTelemetry Collector配置文件中mongodb的监控参数\n\n```yaml\nreceivers:\n  prometheus:\n    config:\n     scrape_configs:\n       - job_name: 'mongodb-monitoring'\n         scrape_interval: 5s\n         static_configs:\n           - targets: ['10.10.2.145:9216']\n             labels:\n               host_name: mongodb-monitoring\n\n\nprocessors:\n  batch:\n\nexporters:\n  otlp:\n    endpoint: 10.10.2.145:11800\n    tls:\n      insecure: true\n  logging:\n    loglevel: debug\n\nservice:\n  pipelines:\n    metrics:\n      receivers:\n      - prometheus\n      processors:\n      - batch\n      exporters:\n      - otlp\n      - logging\n```\n\n### .NET项目服务链路追踪\n\n#### 安装 SkyWalking .NET Core Agent（Windows环境）\n\n（1）在Visual Studio中的项目中安装`nuget`包`SkyAPM.Agent.AspNetCore`。\n\n（2）在`launchSettings.json`文件中新增环境变量`\"ASPNETCORE_HOSTINGSTARTUPASSEMBLIES\": \"SkyAPM.Agent.AspNetCore\"`和 `\"SKYWALKING__SERVICENAME\": \"服务名（与执行的dll程序的名称一致）\"`。\n\n```json\n  \"profiles\": {\n    \"http\": {\n      \"commandName\": \"Project\",\n      \"dotnetRunMessages\": true,\n      \"launchBrowser\": true,\n      \"applicationUrl\": \"http://localhost:5205\",\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\",\n        //新增环境变量\n        \"ASPNETCORE_HOSTINGSTARTUPASSEMBLIES\": \"SkyAPM.Agent.AspNetCore\",\n        \"SKYWALKING__SERVICENAME\": \"服务名（与执行的dll程序的名称一致）\"\n      }\n    },\n    \"https\": {\n      \"commandName\": \"Project\",\n      \"dotnetRunMessages\": true,\n      \"launchBrowser\": true,\n      \"applicationUrl\": \"https://localhost:7105;http://localhost:5205\",\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\",\n        //新增环境变量\n        \"ASPNETCORE_HOSTINGSTARTUPASSEMBLIES\": \"SkyAPM.Agent.AspNetCore\",\n        \"SKYWALKING__SERVICENAME\": \"服务名（与执行的dll程序的名称一致）\"\n      }\n    },\n    \"IIS Express\": {\n      \"commandName\": \"IISExpress\",\n      \"launchBrowser\": true,\n      \"environmentVariables\": {\n        \"ASPNETCORE_ENVIRONMENT\": \"Development\",\n         //新增环境变量\n        \"ASPNETCORE_HOSTINGSTARTUPASSEMBLIES\": \"SkyAPM.Agent.AspNetCore\",\n        \"SKYWALKING__SERVICENAME\": \"服务名（与执行的dll程序的名称一致）\"\n      }\n    }\n  }\n,\n```\n\n（3）在Program.cs中添加配置参数。\n\n```cs\n//SkyApm\nbuilder.Services.AddSkyApmExtensions();\nEnvironment.SetEnvironmentVariable\n(\"ASPNETCORE_HOSTINGSTARTUPASSEMBLIES\", \"SkyAPM.Agent.AspNetCore\");\nEnvironment.SetEnvironmentVariable(\"SKYWALKING__SERVICENAME\", \"服务名（与执行的dll程序的名称一致）\");\n```\n\n（4）添加skyapm.json文件，添加方式有两种：\n\n一是在dll运行程序的同目录下创建skyapm.json，并写入以下内容。\n\n```json\n{\n  \"SkyWalking\": {\n    \"ServiceName\": \"服务名（与执行的dll程序的名称一致）\",\n    \"Namespace\": \"\",\n    \"HeaderVersions\": [\n      \"sw8\"\n    ],\n    \"Sampling\": {\n      \"SamplePer3Secs\": -1,\n      \"Percentage\": -1.0\n    },\n    \"Logging\": {\n      \"Level\": \"Information\",\n      \"FilePath\": \"logs\\\\skyapm-{Date}.log\"\n    },\n    \"Transport\": {\n      \"Interval\": 3000,\n      \"ProtocolVersion\": \"v8\",\n      \"QueueSize\": 30000,\n      \"BatchSize\": 3000,\n      \"gRPC\": {\n        \"Servers\": \"SkyWalking服务ip:11800\",\n        \"Timeout\": 10000,\n        \"ConnectTimeout\": 10000,\n        \"ReportTimeout\": 600000,\n        \"Authentication\": \"\"\n      }\n    }\n  }\n}\n\n```\n\n二是在Visual Studio的控制台中输入命令创建skyapm.json。\n\n```shell\ndotnet tool install -g SkyAPM.DotNet.CLI\ndotnet skyapm config 服务名（与执行的dll程序的名称一致） SkyWalking服务ip:11800\n```\n\n（5）配置完成后，在运行的服务器中加入环境变量\n\n方式一：在服务器添加\n\n```\nvi  ~/.bashrc\n\n\nexport ASPNETCORE_ENVIRONMENT=development\nexport ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=SkyAPM.Agent.AspNetCore\nexport SKYWALKING__SERVICENAME=服务名\n\n\n#使得配置生效\nsource ~/.bashrc\n```\n\n方式二：在容器中添加，可在Dockerfile文件中添加\n\n```\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base\n......\nENV ASPNETCORE_ENVIRONMENT=development\nENV ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=SkyAPM.Agent.AspNetCore\nENV SKYWALKING__SERVICENAME=服务名\n#容器入口点\nENTRYPOINT [\"dotnet\", \"xxx\"]\n```\n\n（6）访问.NET项目，查看Skywalking中常规服务-服务中生成的服务监控。（效果图在最后）\n\n#### 安装 SkyWalking .NET Core Agent（Linux环境）\n\n适用于在Linux上创建的.NET项目\n\n（1）安装 SkyWalking .NET Core Agent\n\n```shell\ndotnet add package SkyAPM.Agent.AspNetCore\nexport ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=SkyAPM.Agent.AspNetCore\nexport SKYWALKING__SERVICENAME=服务名（与执行的dll程序的名称一致）\n```\n\n（2）安装SkyAPM.DotNet.CLI，用于生成skyapm.json\n\n```shell\ndotnet tool install -g SkyAPM.DotNet.CLI\ndotnet skyapm config 服务名（与执行的dll程序的名称一致） SkyWalking服务ip:11800\n```\n\n（3）配置完成后，在运行的服务器中加入环境变量\n\n方式一：在服务器添加\n\n```\nvi  ~/.bashrc\n\n\nexport ASPNETCORE_ENVIRONMENT=development\nexport ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=SkyAPM.Agent.AspNetCore\nexport SKYWALKING__SERVICENAME=服务名\n\n\n#使得配置生效\nsource ~/.bashrc\n```\n\n方式二：在容器中添加，可在Dockerfile文件中添加\n\n```\nFROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base\n......\nENV ASPNETCORE_ENVIRONMENT=development\nENV ASPNETCORE_HOSTINGSTARTUPASSEMBLIES=SkyAPM.Agent.AspNetCore\nENV SKYWALKING__SERVICENAME=服务名\n#容器入口点\nENTRYPOINT [\"dotnet\", \"xxx\"]\n```\n\n（4）访问.NET项目，查看Skywalking中常规服务-服务中生成的服务监控。\n\n![img](docker部署SkyWalking/1.png)\n\n### 接入前端监控\n\n（1）项目中添加skywalking-client-js包\n\n```JSON\nnpm install skywalking-client-js --save\n```\n\n（2）在vue.config.js中配置代理\n\n```JSON\nproxy:{\n      '/browser': {\n        target:'SkyWalking服务ip:12800',//这里是路由和报错报告的代理\n        changeOrigin: true\n      },\n      '/v3':{\n        target:'SkyWalking服务ip:12800',\n        changeOrigin: true//这里是追踪报告的代理\n      }\n}\n```\n\n（3）在main.js中接入skywalking-client-js\n\n```JSON\n//skywalking监控系统\nimport ClientMonitor from 'skywalking-client-js';\n//注册skywalking\nClientMonitor.register({\n    service: '服务名',//服务名称        \n    serviceVersion:'',//应用版本号\n    traceSDKInternal:true,//追踪sdk\n    pagePath: location.href,//当前路由地址\n    useFmp: true\n});\n```\n\n(4)在对应业务系统的服务器中，在对应发布nignx代理服务中加入对应的代理配置。\n\n```JSON\n    location /browser/ {\n        proxy_pass http://SkyWalking服务ip:12800/browser/;\n        client_max_body_size 1000M;\n    }\n        location /v3/ {\n        proxy_pass http://SkyWalking服务ip:12800/v3/;\n        client_max_body_size 1000M;\n    }\n```\n","tags":["Linux","docker","运维","SkyWalking","APM工具","DevOps","分布式链路追踪"],"categories":["容器化","docker"]},{"title":"在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）","url":"/yyg/5ea27f40/","content":"\n# 在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）\n\n## 前言\n\n在搭建`云野阁`博客网站期间，看了好多具有个人特色的博客网站，有不少都有RSS订阅功能，那RSS是什么呢？咱们先唠唠。\n\n{% folding cyan,🔜RSS🔚 %}\n\nRSS 全称 Really Simple Syndication（真正简易联合），是一种基于XML（可扩展标记语言）的内容分发协议，它允许用户订阅网站的内容更新，如新闻、博客文章等。好像还有另外的说法是Rich Site Summary（网站内容摘要）和 RDF Site Summary（资源描述框架站点摘要），不过其实都描述的是从订阅源获取更新的内容，并将获取的内容整合集中显示，方便用户进行查看。\n\n网上有关RSS的相关介绍有很多，感兴趣的话可以搜索了解下。\n\n{% endfolding %}\n\n订阅了RSS，只要再结合RSS阅读器，就可以直接看到有关博客的最新文章，那我们就重点搞RSS阅读器。RSS阅读器无论是自建还是使用现成的，都有很多类型，像浏览器插件、手机端、电脑端程序、web端等。搭建一个web端，方便、省事、对终端依赖小。\n\n[OsmosFeed](https://github.com/osmoscraft/osmosfeed)是GitHub上一个开源的Web版RSS阅读器，可以使用 GitHub Pages托管，利用GitHub Actions实现内容定期自动更新，主题可自定义。\n\n本站的`文界`就是使用OsmosFeed搭建并托管在GitHub Pages上的。{% btn '/yygrss/index.html',点我跳转查看,far fa-hand-point-right,outline  blue %}\n\n## 搭建过程\n\n### 创建仓库\n\n1.访问OsmosFeed仓库的[配置教程](https://github.com/osmoscraft/osmosfeed/blob/master/README_zh.md)，点击教程中”创建新仓库“中的第一步”使用osmosfeed-template官方模板创建仓库“，页面跳转至创建新仓库界面，设置仓库名并将可见性设为Public，点击创建Create repository按钮新建仓库。\n\n![image-20240826224400783](在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）/1.png)\n\n2.进入刚创建好的仓库，进入目录`.github/workflows`,修改`update-feed.yaml`文件为以下内容。\n\n```yaml\nname: Build site on schedule or main branch update\n\non:\n  push:\n    branches:\n      - main\n  schedule:\n    # Adjust refresh schedule here. By default, it runs once per day.\n    # Syntax reference: https://docs.github.com/en/actions/reference/events-that-trigger-workflows#schedule\n    # Recommended tool: https://crontab.guru/\n    - cron: \"0 11 * * *\"    #设置的执行时间周期\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Node.js environment\n        uses: actions/setup-node@v4.0.3\n        with:\n          node-version: \"20\"\n      - name: Install dependencies\n        run: npm i\n      - name: Build the feed\n        run: npm run build\n      - name: Deploy to GitHub Pages\n        uses: peaceiris/actions-gh-pages@v4\n        with:\n          github_token: ${{ secrets.action_token }}   #action_token为设置的Actions secrets ，后面会讲到\n          publish_dir: ./public\n```\n\n3.进入刚创建好的仓库，修改根目录下的osmosfeed.yaml文件，将`cacheUrl:`行前的`#`去除，并将`GITHUB_USERNAME`修改为自己的github名称，`REPO_NAME`修改为本仓库的名。`sources`下的`- href:`为RSS/Atom源。\n\n```yaml\ncacheUrl: https://GITHUB_USERNAME.github.io/REPO_NAME/cache.json\nsources:\n  - href: https://css-tricks.com/feed/\n  - href: https://www.freecodecamp.org/news/rss/\n  - href: https://daverupert.com/atom.xml\n```\n\n### 设置身份验证令牌\n\n1.点击自己的头像，选择 “Settings” -> “Developer settings” -> “Personal access tokens”->”Tokens（classic）“，点击 “Generate new token”，选择”Generate new token（classic）“，验证后，指定一个描述性名称，选择令牌的有效时间，选择要授予此令牌的范围或权限。只需要选择repo一项即可。点击”Generate token“，完成创建。\n\n![image-20240826230331732](在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）/2.png)\n\n2.创建完成后，复制保存token，后面要用。\n\n3.进入刚创建好的仓库，进入仓库的“Settings” -> “Secrets and variables” -> “Actions”。点击”Repository secrets“中的“New repository secret”，输入Actions secrets 的名字为action_token（与update-feed.yaml文件中的保持一致），并将第2步复制的token粘贴至Secrets框中，点击”Add secret“保存。\n\n![image-20240826231333139](在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）/3.png)\n\n### 部署GitHub Pages\n\n1.进入仓库的“Settings” -> “Pages” ，在Branch中选择”gh-pages“，目录选择”/(root)“，点击”save“保存。\n\n![image-20240826232546187](在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）/4.png)\n\n![image-20240826232701080](在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）/5.png)\n\n2. 刷新页面，直到界面上出现 `Your site is published at https://github用户名.github.io/仓库名`的确认信息（最多等待1－3分钟）即可离开。完成部署。部署的详细过程可以进入仓库的”Actions\"，进行查看具体过程与异常情况。\n\n![image-20240826235331777](在Github上使用OsmosFeed搭建在线RSS阅读器（无需服务器）/6.png)\n\n{% folding green open,博客搭建系列文章 %}\n\n{% series 博客搭建教程 %}\n\n{% endfolding %}","tags":["博客","魔改","在线rss阅读器","GitHub Pages"],"categories":["博客教程","rss阅读器"]},{"title":"一些免费的gpt网站","url":"/yyg/30035ff1/","content":"\n{% card CodeNews,https://codenews.cc/chatgpt,,,免费免登录,fas fa-paperclip,,96px,80px %}\n\n{% card 思妍AI,https://chat.ttext.cn/,,,免费免登录,fas fa-paperclip,,96px,80px %}\n\n{% card FreeGPT,https://free.netfly.top/#/chat,,,免费免登录,fas fa-paperclip,,96px,80px %}\n\n{% card 智能AI助手,https://chat.tinycms.xyz:3002/#/chat,,,免费免登录,fas fa-paperclip,,96px,80px %}\n\n{% card RawChat,https://sharedchat.cn/,,,免费免登录,fas fa-paperclip,,96px,80px %}\n\n{% card FreeChat-镜像,https://free-chat.cn/?sockstack&section=table,,,ChatGPT镜像网站列表,fas fa-paperclip,,96px,80px %}\n\n{% card SockStack,https://www.sockstack.cn/chat-mirrors,,,ChatGPT镜像网站列表,fas fa-paperclip,,96px,80px %}\n","tags":["人工智能助手","免费gpt"],"categories":["漫谈","免费gpt网站"]},{"title":"Watchtower自动更新docker容器","url":"/yyg/d3a49a0e/","content":"## 基础环境\n\n系统：openEuler 22.03 (LTS-SP4) X86\n\n软件：docker-26.1.3、 docker compose-2.27.0\n\n## 关于Watchtower\n\nWatchtower 是一个应用程序，监控正在运行的 Docker 容器，并监视这些容器的镜像版本的变化。如果 Watchtower 检测到容器的镜像已更改，它将自动拉取新镜像，关闭现有容器，使用最初部署时的相同选项重新启动它，实现容器的优雅升级。\n\n## 安装docker\n\n(1)配置yum源下载docker。\n\n```shell\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\nsed -i 's/\\$releasever/7/g' /etc/yum.repos.d/docker-ce.repo\n```\n\n（2）安装最新版docker和docker compose。\n\n```shell\n# 下载依赖及docker、docker compose\nyum install -y container-selinux\nyum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin \n```\n\n（3）修改Docker的数据目录为“/data/dockerData”，并配置docker镜像源。\n\n```shell\necho '{\n  \"data-root\": \"/data/dockerData\",\n \"registry-mirrors\": [\"https://dhub.kubesre.xyzcu\"]\n}' > /etc/docker/daemon.json\n```\n\n（4）启动Docker服务，并设置为开机自启动。\n\n```shell\nsystemctl start docker\nsystemctl enable docker\n```\n\n## 安装Watchtower\n\n### 方式一：通过docker命令安装\n\n```shell\ndocker run -itd --name watchtower \\\n--restart=always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-e TZ=Asia/Shanghai \\\ncontainrrr/watchtower \\\n--interval 60 \\\n--cleanup\n```\n\n> `--restart=always`  设置容器开机自启\n> \n> ` -v /var/run/docker.sock:/var/run/docker.sock`  读取docker守护进程的API\n> \n> `-e TZ=Asia/Shanghai` 使用本地时区\n> \n> `--interval 60` 设置轮询间隔为60秒\n> \n> `--cleanup` 更新后删除旧镜像\n\n### 方式二：使用compose文件安装\n\n```shell\nservices:\n watchtower:\n    image: containrrr/watchtower\n    container_name: watchtower\n    restart: always\n    environment:\n       - TZ=Asia/Shanghai\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    command: --interval 60 --cleanup\n    user: \"0\"\n    networks:\n      watchtower-net:\n        ipv4_address: 172.20.17.11\n\nnetworks:\n watchtower-net:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.17.0/24\n```\n\n## Watchtower更多应用\n\n### 1.watchtower 监控更新远程 Docker主机\n\n（1）在远程主机上启用远程 API 访问。\n\n```shell\n #修改docker服务文件\n vi /usr/lib/systemd/system/docker.service\n #在该行后加入 -H tcp://0.0.0.0:2375\n ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375\n #重启服务和docker\n systemctl daemon-reload\n systemctl restart docker\n \n #添加防火墙策略\nsudo firewall-cmd --add-port=2375/tcp --permanent\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all\n```\n\n（2）在本机创建watchtower，监控远程主机的docker容器\n\n```shell\ndocker run -d --name watchtower-1 \\\n-e TZ=Asia/Shanghai \\\ncontainrrr/watchtower \\\n--interval 60 \\\n--cleanup \\\n--host \"tcp://10.0.1.2:2375\"\n```\n\n### 2.按标签筛选\n\n按标签筛选进行容器的更新，需要在创建容器时，给容器打上包含`com.centurylinklabs.watchtower.enable`的标签，`com.centurylinklabs.watchtower.enable=false`表示禁用对容器的监控和更新，`com.centurylinklabs.watchtower.enable=true`表示启用对容器的监控和更新，<mark>此类情况只适用于创建新容器时。</mark>\n\n```shell\n#禁用对容器的监控和更新\ndocker run -itd \\\n--label=com.centurylinklabs.watchtower.enable=false --name test-httpd httpd\n#启用对容器的监控和更新\ndocker run -d --label=com.centurylinklabs.watchtower.enable=true --name test-httpd httpd\n```\n\n### 3.禁用容器名称筛选\n\n要禁用特定容器，需要在watchtower创建时加入`--disable-containers`参数，并在该参数后加上容器名称，如`httpd`，运行时就会不监控名称中含有httpd的容器。\n\n```shell\ndocker run -d \\\n--name watchtower \\\n--restart=always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-e TZ=Asia/Shanghai \\\ncontainrrr/watchtower \\\n--interval 60 \\\n--cleanup \\\n--disable-containers httpd\n```\n\n### 4. 常用命令参数\n\n部分常用参数设置如下表所示。\n\n| 参数                     | 示例                                                                                                                                                                           | 说明                    |\n| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------- |\n| --schedule \"0 9 * * *\" | `docker run -itd --name watchtower --restart always -v /var/run/docker.sock:/var/run/docker.sock -e TZ=Asia/Shanghai containrrr/watchtower --cleanup --schedule \"0 9 * * *\"` | 每天九点执行更新调度            |\n| --run-once             | `docker run -rm --name watchtower --restart always -v /var/run/docker.sock:/var/run/docker.sock -e TZ=Asia/Shanghai containrrr/watchtower --cleanup --run-once`              | 手动更新一次，并将watchtower删除 |\n| 容器名                    | `docker run -itd --name watchtower --restart always -v /var/run/docker.sock:/var/run/docker.sock -e TZ=Asia/Shanghai containrrr/watchtower --cleanup nginx httpd`            | 指定更新容器nginx、httpd     |\n| --monitor-only         | `docker run -itd --name watchtower --restart always -v /var/run/docker.sock:/var/run/docker.sock -e TZ=Asia/Shanghai containrrr/watchtower --cleanup --monitor-only`         | 只检查容器是否需要更新，不应用更新     |\n\n更多参数设置，可查看watchtower官方文档。\n\n{% link watchtower官方文档,https://containrrr.dev/watchtower/arguments/ %}\n","tags":["Linux","docker","运维","Watchtower","自动更新容器"],"categories":["容器化","docker"]},{"title":"Hexo-Butterfly美化教程-[9]繁沙","url":"/yyg/ca46b09/","content":"\n## 前言\n\n平时在工作和生活中，有一些很好用的软件、网站，就想着找一个地方都给记下来。在创建hexo博客时，看到了有在博客里添加网站导航的，感觉这样挺不错的，也就有了`繁沙`：一个搜录（个人）好用软件和网站的地方。样式借用了[Leonus ](https://blog.leonus.cn/)博主的[作品推荐卡片—标签外挂 | Leonus](https://blog.leonus.cn/2022/butterflyTag.html)，做了一点修改适配：\n\n- `卡片盒子可一行放多个`\n\n- `盒内的提示文本内容转成了一个外部独立显示的提示框文本`\n  （尚待优化，若有好的优化方式，请Q我一下），具体效果可以点击下面的按钮预览。\n\n{% hideBlock 点击预览效果,#386768,color %}\n\n![image-20240810223933689](Hexo-Butterfly美化教程[9]-繁沙/1.png)\n\n{% endhideBlock %}\n\n## 教程\n\n（1）在`博客根目录\\themes\\butterfly\\scripts\\tag`中创建`card.js` 并粘入以下代码：\n\n```js\n/**\n * card\n * {% card name,url,bg,star,text,icon,tag,w,h %}\n * {% card 标题,链接,背景,评分,评价,图标,标签,宽度,高度 %}\n */\n\n'use strict'\n\n// 分数转成星星\nfunction tostar(num) {\n    let tmp = ''\n    for (let i = 0; i < Math.floor(num); i++) { tmp += '<i class=\"fa-solid fa-star\"></i>' } // 整数部分加实心星星\n    if (num - Math.floor(num) != 0) tmp += '<i class=\"fa-solid fa-star-half-alt\"></i>' // 小数部分转成半星\n    for (let i = 0; i < 5 - Math.ceil(num); i++) { tmp += '<i class=\"fa-regular fa-star\"></i>' } // 不够5个补空心星星\n    return tmp\n}\n\nfunction card(args) {\n    args = args.join(' ').split(',')\n\n    // 获取参数\n    let name = (args[0] || '未知').trim()\n    let url = (args[1] || '').trim()\n    let bg = (args[2] ? `background-image: url(${args[2]});` : 'background-color: #333;').trim()\n    let star = tostar(Number(args[3]) || 0)\n    let text = (args[4] || '暂无说明').trim()   \n    let icon = (args[5] || '').trim()\n    let tag = (args[6] || '').trim()\n    let w = args[7] || '200px'\n    let h = args[8] || '275px'\n\n    return `<div title=\"${name}\" referrerPolicy=\"no-referrer\" class=\"card_box\" style=\"${bg} width:${w}; height:${h};\">\n    <div class=\"card_mask\">\n      <span class=\"tooltiptext\">${text}</span>  //此处新添加了tooltiptext类名，添加了新css样式\n      ${url?'<a href=\"'+url+'\">查看详情</a>':''}\n    </div>\n    <div class=\"card_top\">\n      <i class=\"${icon}\"></i>\n      <span>${tag}</span>\n    </div>\n    <div class=\"card_content\">\n      <span>${name}</span>\n      <div>${star}</div>\n    </div>\n  </div>`\n}\n\nhexo.extend.tag.register('card', card, { ends: false })\n```\n\n（2）在 `博客根目录\\themes\\butterfly\\source\\css\\_tags` 中创建 `card.styl` 并粘入以下代码：\n\n```js\n.card_box\n  display: inline-flex       //此处将flex改为inline-flex，使得card盒子可以一行放多个\n  justify-content: space-between\n  flex-direction: column\n  background-position: center\n  background-size: cover\n  border-radius: 12px\n  position: relative\n  overflow: hidden\n  padding: 10px\n  color: #fff !important\n  margin: 10px auto\n  &::after\n    content: ''\n    position: absolute\n    height: 100%\n    width: 100%\n    left: 0\n    top: 0\n    background: rgba(0,0,0,0.1)\n    transition: .5s\n    z-index: 0\n  &:hover\n    .card_mask\n      opacity: 1\n      pointer-events: auto\n  .card_top\n    display: flex\n    z-index: 1\n    align-items: center\n    justify-content: space-between\n  .card_mask\n    position: absolute\n    pointer-events: none\n    z-index: 2\n    transition: .5s\n    opacity: 0\n    width: 100%\n    height: 100%\n    left: 0\n    top: 0\n    padding: 20px\n    background: #333\n    span\n      display: block\n      height: calc(100% - 40px)\n      overflow: auto\n    a\n      text-align: center\n      background: #fff\n      color: #333 !important\n      border-radius: 5px\n      position: absolute\n      width: calc(100% - 40px)\n      bottom: 20px\n      left: 20px\n      &:hover\n        text-decoration: none !important\n        color: white !important\n        background: #49b1f5\n\n  .card_content\n    z-index: 1\n    span\n      font-size: 18px\n      font-weight: bold\n\n/* ##################这部分为新加样式代码################## */\n\n/* card_mask 文本 */\n.card_mask .tooltiptext {\n    visibility: hidden;\n    background-color: #f5f9fc;\n    color: #000;\n    text-align: center;\n    padding: 5px 0;\n    border-radius: 6px;\n\n    /* 定位 */\n    position: fixed;\n    z-index: 1;\n}\n\n/* 鼠标移动上去后显示提示框 */\n.card_mask:hover .tooltiptext {\n     visibility: visible;\n     max-width: 40%;\n     word-wrap: break-word;\n     text-align: center;\n     max-height: 10%;\n     top: 10%;\n     padding: 5px;\n     margin-right:20%;    \n}\n/* ##################这部分为新加样式代码################## */\n\n[data-theme='dark']\n  .card_box\n    color: #ddd !important\n    &::after\n      background: rgba(0,0,0,0.1)\n```\n\n## 应用\n\n应用参数格式：\n\n```\n{% card 标题,链接,背景,评分,评价,图标,标签,宽度,高度 %}\n```\n\n具体应用的参数应用访问`Leonus | 作品推荐卡片—标签外挂`的原链接查看。链接如下：\n\n{% link Leonus | 作品推荐卡片—标签外挂,https://blog.leonus.cn/2022/butterflyTag.html,https://q1.qlogo.cn/g?b=qq&nk=990320751&s=5 %}\n\n{% folding green open,博客搭建系列文章 %}\n\n{% series 博客搭建教程 %}\n\n{% endfolding %}\n","tags":["Hexo","Butterfly","博客教程","美化教程"],"categories":["博客教程","美化教程"]},{"title":"docker容器日志配置管理","url":"/yyg/3b57c0c6/","content":"运行的项目容器突然就停了，查看容器日志发现磁盘空间不足，导致容器没法运行了。\n\n> Error response from daemon: Cannot restart container docker-p: mkdir /home/dockerData/overlay2/7bef1e2fa4788ab4d5db7e2e850d7ceeed09185b55c175f20c1500d28a0cc874d: <mark>no space left on device</mark>\n\n先使用`df -h`看下磁盘使用情况。\n\n> overlay                     413G  413G  0   100% /home/dockerData/overlay2/fea24dfbbe9f7d6d8309....../merged\n> overlay                     413G  413G  0   100% /home/dockerData/overlay2/7bef1e2fa4788ab4d5db....../merged\n\n容器的存储目录`/home/dockerData/`（其默认存储目录是`/var/lib/docker`，我这儿是重新配置的）直接把磁盘用完了。\n\n找到原因就好办了，直接到容器的存储目录`/home/dockerData/`先后使用`du -sh`命令分别查看`overlay2`和`containers`两个目录以及里面的文件。发现containers目录里的容器文件`98bbe7f17fa335f1b6e17......`占用了409G。 \n\n> du -sh overlay2\n> 3.7G    overlay2\n> \n>  du -sh containers/98bbe7f17fa335f1b6e17......\n> 409G    containers/98bbe7f17fa335f1b6e17......\n\n再深入容器文件发现，源头是容器的日志文件`98bbe7f17fa335f1b6e17......-json.log`。\n\n把这个日志文件进行删除之后，再重启容器，业务就恢复正常了。\n\n为防止这种情况再次发生，需要在 Docker 守护程序`/etc/docker/daemon.json`配置日志记录驱动程序，配置具体内容如下：\n\n```json\n  \"log-driver\":\"json-file\",\n  \"log-opts\":{\"max-size\" :\"50m\",\"max-file\":\"2\"}\n```\n\n> `\"log-driver\":\"json-file\"`   Docker的默认日志记录驱动程序。日志格式为JSON。\n> \n> `\"max-size\" :\"50m\"`  日志切割前的最大为50M。\n> \n> `\"max-file\":\"2\"`  保留的最大日志文件数量为2。\n\n配置完成后，重启docker。\n\n```shell\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n```\n\n关于docker的日志更多相关内容，可参考下面链接中的内容。\n\n{% link Docker 日志管理最佳实践！看这篇就够了！,https://segmentfault.com/a/1190000023144155 %}\n","tags":["Linux","docker","运维","知识梳理"],"categories":["容器化","梳理总结","docker","知识梳理"]},{"title":"podman-5.1.2 部署","url":"/yyg/c10999ab/","content":"\n## 基础环境\n\n系统：openEuler 22.03 (LTS-SP4) X86\n\n软件：podman-5.1.2、podman-compose-1.2.0\n\n## 配置yum源\n\n```shell\necho '[baseos]\nname=CentOS-Stream-9-Base-mirrors.aliyun.com\nbaseurl=https://mirrors.aliyun.com/centos-stream/9-stream/BaseOS/x86_64/os/\ngpgcheck=0\n\n[appstream]\nname=CentOS-Stream-9-Appstream-mirrors.aliyun.com\nbaseurl=https://mirrors.aliyun.com/centos-stream/9-stream/AppStream/x86_64/os/\ngpgcheck=0' > /etc/yum.repos.d/CentOS.repo\n```\n\n## 安装podman和podman-compose\n\n```shell\n# 安装podman podman-docker\nyum install -y podman podman-docker\n# 下载podman-compose\npip install podman-compose\n# 创建文件，使用docker命令时不报错\ntouch /etc/containers/nodocker\n```\n\n<mark>安装`podman-docker`后可使用docker命令</mark>\n\n## 启动并查看版本\n\n```shell\n# 启动podman\nsystemctl start podman\n# 设置开机自启\nsystemctl enable podman\n# 查看版本\npodman --version\n```\n\n## 永久启用cgroups-v2\n\n{% folding cyan,📚海拾🐚 cgroups-v2 %}\n\ncgroup v2 for containers 需要内核版本 4.15 或更高，而建议在 5.2 或更高再使用 cgroup v2。\n\n{% endfolding %}\n\npodman安装完成后，直接使用podman命令时，会出现以下提示，大致意思是说cgroups-v1已被弃用，要设置环境变量`PODMAN_IGNORE_CGROUPSV1_WARNING`，以启用 cgroups-v2。\n\n> WARN[0000] Using cgroups-v1 which is deprecated in favor of cgroups-v2 with Podman v5 and will be removed in a future version. Set environment variable `PODMAN_IGNORE_CGROUPSV1_WARNING` to hide this warning.\n\n（1）确定系统是否支持 cgroups-v2\n\n```shell\ngrep cgroup /proc/filesystems\n```\n\n如果系统支持会显示以下内容：\n\n> nodev   cgroup\n> nodev   cgroup2\n\n（2）输入`ls /sys/fs/cgroup/cgroup.controllers`检查是否已经激活cgroups-v2，如果显示<mark>ls: cannot access '/sys/fs/cgroup/cgroup.controllers': No such file or directory</mark>，则表示还未激活。\n\n（3）永久激活cgroups-v2\n\n打开GRUB 配置文件`/etc/default/grub`，修改内核命令行参数。在`GRUB_CMDLINE_LINUX`所在行加入以下参数：\n\n```shell\nsystemd.unified_cgroup_hierarchy=1\n```\n\n> GRUB_CMDLINE_LINUX=\"resume=/dev/mapper/openeuler-swap rd.lvm.lv=openeuler/root rd.lvm.lv=openeuler/swap cgroup_disable=files apparmor=0 crashkernel=512M <mark>systemd.unified_cgroup_hierarchy=1</mark>\"\n\n（4）更新GRUB配置，重启系统使更改的参数生效。\n\n```shell\n# 更新GRUB配置\nsudo grub2-mkconfig -o /boot/grub2/grub.cfg\n# 重启\nreboot\n```\n\n## 验证cgroups-v2\n\n重启系统后，输入命令查看podman的cgroupVersion的版本，显示版本为v2，则启用成功。并且在输入podman命令时，不会在有提示。\n\n```shell\npodman info | grep cgroupVersion\n```\n\n>   cgroupVersion: v2\n\n## 配置镜像源\n\n打开配置文件`/etc/containers/registries.conf`，配置国内镜像源，加速拉取镜像。\n\n```shell\necho 'unqualified-search-registries = [\"docker.io\"]\n[[registry]]\nprefix = \"docker.io\"\nlocation = \"dockerhub.icu\"\ninsecure = true\n[[registry.mirror]]\nlocation = \"ghcr.geekery.cn\"\ninsecure = true\n[[registry.mirror]]\nlocation = \"hub.rat.dev\"\ninsecure = true\n[[registry.mirror]]\nlocation = \"docker.wanpeng.top\"\ninsecure = true\n[[registry.mirror]]\nlocation = \"f1361db2.m.daocloud.io\"\ninsecure = true' >> /etc/containers/registries.conf\n```\n\n## 拉取镜像测试\n\n```\npodman run hello-world\n```\n\n成功拉取并运行后会显示以下效果：\n\n>     Resolved \"hello-world\" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf)\n>     Trying to pull quay.io/podman/hello:latest...\n>     Getting image source signatures\n>     Copying blob 81df7ff16254 done   |\n>     Copying config 5dd467fce5 done   |\n>     Writing manifest to image destination\n>     !... Hello Podman World ...!\n>     \n>              .--\"--.\n>            / -     - \\\n>           / (O)   (O) \\\n>        ~~~| -=(,Y,)=- |\n>         .---. /`  \\   |~~\n>      ~/  o  o \\~~~~.----. ~~\n>       | =(X)= |~  / (O (O) \\\n>        ~~~~~~~  ~| =(Y_)=-  |\n>       ~~~~    ~~~|   U      |~~\n>     \n>     Project:   https://github.com/containers/podman\n>     Website:   https://podman.io\n>     Desktop:   https://podman-desktop.io\n>     Documents: https://docs.podman.io\n>     YouTube:   https://youtube.com/@Podman\n>     X/Twitter: @Podman_io\n>     Mastodon:  @Podman_io@fosstodon.org\n","tags":["Linux","运维","podman"],"categories":["容器化","podman"]},{"title":"x86服务器构建ARM架构docker镜像","url":"/yyg/4451cec/","content":"\n## 基础环境\n\n操作系统：openEuler 22.03 (LTS-SP2)\n\n软件：Docker-26.1.2 、Docker Compose-v2.27.0\n\n## 安装Docker\n\n（1）配置yum源下载docker。\n\n```shell\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\nsed -i 's/\\$releasever/7/g' /etc/yum.repos.d/docker-ce.repo\n```\n\n（2）安装最新版docker和docker compose。\n\n```shell\n# 下载依赖及docker、docker compose\nyum install -container-selinux\nyum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin \n```\n\n（3）修改Docker的数据目录为“/data/dockerData”，并配置docker镜像源。\n\n```shell\necho '{\n  \"data-root\": \"/data/dockerData\",\n \"registry-mirrors\": [\"https://dockerhub.icu\"]\n}' > /etc/docker/daemon.json\n```\n\n（4）启动Docker服务，并设置为开机自启动。\n\n```shell\nsystemctl start docker\nsystemctl enable docker\n```\n\n## 使用 `register` 注册可支持的架构解析器\n\n```shell\ndocker run --rm --privileged multiarch/qemu-user-static:register --reset\n```\n\n查看二进制格式的解释器是否启用，显示`enabled`，即为启用。\n\n```shell\nls  /proc/sys/fs/binfmt_misc/ \ncat /proc/sys/fs/binfmt_misc/qemu-aarch64\n```\n\n## 拉取基础arm镜像\n\n此处以httpd镜像为例，在x86服务器上拉取arm64架构的httpd镜像，其中`--platform=arm64`指定拉取的镜像架构位arm64，`--platform=amd64`则是拉取x86架构的镜像。\n\n```shell\ndocker pull --platform=arm64 httpd:latest\n```\n\n## 定义Dockerfile，创建新镜像\n\n通过使用Dockerfile创建新arm镜像的方式有两种，一种是直接下载`qemu-aarch64-static`程序，在构建镜像时直接拷贝进镜像中；另一种是拉取`multiarch/qemu-user-static:x86_64-aarch64`镜像，通过多阶段构建，从`multiarch/qemu-user-static:x86_64-aarch64`镜像中复制 `qemu-aarch64-static`程序 到 `httpd` 镜像中。\n\n### 方式一：下载程序，直接复制\n\n（1）在下载`qemu-aarch64-static`程序时，可直接下载该程序，也可先下载其压缩包，再解压。\n\n```shell\n# 直接下载程序（推荐）\nwget https://github.com/multiarch/qemu-user-static/releases/download/v7.2.0-1/qemu-aarch64-static\n# 先下载其压缩包，再解压\nwget https://github.com/multiarch/qemu-user-static/releases/download/v7.2.0-1/qemu-aarch64-static.tar.gz\ntar -vzxf qemu-aarch64-static.tar.gz\n# 赋予可执行权限\nchmod +x /usr/bin/qemu-aarch64-static\n```\n\n（2）创建Dockerfile文件，在httpd镜像的基础上构建新镜像，`qemu-aarch64-static`<mark>程序必须和Dockerfile文件在同一目录下</mark>。Dockerfile文件的具体内容如下：\n\n```dockerfile\nFROM httpd:latest\n# 将qemu-aarch64-static程序拷贝进镜像的/usr/bin/目录\nCOPY ./qemu-aarch64-static /usr/bin/qemu-aarch64-static\n# 将测试的html文件拷贝到镜像的相关目录\nCOPY ./index.html  /usr/local/apache2/htdocs/\n```\n\n（3）指定构建的架构，执行命令开始构建新镜像，构建完成后，查看新镜像的架构并导出为tar文件，在arm服务器上进行验证。\n\n```shell\n# 构建镜像\ndocker build --platform arm64 -t httpd:1.00 .\n# 查看新镜像的架构\n docker inspect httpd:1.00 | grep Architecture\n# 导出镜像为tar文件\ndocker save -o httpd.tar httpd:1.00\n```\n\n### 方式二：通过镜像多重构建复制\n\n（1）创建Dockerfile文件，分两个阶段构建新镜像，第一阶段从 `multiarch/qemu-user-static` 镜像中获取`qemu-aarch64-static`程序，第二阶段在httpd镜像的基础上构建新镜像，Dockerfile文件的具体内容如下：\n\n```dockerfile\n# 第一阶段：从multiarch/qemu-user-static镜像中获取qemu-aarch64-static程序  \nFROM multiarch/qemu-user-static:x86_64-aarch64 as qemu\n# 第二阶段：在httpd镜像的基础上构建新镜像\nFROM httpd:latest\n# 将qemu-aarch64-static程序拷贝进镜像的/usr/bin/目录\nCOPY --from=qemu /usr/bin/qemu-aarch64-static /usr/bin/\n# 将测试的html文件拷贝到镜像的相关目录\nCOPY ./index.html  /usr/local/apache2/htdocs/\n```\n\n（2）指定构建的架构，执行命令开始构建新镜像，构建完成后，查看新镜像的架构并导出为tar文件，在arm服务器上进行验证。\n\n```shell\n# 构建镜像\ndocker build --platform arm64 -t httpd:1.00 .\n# 查看新镜像的架构\n docker inspect httpd:1.00 | grep Architecture\n# 导出镜像为tar文件\ndocker save -o httpd.tar httpd:1.00\n```\n\n<mark>扩展：通过镜像多重构建复制也可用该docker命令直接替代。</mark>\n\n```shell\ndocker build --rm --platform arm64 -t \"httpd:1.00\" -<<EOF\nFROM multiarch/qemu-user-static:x86_64-aarch64 as qemu\nFROM httpd:latest\nCOPY --from=qemu /usr/bin/qemu-aarch64-static /usr/bin\nCOPY ./index.html  /usr/local/apache2/htdocs/\nEOF\n```\n\n## 在arm服务器上验证\n\n将新的镜像tar文件导入到arm服务器上，创建新容器验证镜像是否可用及镜像中的html文件是否生效。\n\n```shell\n# 导入新镜像\ndocker load -i httpd.tar\n# 查看新镜像的架构\ndocker inspect httpd:1.00 | grep Architecture\n# 运行新容器\ndocker run -itd --name test httpd:1.00\n# 查看运行状态\ndocker ps\n# 查看容器IP\ndocker inspect test | grep IPAddress\n# 访问测试上传的html文件是否生效\ncurl 172.17.0.2\n```\n","tags":["Linux","运维","x86-ARM","docker镜像","跨架构镜像"],"categories":["Linux运维","容器化","docker","容器管理"]},{"title":"使用portainer管理容器","url":"/yyg/c60df0d6/","content":"## 基础环境\n\n系统环境：\n\n操作系统：CentOS 7.9\n\n软件环境：\n\nDocker-26.1.4、Docker Compose-2.27.1 、portainer-2.20.3\n\n配置阿里yum源 :\n\n```shell\ncd /etc/yum.repos.d\nmv CentOS-Base.repo CentOS-Base.repo.bak\ncurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\n```\n\n配置防火墙策略：\n\n```shell\nfirewall-cmd --add-port=9443/tcp --permanent\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all\n```\n\n## 安装Docker\n\n（1）安装 yum-utils 软件包（提供 yum-config-manager 实用程序）并设置存储库。\n\n```shell\n# 安装 yum-utils 软件包\nyum install -y yum-utils\n# 设置docker-ce存储库\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n```\n\n（2）安装最新版docker和docker compose。\n\n```shell\nyum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n\n（3）修改Docker的数据目录为“/data/dockerData”,并修改docker镜像源。\n\n```shell\n# 创建Docker守护进程配置文件,编辑配置文件/etc/docker/daemon.json，配置docker数据目录，并修改docker镜像源\necho '{\n  \"data-root\": \"/data/dockerData\",\n    \"registry-mirrors\": [\n    \"https://registry.docker-cn.com\",\n    \"http://hub-mirror.c.163.com\",\n    \"https://dockerhub.azk8s.cn\",\n    \"https://mirror.ccs.tencentyun.com\",\n    \"https://registry.cn-hangzhou.aliyuncs.com\",\n    \"https://docker.mirrors.ustc.edu.cn\",\n    \"https://docker.m.daocloud.io\",\n    \"https://noohub.ru\",\n    \"https://huecker.io\",\n    \"https://dockerhub.timeweb.cloud\"\n  ]\n}' > /etc/docker/daemon.json\n```\n\n（4）启动Docker服务，并设置为开机自启动。\n\n```shell\n# 启动Docker服务\nsystemctl start docker\n# 设置Docker为开机自启动\nsystemctl enable docker\n# 查看Docker服务状态\nsystemctl status docker\n```\n\n## 安装portainer\n\n（1）拉取portainer镜像并创建容器\n\n方式一：使用docker命令\n\n```\ndocker run -d -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /data/portainer:/data portainer/portainer-ce:2.20.3\n```\n\n方式二：使用compose文件安装\n\n```shell\nvi portainer.yml\n--------------------------------------------------\nservices:\n  portainer:\n    image: portainer/portainer-ce:2.20.3\n    container_name: portainer\n    restart: always\n    ports:\n      - \"9443:9443\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /data/portainer:/data\n    user: \"0\"\n    networks:\n      net:\n        ipv4_address: 172.20.110.10\n\nnetworks:\n net:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.110.0/24\n--------------------------------------------------\n#运行安装\ndocker compose -f portainer.yml up -d\n```\n\n（2）初始化portainer，设置admin用户的密码\n\n![image-20240718163227768](使用portainer管理容器/1.png)\n\n（3）进入系统后，点击“Get Started”，查看本机的容器，如下图。\n\n![image-20240718164748722](使用portainer管理容器/2.png)\n\n## 多实例环境管理\n\nPortainer可以管理多个环境，包括Docker Standalone、Docker Swarm、Kubernetes、ACI等。\n\n使用Portainer管理多Docker Standalone实例，有两种方式，分别是api和客户端容器。\n\n### 方式一：使用docker api 管理Docker Standalone实例\n\n（1）在其他实例环境上，修改docker服务的配置文件，让Docker守护进程通过TCP在的2375端口监听连接，并通过UNIX套接字在本地监听连接。\n\n即在ExecStart后添加-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock参数。\n\n```shell\nvi /usr/lib/systemd/system/docker.service\n#修改内容如下\n[Service]\nExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\n```\n\n（2）重新加载配置，使配置生效。\n\n```\n#重新加载配置\nsystemctl daemon-reload\n#重启docker\nsystemctl restart docker\n```\n\n（3）设置防火墙策略\n\n```\n#只允许Portainer主机访问该主机的2375端口\nsudo firewall-cmd --permanent --add-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.22.23/32\" port protocol=\"tcp\" port=\"2357\" accept\"\n```\n\n（4）点击Environment-related菜单中的Environments，点击Add environment按钮，添加Docker Standalone，如下图。\n\n![image-20240723122531889](使用portainer管理容器/3.png)\n\n（5）选择API，填写名称和Docker Standalone实例的ip和端口（2375），点击Connect，连接成功后，点击Close，如下图。\n\n![image-20240723122857044](使用portainer管理容器/4.png)\n\n### 方式二：使用agent容器管理Docker Standalone实例\n\n（1）在其他实例环境上，部署agent容器进行连接，具体命令如下：\n\n```\ndocker run -d \\\n  -p 9001:9001 \\\n  --name portainer_agent \\\n  --restart=always \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /var/lib/docker/volumes:/var/lib/docker/volumes \\\n  portainer/agent:2.20.3\n```\n\n（2）设置防火墙策略\n\n```\n#只允许Portainer主机访问该主机的9001端口\nsudo firewall-cmd --permanent --add-rich-rule=\"rule family=\"ipv4\" source address=\"192.168.22.23/9001\" port protocol=\"tcp\" port=\"9001\" accept\"\n```\n\n连接成功后，在Home界面查看两种方式连接的Docker Standalone实例\n\n![image-20240723124359442](使用portainer管理容器/5.png)\n\n","tags":["docker","portainer"],"categories":["Linux运维","容器化","docker","容器管理"]},{"title":"ELK | 使用 docker部署三节点 Elasticsearch 集群","url":"/yyg/56088d5a/","content":"## 环境准备\n\n1. 硬件环境\n\n   操作系统：Centos Stream 9\n\n   CPU：4颗\n\n   内存：8GB\n\n   硬盘：50GB\n\n2. 软件环境\n\n   docker版本：26.1.0\n\n   docker compose版本：v2.26.1\n\n## 部署过程\n\n### 准备基础环境，安装Docker\n\n1、安装 yum-utils 软件包（提供 yum-config-manager 实用程序）并设置存储库，命令如下。\n\n```\n# 安装 yum-utils 软件包\nyum install -y yum-utils\n# 设置docker-ce存储库\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n```\n\n2、安装最新版docker及其组件，命令如下。\n\n```\nyum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n\n3、启动Docker服务，并设置为开机自启动，命令如下。\n\n```\n# 启动Docker服务\nsystemctl start docker\n# 设置Docker为开机自启动\nsystemctl enable docker\n# 查看Docker服务状态\nsystemctl status docker\n```\n\n4、修改Docker的数据目录为“/data/dockerData”，并重启docker服务使配置生效。\n\n```\n# 创建Docker守护进程配置文件\n# 编辑配置文件/etc/docker/daemon.json，配置docker数据目录\nvi /etc/docker/daemon.json\n**************************daemon.json**************************\n{\n  \"data-root\": \"/data/dockerData\"\n}\n**************************daemon.json**************************\n\n# 重启docker服务，使配置生效\nsystemctl restart docker\n```\n\n5、设置与内存映射相关的内核参数为262144，查看应用到系统的内核参数。\n\n```\necho \"vm.max_map_count=262144\" >> /etc/sysctl.conf\nsysctl -p\n```\n\n6、配置防火墙策略\n\n```\nfirewall-cmd --add-port=9200/tcp --permanent\nfirewall-cmd --add-port=80/tcp --permanent\nfirewall-cmd --add-port=443/tcp --permanent\nfirewall-cmd --add-port=5601/tcp --permanent\nfirewall-cmd --reload\nfirewall-cmd --list-all\n```\n\n### 部署elk集群\n\n1、创建构建容器所需要的文件目录，并赋予相应的访问权限。\n\n```\ncd /data\nmkdir es-node1  es-node2  es-node3  elk-kibana\nchmod g+rwx es-node1  es-node2  es-node3  elk-kibana\nchgrp 0 es-node1  es-node2  es-node3  elk-kibana\n```\n\n2、在/data/script 目录下，创建并编辑 .env 文件，用来配置elk的环境变量。\n\n```\nmkdir /data/script\nvi /data/script/.env\n**************************.env**************************\n# Elasticsearch的密码\nELASTIC_PASSWORD=elk#bd@123\n# Kibana的密码\nKIBANA_PASSWORD=elk#bd@123\n# Elastic Stack的版本号\nSTACK_VERSION=8.13.3\n# Elastic Stack的集群名称\nCLUSTER_NAME=ELK-docker-Cluster\n# 指定Elastic Stack的许可证类型\nLICENSE=basic\n# 指定Elasticsearch的端口号\nES_PORT=9200\n# 指定Kibana的端口号\nKIBANA_PORT=5601\n# 指定内存限制\nMEM_LIMIT=2147483648\n# 指定Docker Compose项目的名称\nCOMPOSE_PROJECT_NAME=elk-docker-project\n**************************.env**************************\n```\n\n3、创建logstash容器所需的目录和配置文件。\n\n```\nmkdir -p logstash/config logstash/pipeline\necho 'http.host: \"0.0.0.0\"' > /data/logstash/config/logstash.yml\nvi /data/logstash/pipeline/logstash.conf\n**************************logstash.conf**************************\ninput {\n  beats {\n    port => 5044\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"https://es-node1:9200\"]\n    index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\"\n    user => \"elastic\"\n    password => \"elk#bd@123\"\n    cacert=> \"/usr/share/logstash/config/certs/ca/ca.crt\"\n  }\n}\n**************************logstash.conf**************************\n```\n\n4、创建docker compose文件，编排创建es初始化节点、es三节点、Kibana、logstash容器。\n\n```\nvi /data/script/elk.yml\n**************************elk.yml**************************\nservices:\n  es-setup:\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    container_name: es-setup\n    volumes:\n      - /data/elk-certs:/usr/share/elasticsearch/config/certs\n    user: \"0\"\n    networks:\n      net:\n        ipv4_address: 172.20.100.10\n    command: >\n      bash -c '\n        if [ x${ELASTIC_PASSWORD} == x ]; then\n          echo \"Set the ELASTIC_PASSWORD environment variable in the .env file\";\n          exit 1;\n        elif [ x${KIBANA_PASSWORD} == x ]; then\n          echo \"Set the KIBANA_PASSWORD environment variable in the .env file\";\n          exit 1;\n        fi;\n        \n        if [ ! -f config/certs/ca.zip ]; then\n          echo \"Creating CA\";\n          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;\n          unzip config/certs/ca.zip -d config/certs;\n        fi;\n        if [ ! -f config/certs/certs.zip ]; then\n          echo \"Creating certs\";\n          echo -ne \\\n          \"instances:\\n\"\\\n          \"  - name: es-node1\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es-node1\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          \"  - name: es-node2\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es-node2\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          \"  - name: es-node3\\n\"\\\n          \"    dns:\\n\"\\\n          \"      - es-node3\\n\"\\\n          \"      - localhost\\n\"\\\n          \"    ip:\\n\"\\\n          \"      - 127.0.0.1\\n\"\\\n          > config/certs/instances.yml;\n          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;\n          unzip config/certs/certs.zip -d config/certs;\n        fi;\n\n        echo \"Setting file permissions\";\n        chown -R root:root config/certs;\n        find . -type d -exec chmod 750 \\{\\} \\;;\n        find . -type f -exec chmod 640 \\{\\} \\;;\n        echo \"Waiting for Elasticsearch availability\";\n        until curl -s --cacert config/certs/ca/ca.crt https://es-node1:9200 | grep -q \"missing authentication credentials\"; do sleep 30; done;\n        echo \"Setting kibana_system password\";\n        until curl -s -X POST --cacert config/certs/ca/ca.crt -u \"elastic:${ELASTIC_PASSWORD}\" -H \"Content-Type: application/json\" https://es-node1:9200/_security/user/kibana_system/_password -d \"{\\\"password\\\":\\\"${KIBANA_PASSWORD}\\\"}\" | grep -q \"^{}\"; do sleep 10; done;\n        echo \"All done!\";\n      '\n    healthcheck:\n      test: [\"CMD-\", \"[ -f config/certs/es-node1/es-node1.crt ]\"]\n      interval: 1s\n      timeout: 5s\n      retries: 120\n\n  es-node1:\n    depends_on:\n      es-setup:\n        condition: service_healthy\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    container_name: es-node1\n    restart: always\n    networks:\n      net:\n        ipv4_address: 172.20.100.11\n    volumes:\n      - /data/elk-certs:/usr/share/elasticsearch/config/certs\n      - /data/es-node1:/usr/share/elasticsearch/data\n    ports:\n      - ${ES_PORT}:9200\n    environment:\n      - node.name=es-node1\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es-node1,es-node2,es-node3\n      - discovery.seed_hosts=es-node2,es-node3\n      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es-node1/es-node1.key\n      - xpack.security.http.ssl.certificate=certs/es-node1/es-node1.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es-node1/es-node1.key\n      - xpack.security.transport.ssl.certificate=certs/es-node1/es-node1.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n    extra_hosts:\n      - \"es-node1:172.20.100.11\"\n      - \"es-node2:172.20.100.12\"\n      - \"es-node3:172.20.100.13\"\n\n  es-node2:\n    depends_on:\n      - es-node1\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    container_name: es-node2\n    restart: always\n    networks:\n      net:\n        ipv4_address: 172.20.100.12\n    volumes:\n      - /data/elk-certs:/usr/share/elasticsearch/config/certs\n      - /data/es-node2:/usr/share/elasticsearch/data\n    environment:\n      - node.name=es-node2\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es-node1,es-node2,es-node3\n      - discovery.seed_hosts=es-node1,es-node3\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es-node2/es-node2.key\n      - xpack.security.http.ssl.certificate=certs/es-node2/es-node2.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es-node2/es-node2.key\n      - xpack.security.transport.ssl.certificate=certs/es-node2/es-node2.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n    extra_hosts:\n      - \"es-node1:172.20.100.11\"\n      - \"es-node2:172.20.100.12\"\n      - \"es-node3:172.20.100.13\"\n\n  es-node3:\n    depends_on:\n      - es-node2\n    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}\n    container_name: es-node3\n    restart: always\n    networks:\n      net:\n        ipv4_address: 172.20.100.13\n    volumes:\n      - /data/elk-certs:/usr/share/elasticsearch/config/certs\n      - /data/es-node3:/usr/share/elasticsearch/data\n    environment:\n      - node.name=es-node3\n      - cluster.name=${CLUSTER_NAME}\n      - cluster.initial_master_nodes=es-node1,es-node2,es-node3\n      - discovery.seed_hosts=es-node1,es-node2\n      - bootstrap.memory_lock=true\n      - xpack.security.enabled=true\n      - xpack.security.http.ssl.enabled=true\n      - xpack.security.http.ssl.key=certs/es-node3/es-node3.key\n      - xpack.security.http.ssl.certificate=certs/es-node3/es-node3.crt\n      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.enabled=true\n      - xpack.security.transport.ssl.key=certs/es-node3/es-node3.key\n      - xpack.security.transport.ssl.certificate=certs/es-node3/es-node3.crt\n      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt\n      - xpack.security.transport.ssl.verification_mode=certificate\n      - xpack.license.self_generated.type=${LICENSE}\n    mem_limit: ${MEM_LIMIT}\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    healthcheck:\n      test:\n        [\n          \"CMD-\",\n          \"curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n    extra_hosts:\n      - \"es-node1:172.20.100.11\"\n      - \"es-node2:172.20.100.12\"\n      - \"es-node3:172.20.100.13\"\n\n  kibana:\n    depends_on:\n      es-node1:\n        condition: service_healthy\n      es-node2:\n        condition: service_healthy\n      es-node3:\n        condition: service_healthy\n    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}\n    container_name: kibana\n    restart: always\n    networks:\n      net:\n        ipv4_address: 172.20.100.14\n    volumes:\n      - /data/elk-certs:/usr/share/kibana/config/certs\n      - /data/kibana:/usr/share/kibana/data\n    ports:\n      - ${KIBANA_PORT}:5601\n    environment:\n      - SERVERNAME=kibana\n      - ELASTICSEARCH_HOSTS=https://es-node1:9200\n      - ELASTICSEARCH_USERNAME=kibana_system\n      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}\n      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt\n    mem_limit: ${MEM_LIMIT}\n    healthcheck:\n      test:\n        [\n          \"CMD-\",\n          \"curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'\",\n        ]\n      interval: 10s\n      timeout: 10s\n      retries: 120\n  logstash:\n    image: docker.elastic.co/logstash/logstash:${STACK_VERSION}\n    container_name: logstash\n    volumes:\n      - /data/elk-certs:/usr/share/logstash/config/certs\n      - /data/logstash/pipeline:/usr/share/logstash/pipeline\n      - /data/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml\n    user: \"0\"\n    restart: always\n    ports:\n      - 5044:5044\n    networks:\n      net:\n        ipv4_address: 172.20.100.15\n    extra_hosts:\n      - \"es-node1:172.20.100.11\"\n      - \"es-node2:172.20.100.12\"\n      - \"es-node3:172.20.100.13\"\nnetworks:\n  net:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.100.0/24\n**************************elk.yml**************************\n```\n\n执行命令，创建容器\n\n```\ndocker compose -f /data/script/elk.yml up -d\n```\n\n> [+] Running 42/16\n> ✔ es-node3 Pulled 35.6s\n> ✔ es-setup Pulled 35.6s\n> ✔ logstash Pulled 96.0s\n> ✔ kibana Pulled 99.3s\n> ✔ es-node2 Pulled 35.6s\n> ✔ es-node1 Pulled 35.6s\n>\n> [+] Running 7/7\n> ✔ Network elk-docker-project_net Crea… 0.6s\n> ✔ Container es-setup Healthy 21.6s\n> ✔ Container logstash Started 12.5s\n> ✔ Container es-node1 Healthy 72.5s\n> ✔ Container es-node2 Healthy 72.5s\n> ✔ Container es-node3 Healthy 73.0s\n> ✔ Container kibana Started 73.4s\n\n容器创建完成后，查看各容器状态是否正常\n\n```\ndocker ps\n```\n\n> CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n> 5b591d4ade82 docker.elastic.co/kibana/kibana:8.13.3 “/bin/tini — /usr/l…” 5 minutes ago Up 4 minutes (healthy) 0.0.0.0:5601->5601/tcp, :::5601->5601/tcp kibana\n> e2e75a54149e docker.elastic.co/elasticsearch/elasticsearch:8.13.3 “/bin/tini — /usr/l…” 5 minutes ago Up 5 minutes (healthy) 9200/tcp, 9300/tcp es-node3\n> b8c8357f3634 docker.elastic.co/elasticsearch/elasticsearch:8.13.3 “/bin/tini — /usr/l…” 5 minutes ago Up 5 minutes (healthy) 9200/tcp, 9300/tcp es-node2\n> c552940bd559 docker.elastic.co/elasticsearch/elasticsearch:8.13.3 “/bin/tini — /usr/l…” 5 minutes ago Up 5 minutes (healthy) 0.0.0.0:9200->9200/tcp, :::9200->9200/tcp, 9300/tcp es-node1\n> 0366974065d8 docker.elastic.co/logstash/logstash:8.13.3 “/usr/local/bin/dock…” 5 minutes ago Up 5 minutes 0.0.0.0:5044->5044/tcp, :::5044->5044/tcp, 9600/tcp logstash\n\n5、修改kibana的配置文件，将其语言转化为中文。\n\n```\ndocker cp kibana:/usr/share/kibana/config/kibana.yml .\nvi kibana.yml\n**************************kibana.yml**************************\n #\n# ** THIS IS AN AUTO-GENERATED FILE **\n#\n\n# Default Kibana configuration for docker target\nserver.host: \"0.0.0.0\"\nserver.shutdownTimeout: \"5s\"\nelasticsearch.hosts: [ \"http://elasticsearch:9200\" ]\nmonitoring.ui.container.elasticsearch.enabled: true\ni18n.locale: \"zh-CN\"\nserver.publicBaseUrl: \"http://localhost:5601/\"\n**************************kibana.yml**************************\ndocker cp kibana.yml  kibana:/usr/share/kibana/config/kibana.yml\ndocker restart kibana\n```\n\n6、在浏览器中分别访问`https://10.10.2.103:9200/`和`http://10.10.2.103:5601/`，输入账户和密码，进行查看，如下图所示。\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/1.png)\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/2.png)\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/3.png)\n\n### 推送Linux日志，验证部署\n\n1、安装Filebeat日志采集器\n\n（1）通过rpm方式安装Filebeat日志采集器。\n\n```\n#下载rpm包\ncurl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.13.3-x86_64.rpm\n#执行安装\nrpm -vi filebeat-8.13.3-x86_64.rpm\n```\n\n（2）配置Filebeat\n\n```\n#启动Filebeat\nsystemctl start filebeat\n#查看Filebeat运行状态\nsystemctl status filebeat\n#设置Filebeat为开机自启\nsystemctl enable filebeat\n```\n\n2、配置Filebeat收集Linux日志\n\n（1）配置filebeat文件，将日志发送到 Logstash 服务。\n\n```\n# 修改filebeat配置文件\nmv /etc/filebeat/filebeat.yml /etc/filebeat/filebeat.yml.bak\n# 新建filebeat配置文件\nvi /etc/filebeat/filebeat.yml\n**************************filebeat-dns.yml**************************\n# 配置文件输入，监视日志文件\nfilebeat.inputs:\n- type: log\n  enabled: true\n  # 指定 DNS 日志文件路径\n  paths:\n    - /var/log/messages* \n  # 标识日志类型为elk-linux\n  fields:\n    type: elk-linux      \n  # 将额外的字段添加到根级别    \n  fields_under_root: true        \n\n# 配置输出到 Logstash\n# 指定 Logstash 服务的地址和端口\noutput.logstash:\n  hosts: [\"10.10.2.103:5044\"]   \n**************************filebeat-dns.yml**************************\n```\n\n（2）重启filebeat服务，验证配置是否生效\n\n```\n# 重启filebeat\nsystemctl restart filebeat\n# 删除锁文件后，重新启动 Filebeat 服务\nrm -rf /var/lib/filebeat/filebeat.lock\n# 运行并查看 Filebeat 的运行日志\nfilebeat -e -c /etc/filebeat/filebeat.yml\n```\n\n3、在浏览器中登录Kibana，点击左侧菜单中的 “Stack Management”，管理创建数据视图。如下图所示。\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/4.png)\n\n4、在“Stack Management”界面中，点击“数据视图”，选择索引模式，创建“elk-linux”视图。如下图所示。\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/5.png)\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/6.png)\n\n5、数据视图创建完成后，选择左侧菜单中的“Discover”，查看Linux日志详细内容。如下图所示。由此，elk集群搭建完成。\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/7.png)\n\n![img](使用 docker部署三节点 Elasticsearch 集群(ELK)/8.png)","tags":["docker","日志分析","ELK","Elasticsearch","Kibana","Logstash"],"categories":["日志分析","ELK"]},{"title":"项目问题总结","url":"/yyg/2855a772/","content":"\n### centos 7 开启SSH服务及静态ip设置\n\n步骤 1： 配置远程登录服务\n\n```PowerShell\n#配置远程ssh服务：\n#开启ssh服务：\nvi /etc/ssh/sshd_config,进入配置文件界面\n找到 #port22这一行，将前缀注释符删掉\n找到#PermitRootLogin这一行，将前缀注释符删掉并修改PermitRootLogin空格后的内容为yes\n找到 #PasswordAuthentication这一行，将前缀注释符删掉\n保存并退出配置文件，:wq！\n重启ssh服务，命令为：/etc/init.d/ssh restart\n若系统下重启ssh服务验证码不生效，则reboot重启系统即可生效\n```\n\n步骤 2：开放端口及重新加载防火墙\n\n```PowerShell\nfirewall-cmd --zone=public --add-port=22/tcp --permanent\nfirewall-cmd --reload\n```\n\n步骤 3： 设置 IP 地址，网关 DNS\n\n```PowerShell\n#首先要编辑网卡的配置文件，CentOS 7.9网卡配置文件的路径：/etc/sysconfig/network-scripts/ifcfg-eth0,此路径非常重要，以后主机网络不通，可从此配置文件中查看。\nvi /etc/sysconfig/network-scripts/ifcfg-eth0\n```\n\n步骤 4： 进入配置文件，按 i 对配置文件进行编辑\n\n```PowerShell\n找到BOOTPROTO这一行，将后面改为static\n找到ONBOOT这一行，将后面改为yes\n在末尾添加IP地址、网关DNS\nIPADDR=172.20.1.80\nGATEWAY=172.20.1.1\nNETMASK=255.255.255.0\nDNS1=8.8.8.8\nDNS2=114.114.114.114\n#配置完成之后，重启网络服务即可\n#重启网卡之前一定要重新载入一下配置文件，不然不能立即生效 \nnmcli c reload\nnmcli d connect eth0 #重启网卡\n```\n\n成功之后，进行 ping 测试。\n\n### Centos9 下载python2\n\n1. 安装[zlib](https://so.csdn.net/so/search?q=zlib&spm=1001.2101.3001.7020)库，不然安装pip时会报错（还要重新编译python）\n\n```Plain\n yum -y install zlib*\n```\n\n1. 安装 GCC 包，如果没有安装 GCC，请使用以下命令进行安装\n\n```Shell\nyum -y install gcc openssl-devel bzip2-devel\n```\n\n1. 下载Python-2.7.18\n\n```Plain\n cd /usr/src\n yum -y install wget\n wget https://www.python.org/ftp/python/2.7.18/Python-2.7.18.tgz\n tar xzf Python-2.7.18.tgz\n```\n\n1. 在编译之前还需要在安装源文件中修改Modules/Setup.dist文件，将注释去掉\n\n```Shell\n#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz\n```\n\n1. 编译Python-2.7.18（`make altinstall`用于防止替换默认的 python 二进制文件 /usr/bin/python）\n\n```Shell\ncd /usr/src/Python-2.7.18\n./configure --enable-optimizations\nmake altinstall\n```\n\n不要覆盖或链接原始的 Python 二进制文件，这可能会损坏系统\n\n1. 设置环境变量\n\n```Shell\nvim /etc/profile\nexport PYTHON_HOME=/usr/local/\nPATH=$PATH:$PYTHON_HOME/bin\nsource /etc/profile\n```\n\n1. 下载安装pip\n\n方法一：\n\n```Shell\ncurl \"https://bootstrap.pypa.io/pip/2.7/get-pip.py\" -o \"get-pip.py\"\npython2.7 get-pip.py \n```\n\n方法二：\n\n```Shell\n#安装setuptools\nwget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg --no-check-certificate\nchmod +x setuptools-0.6c11-py2.7.egg\nsh setuptools-0.6c11-py2.7.egg\n#安装pip\nchmod +x pip-1.3.1.tar.gz\ntar xzvf pip-1.3.1.tar.gz\ncd pip-1.3.1\npython2.7 setup.py install\n```\n\n### centos安装gnuplot（centos9需要安装最新版gnuplot）\n\nhttps://blog.csdn.net/weixin_33720452/article/details/91719651\n\ngnuplot官网\n\nhttp://www.gnuplot.info/\n\n#### 解决gnuplot画图中文乱码问题\n\n在 win10 的 C:\\Windows\\Fonts 目录下，找到 “宋体”（simsun.ttc）\n\n放在 centos9 的 /usr/share/fonts/ 目录下。\n\n在root目录下新建一个“.gnuplot”文件将以下代码写进文件中即可\n\n```Shell\n########################\n### 设置默认输入中文 ###\n########################\n\n# 设置终端字体\nset term x11 font \"Monospace\" enhanced\n\n# 设置终端编码为UTF-8\nset encoding utf8\n\n# 设置标签和标题的字体和大小\nset label font \",14\"\nset title font \",16\"\n\n# 设置中文字体和大小\nset label font \"宋体,14\"\nset title font \"宋体,16\"\n\n# 设置刻度标签和图例的字体和大小\nset key font \",14\"\nset xtics font \",14\"\nset ytics font \",14\"\n\n# 设置中文字体和大小\nset key font \"宋体,14\"\nset xtics font \"宋体,14\"\nset ytics font \"宋体,14\"\n\n#########################\n### 避免乱码问题 ###\n#########################\n\n# 设置终端为PNG格式，并指定输出文件名称\nset term png font \",14\" enhanced size 800,600\n\n# 设置终端编码为UTF-8\nset encoding utf8\n```\n\n### Git clone代码到本地每次要输入用户名和密码\n\n```Shell\n# 运行改命令后的第一次执行git pull或git push或git clone命令仍需要输入用户名密码\ngit config --global credential.helper store\n```\n\n### docker中的Api容器()缺少GD而使得验证码无法显示\n\n#### 方法一：\n\n1. 进入到你的Docker容器中。可以使用以下命令进入容器的终端：\n\n```Plaintext\ndocker exec -it [容器ID] /bin/bash\n```\n\n其中`[容器ID]`是你的API容器的ID。\n\n1. 安装GD库。根据你的操作系统和容器配置，可以尝试以下命令：\n   1. ```Plaintext\n      apt-get update\n      apt-get install -y libgd-dev\n      docker-php-ext-configure gd --with-freetype-dir=/usr/include/ --with-jpeg-dir=/usr/include/\n      docker-php-ext-install -j$(nproc) gd\n      ```\n2. 退出容器终端并重新启动容器，问题解决。\n\n![img](项目问题总结/1711208815491-1-1711208824466-4.png)\n\n#### 方法二：\n\n1. 编译 phpGD 库扩展\n\n```PowerShell\n# 更新软件源\napt-get update\n# 安装相关库\napt-get install -y libwebp-dev libjpeg-dev libpng-dev libfreetype6-dev zlib1g-dev\n# 解压源码\ndocker-php-source extract \n# gd 源码文件夹\ncd /usr/src/php/ext/gd\ndocker-php-ext-configure gd --with-webp=/usr/include/webp --with-jpeg=/usr/include --with-freetype=/usr/include/freetype2\n或 \ndocker-php-ext-configure gd --with-webp-dir=/usr/include/webp --with-jpeg-dir=/usr/include --with-freetype-dir=/usr/include/freetype2\n#安装\nmake && make install\n```\n\n1. 安装 gd 扩展\n\n```PowerShell\ndocker-php-ext-install gd\n```\n\n### docker容器中部署的项目无法与gitee上的代码同步\n\n1. 进入存储项目代码的目录\n2. **强制覆盖本地代码**\n\n```PowerShell\ngit fetch --all\ngit reset --hard origin/master\n```\n\n### 网站访问显示 “Index of” 列表页面\n\n只需要在该目录下设置一个.htaccess文件,并在该文件中注入以下代码\n\n```PowerShell\nOptions -Indexes\n```\n\n**给.sh添加可执行权限并运行**\n\n1、添加可执行权限\n\n```Plain\nchmod +x xxx.sh\n```\n\n2、执行.sh文件\n\n```Plain\n./xxx.sh\n```\n\n### 同时日志同时打印在屏幕和文件中\n\n```Shell\n#将输出结果同时打印在屏幕和test.log文件中\ndocker images | grep mysql | tee -a test.log\n```\n\n### 常用命令：\n\n```PowerShell\ndocker ps | grep pj-zznu\n docker exec -it   /bin/bash\n apt-get dist-upgrade\n```\n\n### 项目中的api界面不显示\n\n进入php.ini文件所在位置，将display_errors = off改为display_errors = On，并将 .htaccess 改为.htaccess.bak（或删除）即可\n\n```PowerShell\n cd /usr/local/etc/php/\n cat php.ini |grep display\n mv .htaccess .htaccess.bak\n```\n\n### 一键部署docker-compose文件，并生成镜像脚本\n\n```Shell\n#!/bin/bash\n\n# 创建docker-compose文件，安装部署容器\n#新建docker-compose文件\ntouch docker-compose.yml\n\nread -p \"请输入容器名称：\" container_name\nread -p \"请输入镜像名称(格式如 mysql:5.8)：\" images_name\necho \"请输入environment所需内容，可输入多行数据，按Ctrl+D结束：\"\nenvironment_things=$(cat)\nread -p \"请输入端口号(格式如\"\\\"3306:3306\"\\\",引号也有)：\" port_name\nread -p \"请输入映射目录(格式如/data/database/pj-zznu-ywgl-1.0:/var/lib/mysql)：\" contents_name\nread -p \"请输入容器ip地址(格式如 172.1.21.12)：\" ip_name\n#将配置写入文件\nif [ -n \"$environment_things\" ] && [ -n \"$port_name\" ]; then\n  echo \"version: '2.2'\nservices:\n  $container_name:\n    image: $images_name\n    container_name: $container_name\n    restart: always\n    environment:\n      $environment_things\n    ports:\n      - $port_name\n    volumes:\n      - $contents_name\n    user: \\\"0\\\"\n    networks:\n      net:\n        ipv4_address: $ip_name\n\" > docker-compose.yml\nelif [ -z \"$environment_things\" ]; then\n  echo \"environment设置已跳过\n  \"\n  echo \"version: '2.2'\nservices:\n  $container_name:\n    image: $images_name\n    container_name: $container_name\n    restart: always\n    ports:\n      - $port_name\n    volumes:\n      - $contents_name\n    user: \\\"0\\\"\n    networks:\n      net:\n        ipv4_address: $ip_name\n\" > docker-compose.yml\nelif [ -z \"$port_name\" ]; then\n  echo \"端口设置已跳过\n  \"\n  echo \"version: '2.2'\nservices:\n  $container_name:\n    image: $images_name\n    container_name: $container_name\n    restart: always\n    environment:\n      $environment_things\n    volumes:\n      - $contents_name\n    user: \\\"0\\\"\n    networks:\n      net:\n        ipv4_address: $ip_name\n\" > docker-compose.yml\nelif [ -z \"$environment_things\" ] && [ -z \"$port_name\" ]; then\n  echo \"environment和端口设置已跳过\n  \"\n  echo \"version: '2.2'\nservices:\n  $container_name:\n    image: $images_name\n    container_name: $container_name\n    restart: always\n    volumes:\n      - $contents_name\n    user: \\\"0\\\"\n    networks:\n      net:\n        ipv4_address: $ip_name\n\" > docker-compose.yml\nfi\n\n\n#给docker-compose文件添加可执行权限\nchmod +x docker-compose.yml\n\n#重命名文件\nmv docker-compose.yml docker-compose-$container_name.yml\n\n#运行docker-compose文件，生成容器\ndocker compose -f docker-compose-$container_name.yml up -d\n```\n\n在httpd容器中改变映射目录\n\n```Shell\ndocker cp htttpd-test:/usr/local/apache2/conf/httpd.conf httpd.conf\n#将httpd.conf中的DocumentRoot修改为容器中要映射的新目录\n#重启即可\ndocker restart tl-dev-stat\n```\n\n### nginx跨主机代理容器\n\n如下图所示，主机2上的nginx要要想代理到主机1的容器，有两种方式：\n\n![image-20240323234857825](项目问题总结/image-20240323234857825-1711208939935-6.png)\n\n1. 二次代理\n\n在主机1上安装nginx，通过主机1上的nginx将容器代理出来，再使用主机二上的nginx，将主机1代理出来\n\n2. 配置路由\n\n直接将容器的ip或者docker网络写入主机2的路由\n\ndocker容器设置87、95端口在浏览器访问受限的解决方法\n\nhttps://blog.csdn.net/qq_44159028/article/details/115705470\n\ndocker修改默认存储路径\n\n```Shell\n#/data/docker为存储新路径\n#############修改内容################\nvi /etc/docker/daemon.json \n{\n  \"data-root\": \"/data/docker\"\n}\n#############修改内容################\nsystemctl restart docker\n```\n\n### api容器内安装数据库依赖\n\n```Shell\n#进入api容器后安装\ndocker-php-ext-install mysqli\ndocker-php-ext-install pdo_mysql\napt-get install libpq-dev\ndocker-php-ext-install pdo_pgsql\ndocker-php-ext-install pgsql\napt-get install libxml2-dev -y\ndocker-php-ext-install soap\n```\n\n### api容器内添加/api到000-default.conf配置文件\n\n```Shell\ncd  /etc/apache2/sites-available\n#写入命令\n sed -i '/<\\/VirtualHost>/i\\\nAlias \\/api \"\\/var\\/www\\/html\\/OpenAPI\"\\\n\\<Directory \"\\/var\\/www\\/html\\/OpenAPI\"\\>\\\n    Options FollowSymLinks\\\n    AllowOverride None\\\n    Require all granted\\\n\\<\\/Directory\\>' 000-default.conf\n```\n\n### 在云服务器上使用openEuler 20.03 LTS安装软件时候报错:Error: GPG check FAILED\n\n方法1、可以在安装包的后面加一个 “–nogpgcheck” 这个参数，可绕过GPG验证成功安装。\n\n方法2、可以在安装包的后面加一个 “ --nogpgcheck --nobest“这个参数\n\n方法3、\n\nhttps://blog.csdn.net/xukeworknet/article/details/84681626\n\n并修改openEuler.repo中的gpgcheck为0\n\n### ceph出现`1 monitors have not enabled msgr2`\n\n```Shell\nceph mon enable-msgr2\n```\n\n### ceph出现`mons are allowing insecure global_id reclaim`\n\n```Shell\nceph config set mon auth_allow_insecure_global_id_reclaim false\n```\n\n### docker安装zip、pcntl、imagick\n\n```Shell\ndocker-php-ext-install zip\ndocker-php-ext-install pcntl\n#安装imagick\napt-get install imagemagick\napt-get install libmagick++-dev\npecl install imagick\ndocker-php-ext-enable imagick\nphp -m|grep imagick\n```\n\n### nginx代理https，并将http强制转为https\n\n安装https证书\n\n```Shell\n#创建证书目录，并赋予权限\nmkdir -p /etc/nginx/https-ca\nchmod  -R 777 /etc/nginx/https-ca\ncd /etc/nginx/https-ca\n#创建私钥\nopenssl genrsa -des3 -out ca.key 2048\n#生成ca证书，ip为本机ip\nopenssl req -sha512 -new \\\n     -subj \"/C=CN/ST=CN/L=CN/O=test/OU=test/CN=test\" \\\n     -key ca.key \\\n     -out ca.csr\n #备份证书\n cp ca.key  ca.key.org\n #转化为不带密码的私钥\n openssl rsa -in ca.key.org -out ca.key\n #使用证书进行签名\n  openssl x509 -req -days 100000  -in ca.csr -signkey ca.key -out ca.crt\nsudo firewall-cmd  --add-port=443/tcp --permanent\nsudo firewall-cmd  --add-port=80/tcp --permanent\nsudo firewall-cmd --reload\nsudo firewall-cmd --list-all\n\n#修改nginx.conf\nsudo cp nginx.conf nginx.conf.bak\n#将80部分注释\n###################################\n#    server {\n#        listen       80;\n#        listen       [::]:80;\n#        server_name  _;\n#        root         /usr/share/nginx/html;\n#\n#        # Load configuration files for the default server block.\n#        include /etc/nginx/default.d/*.conf;\n#\n#        error_page 404 /404.html;\n#            location = /40x.html {\n#        }\n#\n#        error_page 500 502 503 504 /50x.html;\n#            location = /50x.html {\n#        }\n#    }\n#\n###################################\n#将最后的部分修改成如下内容：\n###################################\nserver {\n        listen       443 ssl http2;\n        listen       [::]:443 ssl http2;\n        server_name  _;\n        root         /usr/share/nginx/html;\n\n        ssl_certificate \"/etc/nginx/https-ca/ca.crt\";\n        ssl_certificate_key \"/etc/nginx/https-ca/ca.key\";\n        ssl_session_cache shared:SSL:1m;\n        ssl_session_timeout  10m;\n       # ssl_ciphers PROFILE=SYSTEM;\n\n        ssl_prefer_server_ciphers on;\n\n        # Load configuration files for the default server block.\n        include /etc/nginx/default.d/*.conf;\n\n        error_page 404 /404.html;\n            location = /40x.html {\n        }\n\n        error_page 500 502 503 504 /50x.html;\n            location = /50x.html {\n        }\n    }\n\n###################################\n\n#在/etc/nginx/conf.d中对应业务的nginx配置文件中加入以下内容\n######################################\nserver {\n    listen 443 ssl http2;\n    listen [::]:443 ssl http2;\n    server_name mlzy.hactcm.edu.cn;\n    root /usr/share/nginx/html;\n\n    ssl_certificate \"/etc/nginx/ssl/ca.crt\"; #证书位置\n    ssl_certificate_key \"/etc/nginx/ssl/ca.key\"; #密钥位置\n    ssl_session_cache shared:SSL:1m;\n    ssl_session_timeout 10m;\n\n    ssl_prefer_server_ciphers on;\n\n    location / {\n        proxy_pass http://211.69.32.39/; #主机ip\n        proxy_set_header Host $host;\n        client_max_body_size 1000M;\n    }\n\n    error_page 497 https://$host$request_uri;\n\n    if ($scheme != \"https\") {\n        return 301 https://$host$request_uri;\n    }\n}\n######################################\n```\n\n### arm架构部署dotnet:8.0容器\n\n使用docker compose编排部署容器时，dotnet8出现无法启动现象，具体异常日志如下：\n\n> GC: Failed to initialize GCToOSInterface\n> \n> GC initialization failed with error 0x80004005\n> \n> Failed to create CoreCLR, HRESULT: 0x80004005\n\n```Shell\n #要使得容器正常运行，需要将标注部分加入compose文件\ntest-dotnet:\n    image: test-dotnet:1.0\n    container_name: test-dotnet\n    restart: always\n    security_opt:  #加入\n      - \"seccomp=unconfined\" #加入\n    volumes:\n      - /xxxxx/:/xxxxx\n    user: \"0\"\n    networks:\n      net:\n        ipv4_address: 172.20.106.74\n```\n\n<mark>dotnet8容器的默认开放端口为8080</mark>\n\n### liunx安装db_load\n\n```Shell\n#Debian\napt-get install db-util\n\n#Ubuntu\napt-get install db-util\n\n#Alpine\napk add db\n\n#Arch Linux\npacman -S db\n\n#Kali Linux\napt-get install db-util\n\n#CentOS\nyum install libdb-utils\n\n#Fedora\ndnf install libdb-utils\n\n#Raspbian\napt-get install db-util\n\n#Docker\ndocker run cmd.cat/db_load db_load\n```\n\n### 查看当前目录下文件数量\n\n```Shell\n#当前目录下的文件数量（不包含子目录中的文件）\nls -l | grep \"^-\" | wc -l\n#当前目录下的目录数量（不包含子目录）\nls -l | grep \"^d\" | wc -l\n#查看当前目录下的文件数量（包含子目录中的文件）注意：R，代表子目录\nls -lR | grep \"^-\" | wc -l\n#查看当前目录下的目录个数（不包含子目录中的目录）\nls -l | grep \"^d\" | wc -l\n#查看当前目录下的目录数量（包含子目录中的目录）注意：R，代表子目录\nls -lR | grep \"^d\" | wc -l\n# 例如：统计所有以“20161124”开头的目录下的全部文件数量\nls -lR 20161124*/|grep \"^-\"| wc -l\n```\n\n### 在x86机器上使用**QEMU和TAP-Windows安装****arm****虚拟机**\n\n```Shell\n#在qemu所在目录打开cmd，执行以下命令\n#创建硬盘镜像\nqemu-img create -f qcow2 openEuler-22.03-LTS-SP2-aarch64.img 20G\n#安装系统\nqemu-system-aarch64 -m 4096 -cpu cortex-a57 -smp 2 -M virt -bios F:\\qemu\\script\\QEMU_EFI.fd -net nic -net tap,ifname=TAP -device nec-usb-xhci -device usb-kbd -device usb-mouse -device VGA -drive if=none,file=F:\\OS\\openEuler-22.03-LTS-SP2-aarch64-dvd.iso,id=cdrom,media=cdrom -device virtio-scsi-device -device scsi-cd,drive=cdrom -drive if=none,file=E:\\centos-clean-VBox\\openEuler-arm\\openEuler-arm.qcow2,id=hd0 -device virtio-blk-device,drive=hd0\nqemu-system-aarch64 -m 4G -cpu cortex-a72 -smp 2 -M virt -bios F:\\qemu\\script\\QEMU_EFI.fd -device nec-usb-xhci -device usb-kbd -device usb-mouse -device VGA -cdrom F:\\OS\\openEuler-22.03-LTS-SP2-aarch64-dvd.iso -hda E:\\centos-VBox\\openeuler-arm-172.20.1.60\\openEuler-22.03-LTS-SP2-aarch64.img\n```","tags":["运维","总结"],"categories":["梳理总结"]},{"title":"Hexo-Butterfly美化教程-[1]基础环境","url":"/yyg/e102aa73/","content":"\n## 基础环境：\n\n1. 操作系统：Windows10/11\n2. 运行环境：node.js\n\n## 实现过程：\n\n### 安装node.js\n\n在node.js中文官网`https://www.nodejs.com.cn/download_current.html`，下载适用于Windows的64位安装包，安装JavaScript运行环境。下载完成后，双击安装程序，按引导进行安装，在安装过程中需勾选”Automatically install the necessary tools. ……“，如图所示。安装过程中，弹出命令指示符，回车下载依赖工具。\n\n![img](Hexo-Butterfly美化教程[1]-基础环境/1.png)\n\n安装完成后，使用快捷键`win+r`,调用运行窗口，输入`cmd`，在命令指示符中输入命令，验证node和npm版本号。\n\n```\nnode -v\nnpm -v\n```\n\n![img](Hexo-Butterfly美化教程[1]-基础环境/2.png)\n\n### 安装git\n\n从官网`https://git-scm.com/download/win`下载安装程序，按安装引导一直点击下一步即可。\n\n### 安装hexo\n\n在命令指示符中输入命令安装hexo。\n\n```\nnpm install hexo-cli -g \n```\n\n创建博客存放目录\n\n进入要存放博客目录的磁盘，以“F”盘为例，在如下图所示的磁盘路径框中输入“cmd”，点击回车，打开命名指示符。\n\n![img](Hexo-Butterfly美化教程[1]-基础环境/3.png)\n\n在其中输入如下命令创建博客目录，此处的“blog”为自定义名称。等待博客目录初始化完成。\n\n```\nhexo init blog\n```\n\n初始化完成后，进入blog目录，在磁盘路径框中输入“cmd”，点击回车，输入以下命令下载依赖和测试博客是否部署成功。\n\n```\n#下载依赖\nnpm install\n\n#本地测试\nhexo clean && hexo g && hexo s\n```\n\n成功显示的命令细节如下：\n\n> F:\\blog>hexo clean && hexo g && hexo s\n> INFO Validating config\n> INFO Deleted database.\n> INFO Validating config\n> INFO Start processing\n> INFO Files loaded in 463 ms\n> INFO Generated: archives/index.html\n> INFO Generated: archives/2024/index.html\n> INFO Generated: archives/2024/04/index.html\n> INFO Generated: index.html\n> INFO Generated: css/style.css\n> INFO Generated: fancybox/jquery.fancybox.min.css\n> INFO Generated: fancybox/jquery.fancybox.min.js\n> INFO Generated: js/jquery-3.6.4.min.js\n> INFO Generated: js/script.js\n> INFO Generated: css/images/banner.jpg\n> INFO Generated: 2024/04/18/hello-world/index.html\n> INFO 11 files generated in 660 ms\n> INFO Validating config\n> INFO Start processing\n> INFO Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.\n\n在浏览器中访问 `http://localhost:4000/` ，查看默认博客界面。\n\n{% folding green open,博客搭建系列文章 %}\n\n{% series 博客搭建教程 %}\n\n{% endfolding %}\n\n","tags":["Hexo","Butterfly","博客教程"],"categories":["博客教程","博客搭建"]},{"title":"LAMP部署","url":"/yyg/a8d3d305/","content":"\n## 基础环境\n\n系统环境：\nCentOS Stream 9\n\n软件环境：\nApache-2.4.57\nMysql-8.0.36（mariaDB）\nPHP-8.0.30\n\n## 部署过程\n\n### 配置安全策略\n\n```\nfirewall-cmd --add-port=80/tcp --permanent\nfirewall-cmd --reload\nfirewall-cmd --list-all\n```\n\n### 安装Apache\n\n```\nyum -y install httpd\nsystemctl start httpd\nsystemctl enable httpd\n# 查看版本\nhttpd -v\n```\n\n### 安装PHP\n\n```\n# 安装php相应模块\nyum -y install php php-curl php-dom php-exif php-fileinfo php-fpm php-gd php-hash php-mbstring php-mysqli php-openssl php-pcre php-xml\n# 查看版本\nphp -v\n# 测试php\necho \"<?php phpinfo(); ?>\" > /var/www/html/test.php\n#在浏览器中访问http://本机ip/test.php\n# 删除测试用例\nrm -f /var/www/html/test.php\n```\n\n### 安装Mysql\n\n```\nyum -y install mysql-server\n# 查看版本\nmysql -V\nsystemctl start mysqld\nsystemctl enable mysqld\n# 查看MySQL的初始密码\ngrep \"password\" /var/log/mysql/mysqld.log\n# MySQL的安全性配置\nmysql_secure_installation\n```\n\n> Securing the MySQL server deployment.\n> \n> Connecting to MySQL using a blank password.\n> \n> VALIDATE PASSWORD COMPONENT can be used to test passwords\n> and improve security. It checks the strength of password\n> and allows the users to set only those passwords which are\n> secure enough. Would you like to setup VALIDATE PASSWORD component?\n> \n> Press y|Y for Yes, any other key for No: y\n> \n> There are three levels of password validation policy:\n> \n> LOW Length >= 8\n> MEDIUM Length >= 8, numeric, mixed case, and special characters\n> STRONG Length >= 8, numeric, mixed case, special characters and dictionary file\n> \n> Please enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 0\n> Please set the password for root here.\n> \n> New password:\n> \n> Re-enter new password:\n> \n> Estimated strength of the password: 50\n> Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : y\n> By default, a MySQL installation has an anonymous user,\n> allowing anyone to log into MySQL without having to have\n> a user account created for them. This is intended only for\n> testing, and to make the installation go a bit smoother.\n> You should remove them before moving into a production\n> environment.\n> \n> Remove anonymous users? (Press y|Y for Yes, any other key for No) : y\n> Success.\n> \n> Normally, root should only be allowed to connect from\n> ‘localhost’. This ensures that someone cannot guess at\n> the root password from the network.\n> \n> Disallow root login remotely? (Press y|Y for Yes, any other key for No) : y\n> Success.\n> \n> By default, MySQL comes with a database named ‘test’ that\n> anyone can access. This is also intended only for testing,\n> and should be removed before moving into a production\n> environment.\n> \n> Remove test database and access to it? (Press y|Y for Yes, any other key for No) : y\n> \n> - Dropping test database…\n>   Success.\n> - Removing privileges on test database…\n>   Success.\n> \n> Reloading the privilege tables will ensure that all changes\n> made so far will take effect immediately.\n> \n> Reload privilege tables now? (Press y|Y for Yes, any other key for No) : y\n> Success.\n> \n> All done!\n\n### 验证测试\n\n安装WordPress进行验证LAMP的验证测试\n\n（1）下载并配置WordPress\n\n```\ncd /var/www/html/\n# 安装下载解压工具\nyum -y install wget tar\n# 下载并解压WordPress程序\nwget https://cn.wordpress.org/latest-zh_CN.tar.gz\ntar zxvf latest-zh_CN.tar.gz\n```\n\n（2）创建数据库\n\n```\ncreate user 'wordpress'@'localhost' identified by 'wordpress@123';\ngrant all privileges on wordpress.* to 'wordpress'@'localhost';\nflush privileges;\nexit;\n```\n\n（3）重启服务\n\n```\nsystemctl restart httpd\n```\n\n（4）WordPress初始化配置\n\n在`/var/www/html/wordpress`中创建wp-config.php文件。\n\n```\nvi /var/www/html/wordpress/wp-config.php\n**************************wp-config.php**************************\n<?php\n/**\n * WordPress 基础配置文件\n *\n * 这个文件被安装程序用于自动生成 wp-config.php 配置文件\n * 您不必使用网站，可以将这个文件复制到「wp-config.php」并填写这些值。\n *\n * 本文件包含以下配置选项：\n *\n * * 数据库设置\n * * 密钥\n * * 数据库表名前缀\n * * ABSPATH\n *\n * @link https://wordpress.org/documentation/article/editing-wp-config-php/\n *\n * @package WordPress\n */\n\n// ** 数据库设置 - 您可以从您的主机获取这些信息 ** //\n/** WordPress 数据库名称 */\ndefine( 'DB_NAME', 'wordpress' );\n\n/** 数据库用户名 */\ndefine( 'DB_USER', 'wordpress' );\n\n/** 数据库密码 */\ndefine( 'DB_PASSWORD', 'wordpress@123' );\n\n/** 数据库主机 */\ndefine( 'DB_HOST', 'localhost' );\n\n/** 创建表时使用的数据库字符集。 */\ndefine( 'DB_CHARSET', 'utf8mb4' );\n\n/** 数据库排序规则类型。如不确定，请勿更改。 */\ndefine( 'DB_COLLATE', '' );\n\n/**#@+\n * 身份验证唯一密钥与盐。\n *\n * 将这些更改为任意独一无二的字符串！您可以使用\n * {@link https://api.wordpress.org/secret-key/1.1/salt/ WordPress.org 密钥服务}\n * 生成这些。\n *\n * 您可以随时更改这些内容以使所有现有 cookies 失效。\n * 这将强制所有用户必须重新登录。\n *\n * @since 2.6.0\n */\ndefine( 'AUTH_KEY',         'T]opAWiO&i]=T0%gG|Cr8ZiHC4(AjG3#,Oa%Z,Xus}tB:CMH*m?TAJ!g8rCgn9G{' );\ndefine( 'SECURE_AUTH_KEY',  'u]bD9(4N<|LD=!XlaLf8mI5vQM!!Wy,`=9[2g1g86Cp~EZnUe$q{P3W.[aH$n,bV' );\ndefine( 'LOGGED_IN_KEY',    '-}sR0i@QOgDAbYWs:blZjDaJF-NdN|l5Vtiw2kVy(TUd0]:7~_)Lsg(t5kP>FVs~' );\ndefine( 'NONCE_KEY',        ':*qn^%vaF;vWh$f/rR*SI8]Er=jpW{bKRWt4/F25b:ULoJGi&:*B{SP9D$Dx]t1=' );\ndefine( 'AUTH_SALT',        ':1NyIcq-rH.2Qe@cH{xbKICQ`>!bmz_cKl@SQcLYM]||y9}dII:4j<d~sG~[mGR/' );\ndefine( 'SECURE_AUTH_SALT', 'IRC$m2W;1CZ`PaR315_evXujKAgHr?x1.fLws,8tJ_$g+0EJkY?Y$LF*QXi*7h=l' );\ndefine( 'LOGGED_IN_SALT',   'Zcg^hRDB5DXV<v?NxVNBMoT=s#mFvCw$A%!8{uBdl6EeZ`IhB.~8FNPprKgB>sKZ' );\ndefine( 'NONCE_SALT',       'T#D,{:EEHw`~F~B2%R/TP*Kt-,/-| /](V-MJz7Nr?)XBr~rnEY*Hfs1rwUUMYi-' );\n\n/**#@-*/\n\n/**\n * WordPress 数据表前缀。\n *\n * 如果您为每个安装分配一个唯一前缀，您可以在一个数据库中拥有多个安装。\n * 请只使用数字、字母和下划线！\n */\n$table_prefix = 'wp_';\n\n/**\n * 开发者专用：WordPress 调试模式。\n *\n * 将此值更改为 true 以启用开发过程中的通知显示。\n * 强烈建议插件和主题开发人员在其开发环境中使用 WP_DEBUG。\n *\n * 有关可用于调试的其他常量的信息，请访问文档。\n *\n * @link https://wordpress.org/documentation/article/debugging-in-wordpress/\n */\ndefine( 'WP_DEBUG', false );\n\n/**\n * 简体中文专属：ICP 备案号显示\n *\n * 在设置 → 常规中设置你的 ICP 备案号。\n * 可调用简码 [cn_icp] 或函数 cn_icp() 显示。\n *\n * @since 6.5.0\n * @link https://cn.wordpress.org/support/i10n-features/\n */\ndefine( 'CN_ICP', true );\n\n/**\n * 简体中文专属：公安备案号显示\n *\n * 在设置 → 常规中设置你的公安备案号。\n * 可调用简码 [cn_ga] 或函数 cn_ga() 显示。\n *\n * @since 6.5.0\n * @link https://cn.wordpress.org/support/i10n-features/\n */\ndefine( 'CN_GA', true );\n\n/* 在这行和「停止编辑」行之间添加任何自定义值。 */\n\n\n\n/* 就这样，停止编辑！祝您使用愉快。 */\n\n/** WordPress 目录的绝对路径。 */\nif ( ! defined( 'ABSPATH' ) ) {\n    define( 'ABSPATH', __DIR__ . '/' );\n}\n\n/** 设置 WordPress 变量和包含的文件。 */\nrequire_once ABSPATH . 'wp-settings.php';\n**************************wp-config.php**************************\n```\n\n在浏览器中输入`http://本地ip/wordpress`，进行初始化配置。配置完成并登录WordPress之后，LAMP验证测试成功。","tags":["Linux","Apache","Mysql","PHP"],"categories":["Linux运维","Web服务架构部署"]},{"title":"LNMP部署","url":"/yyg/776a1cd4/","content":"## 基础环境\n\n系统环境：CentOS Stream 9\n\n软件环境：nginx-1.20.1、mysql-8.0.36（mariaDB）、php-8.0.30\n\n## 部署过程\n\n### 方案一：yum安装\n\n#### 步骤一：配置安全策略\n\n```\n# 开放80端口\nfirewall-cmd --add-port=80/tcp --permanent\nsudo firewall-cmd --reload && sudo firewall-cmd --list-all\n# 关闭SELinux\nsetenforce 0\nsed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config\n```\n\n#### 步骤二：安装nginx\n\n```\nyum -y install nginx\n# 查看nginx版本\nnginx -v\n```\n\n#### 步骤三：安装并配置mysql\n\n安装mysql\n\n```\nyum -y install mysql-server\n#查看mysql版本\nmysql -V\n# 启动并设置开机自启\nsystemctl start mysqld\nsystemctl enable mysqld\n```\n\n配置mysql\n\n（1）查看root用户的初始密码\n\n```\ngrep 'password' /var/log/mysql/mysqld.log\n```\n\n> 2024-05-08T09:37:31.765242Z 6 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the —initialize-insecure option.\n\n（2）配置MySQL的安全性\n\n```\nmysql_secure_installation\n```\n\n> Securing the MySQL server deployment.\n>\n> Connecting to MySQL using a blank password.\n>\n> VALIDATE PASSWORD COMPONENT can be used to test passwords\n> and improve security. It checks the strength of password\n> and allows the users to set only those passwords which are\n> secure enough. Would you like to setup VALIDATE PASSWORD component?\n>\n> **设置MySQL的新密码**\n>\n> Press y|Y for Yes, any other key for No: y\n>\n> There are three levels of password validation policy:\n>\n> LOW Length >= 8\n> MEDIUM Length >= 8, numeric, mixed case, and special characters\n> STRONG Length >= 8, numeric, mixed case, special characters and dictionary file\n>\n> **设置的密码强度**\n>\n> Please enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 0\n> Please set the password for root here.\n>\n> New password:\n>\n> Re-enter new password:\n>\n> Estimated strength of the password: 50\n> Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : y\n> By default, a MySQL installation has an anonymous user,\n> allowing anyone to log into MySQL without having to have\n> a user account created for them. This is intended only for\n> testing, and to make the installation go a bit smoother.\n> You should remove them before moving into a production\n> environment.\n>\n> **删除匿名用户**\n>\n> Remove anonymous users? (Press y|Y for Yes, any other key for No) : y\n> Success.\n>\n> Normally, root should only be allowed to connect from\n> ‘localhost’. This ensures that someone cannot guess at\n> the root password from the network.\n>\n> **禁止使用root用户远程登录MySQL**\n>\n> Disallow root login remotely? (Press y|Y for Yes, any other key for No) : y\n> Success.\n>\n> By default, MySQL comes with a database named ‘test’ that\n> anyone can access. This is also intended only for testing,\n> and should be removed before moving into a production\n> environment.\n>\n> **删除test库以及用户对test库的访问权限**\n>\n> Remove test database and access to it? (Press y|Y for Yes, any other key for No) : y\n>\n> - Dropping test database…\n>   Success.\n> - Removing privileges on test database…\n>   Success.\n>\n> Reloading the privilege tables will ensure that all changes\n> made so far will take effect immediately.\n>\n> **重新加载授权表**\n>\n> Reload privilege tables now? (Press y|Y for Yes, any other key for No) : y\n> Success.\n>\n> All done!\n\n#### 步骤四：安装php\n\n```\n# 下载php组件\n yum install -y php php-cli php-fpm php-common php-mysqlnd php-gd php-mbstring\n# 查看php版本\n php -v\n```\n\n修改nginx配置文件以启用php\n\n```\n# 备份Nginx配置文件\ncp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.bak\n# 在Nginx配置文件中修改内容，在server部分添加内容\nvi /etc/nginx/nginx.conf\n**************************nginx.conf**************************\n    server {\n        listen       80;\n        listen       [::]:80;\n        server_name  _;\n        root         /data; #网站根目录\n\n        # Load configuration files for the default server block.\n        include /etc/nginx/default.d/*.conf;\n        location / { #添加\n            index index.php index.html index.htm;\n        }\n        location ~ .php$ {\n            root /data; #网站根目录\n            fastcgi_pass 127.0.0.1:9000; #Nginx通过本机的9000端口将PHP请求转发给PHP-FPM进行处理\n            fastcgi_index index.php;\n            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\n            include fastcgi_params; #Nginx调用fastcgi接口处理PHP请求\n        }\n        error_page 404 /404.html;\n        location = /404.html {\n        }\n\n        error_page 500 502 503 504 /50x.html;\n        location = /50x.html {\n        }\n    }\n\n**************************nginx.conf**************************\n\n# 启动nginx，并设置开机自启\nsystemctl start nginx\nsystemctl enable nginx\n```\n\n配置并验证php\n\n```\n# 保证 nginx 进程的管理用户和 PHP 服务进程的管理用户保持一致\nvi /etc/php-fpm.d/www.conf\nuser = nginx\ngroup = nginx\n# 重启nginx\nsystemctl restart nginx\n\n# 创建网站根目录\nmkdir /data\n# 编写测试php界面\necho \"<?php echo phpinfo(); ?>\" >/data/index.php\n# 启动php-fpm，并设置开机自启\nsystemctl start php-fpm\nsystemctl enable php-fpm\n```\n\n#### 步骤五：访问测试\n\n在浏览器中输入`http://本机ip/index.php`进行访问，如下图所示，表示LNMP环境部署成功。\n\n![img](LNMP部署/1.png)\n\n### 方案二：脚本一键安装\n\n```\nyum install -y wget\n# 下载lnmp安装脚本\nwget https://soft.lnmp.com/lnmp/lnmp2.1beta.tar.gz\n# 解压\ntar -vzxf lnmp2.1beta.tar.gz\ncd lnmp2.1\n# 执行脚本\n./install.sh\n# 按引导进行安装即可\n```\n\n在浏览器`http://本机ip/`访问，出现以下界面，即为部署成功。\n\n![img](LNMP部署/2.png)\n\n打开lnmp2.1/conf/nginx.conf文件，查看其网站根目录为/home/wwwroot/default，\n\n在其网站根目录/home/wwwroot/default中# 编写测试php界面，在浏览器中输入`http://本机ip/index.php`进行访问，结果如方案一中的《步骤五》中的图所示，即为成功。\n\n```\necho \"<?php echo phpinfo(); ?>\" >/data/index.php\n```","tags":["Linux","Mysql","PHP","Nginx"],"categories":["Linux运维","Web服务架构部署"]},{"title":"实现Nextcloud私有云盘","url":"/yyg/f7b8055d/","content":"## 任务目标\n\n1. 完成Nextcloud私有云盘的搭建\n\n## 部署指南\n\n### 基础环境准备\n\n配置防火墙策略\n\n```shell\nfirewall-cmd --zone=public --add-service=http --permanent\nfirewall-cmd --reload\n```\n\n配置SELinux策略\n\n设置SELinux为permissive或disabled。\n\n```shell\n#修改\nsetenforce 0\nsudo sed -i 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config\n#或\nsetenforce 0\nsudo sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config\n```\n\n### 安装Apache\n\n```shell\nyum -y install httpd\n```\n\n### 安装PHP\n\n```shell\nyum -y install php\n#安装php模块依赖\nyum -y install php-zip php-dom php-xml php-mbstring php-pdo php-gd\n```\n\n### 添加Nextcloud站点配置文件\n\n```shell\nmkdir /var/www/html/nextcloud/\nchmod 777 /var/www/html/nextcloud/\n#新建文件\nvi /etc/httpd/conf.d/nextcloud.conf\n<VirtualHost *:80>\n  DocumentRoot /var/www/html/nextcloud/\n  ServerName  your.server.com\n\n  <Directory /var/www/html/nextcloud/>\n    Require all granted\n    AllowOverride All\n    Options FollowSymLinks MultiViews\n\n    <IfModule mod_dav.c>\n      Dav off\n    </IfModule>\n\n  </Directory>\n</VirtualHost>\n```\n\n### 下载Nextcloud安装文件\n\n```shell\nwget -P /var/www/html/nextcloud/ https://download.nextcloud.com/server/installer/setup-nextcloud.php\n```\n\n### 启动服务\n\n```shell\nsystemctl start httpd\nsystemctl enable httpd\n```\n\n### 通过浏览器访问\n\n在浏览器中输入`http://172.20.1.51/setup-nextcloud.php`，按引导进行设置。如下图所示。\n\n![img](实现Nextcloud私有云盘/2.jpg)\n\n设置用户名和密码，安装数据库如下图所示。\n\n![img](实现Nextcloud私有云盘/4.png)\n\n成功进入内容界面，部署完成，如下图所示。\n\n![img](实现Nextcloud私有云盘/3.png)","tags":["Linux","Nextcloud","私有云盘"],"categories":["Linux运维"]},{"title":"Docker部署Matomo","url":"/yyg/a52e3ab6/","content":"## 基础环境\n\n系统环境：\n\n操作系统：CentOS Stream 9\n\n软件环境：\n\nDocker、Docker Compose\n\n## 安装Docker\n\n（1）安装 yum-utils 软件包（提供 yum-config-manager 实用程序）并设置存储库。\n\n```\n# 安装 yum-utils 软件包\nyum install -y yum-utils\n# 设置docker-ce存储库\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n```\n\n（2）安装最新版docker和docker compose。\n\n```\n\nyum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n\n（3）修改Docker的数据目录为“/data/dockerData”，并重启docker服务使配置生效。\n\n```\n# 创建Docker守护进程配置文件\n# 编辑配置文件/etc/docker/daemon.json，配置docker数据目录\nvi /etc/docker/daemon.json\n**************************daemon.json**************************\n{\n  \"data-root\": \"/data/dockerData\"\n}\n**************************daemon.json**************************\n\n# 重启docker服务，使配置生效\n[root@Book-ELK-VM-113 data]# systemctl restart docker\n```\n\n（4）启动Docker服务，并设置为开机自启动。\n\n```\n# 启动Docker服务\nsystemctl start docker\n# 设置Docker为开机自启动\nsystemctl enable docker\n# 查看Docker服务状态\nsystemctl status docker\n```\n\n## 部署并配置Matomo\n\n### 部署Matomo\n\n（1）创建script目录存放yml文件，编排部署Matomo。\n\n```\nmkdir /data/script\nvi /data/script/matomo.yml\n**************************matomo.yml**************************\nservices:\n  matomo-sql:\n    # 使用 MySQL 数据库镜像\n    image: mysql:latest\n    # 容器名称\n    container_name: matomo-sql\n    # 容器重启策略：如果容器停止，总是重启\n    restart: always\n    # 配置环境变量\n    environment:\n      # 设置数据库名\n      - MYSQL_DATABASE=matomo\n      # 设置数据库用户名\n      - MYSQL_USER=matomo\n      # 设置数据库用户密码\n      - MYSQL_PASSWORD=matomo\n      # 设置数据库 root 用户密码\n      - MYSQL_ROOT_PASSWORD=matomo\n      # 设置时区\n      - TZ=Asia/Shanghai\n    # 暴露的端口号映射，将 MySQL 的 3306 端口映射到宿主机\n    ports:\n      - \"3306:3306\"\n    # 将宿主机的目录挂载到容器的 MySQL 数据目录\n    volumes:\n      - /data/database/matomo-sql:/var/lib/mysql\n    # 使用 root 用户权限运行容器\n    user: \"0\"\n    # 网络配置\n    networks:\n      # 使用自定义网络 'net'\n      net:\n        # 为容器分配固定的 IPv4 地址\n        ipv4_address: 172.18.0.10\n\n  matomo-app:\n    # 使用 Matomo 应用镜像\n    image: matomo:latest\n    # 容器名称\n    container_name: matomo-app\n    # 容器重启策略：如果容器停止，总是重启\n    restart: always\n    # 链接到 MySQL 数据库服务\n    links:\n      - matomo-sql\n    # 将宿主机的目录挂载到 Matomo 应用目录\n    volumes:\n      - /data/matomo:/var/www/html\n    # 设置环境变量\n    environment:\n      # 设置时区\n      - TZ=Asia/Shanghai\n      # 指定数据库主机名为 'matomo-sql'\n      - MATOMO_DATABASE_HOST=matomo-sql\n      # 设置 PHP 内存限制\n      - PHP_MEMORY_LIMIT=2048M\n      # 指定数据库适配器为 MySQL\n      - MATOMO_DATABASE_ADAPTER=mysql\n      # 设置数据库表名前缀\n      - MATOMO_DATABASE_TABLES_PREFIX=matomo_\n      # 指定数据库用户名\n      - MATOMO_DATABASE_USERNAME=matomo\n      # 指定数据库用户密码\n      - MATOMO_DATABASE_PASSWORD=matomo\n      # 指定数据库名称\n      - MATOMO_DATABASE_DBNAME=matomo\n    # 使用 root 用户权限运行容器\n    user: \"0\"\n    # 容器端口映射，将 Matomo 的 80 端口映射到宿主机\n    ports:\n      - \"80:80\"\n    # 网络配置\n    networks:\n      # 使用自定义网络 'net'\n      net:\n        # 为容器分配固定的 IPv4 地址\n        ipv4_address: 172.18.0.11\n\n# 自定义网络配置\nnetworks:\n  # 定义自定义网络 'net'\n  net:\n    # 使用桥接网络驱动\n    driver: bridge\n    # IP 地址管理配置\n    ipam:\n      config:\n        # 为自定义网络设置子网\n        - subnet: 172.18.0.0/24\n**************************matomo.yml**************************\n```\n\n（2）执行Docker Compose创建容器，并查看当前正在运行的 Docker 容器。\n\n```\ndocker compose -f /data/script/matomo.yml up -d\ndocker ps\n```\n\n### 配置Matomo\n\n（1）在浏览器中访问`http://本机ip`，按引导进行安装。\n\n![img](Docker部署Matomo/1.png)\n\n（2）在向导”3.数据库设置“界面中查看并修改数据库配置信息，此处的“数据库服务器”名称填写的是数据库容器名称。\n\n![img](Docker部署Matomo/2.png)\n\n（3）在向导”5.超级用户“界面中，创建超级用户的用户名、密码及电子邮箱。\n\n![img](Docker部署Matomo/3.png)\n\n（4）在向导”6.设置网站“界面中，请设置一个想用Matomo追踪、分析的站点。\n\n![img](Docker部署Matomo/4.png)\n\n（5）在向导”7.JavaScript 跟踪代码“界面中，复制代码片段,添加到网站的每个页面，追踪网站的流量。\n\n![img](Docker部署Matomo/5.png)\n\n（6）安装完成后登录matomo，在“仪表面板”界面中，选择【INSTALL WITH JAVASRCIPT代码】，查看追踪代码。\n\n![img](Docker部署Matomo/6.png)\n\n（7）选择【TEST INSTALLATION】，测试是否能够监控网站流量，当出现”The tracking code is installed successfully! This screen will disappear as soon as some data is tracked for your website.“的绿色字样时，网站流量可正常追踪。\n\n![img](Docker部署Matomo/7.png)\n\n（8）选择菜单栏中的”所有网站”，查看所有监控的网站报表及总访问量、访问者、浏览量、活动者、收入等元素。\n\n![img](Docker部署Matomo/8.png)\n\n### 可选配置\n\n（1）选择“设置”图标，选择“个人”中“设置”，将默认加载的报表日期改为“今天”。\n\n（2）选择“设置”图标，选择“隐私设置”中“匿名化数据”，取消勾选“隐藏访客的IP地址”。\n\n（3）选择“设置”图标，选择“网站”中“设置”，新网站默认时区选择“中国-上海”，新网站的默认货币选择“人民币 (¥)”。\n\n（4）选择“设置”图标，选择“系统”中“地理位置”，将位置信息提供商修改为“DBIP / GeolP 2 (Php)”。\n\n（5）选择“设置”图标，选择“平台”中“商城”，安装IP2Location、LogViewer、MarketingCampaignsReporting、Profile Gravatar插件，安装LogViewer插件后，需要在本地映射目录/data/matomo/config/中,对global.ini.php中的[log]模块进行修改。修改完成后，重启app容器。\n\n```\n[log]\nlog_writers[] = file\nlog_level = INFO\n\n# 重启app容器\ndocker restart matomo-app\n```\n\n（6）启用插件TagManager 、DBStats。","tags":["docker","Matomo"],"categories":["Linux运维","容器化","docker","网络分析"]},{"title":"DHCP服务器搭建","url":"/yyg/97ec93b/","content":"## 服务器端\n\n（1）通过在线方式安装dhcp-server\n\n```\n[root@dncp-103 ~]# yum -y install dhcp-server\n```\n\n（2）修改dhcpd服务配置文件，完成DHCP服务器的搭建\n\n```\n[root@dncp-103 ~]# vi /etc/dhcp/dhcpd.conf\n-------------------dhcpd.conf-------------------\n# DHCP Server Configuration file.\n#   see /usr/share/doc/dhcp-server/dhcpd.conf.example\n#   see dhcpd.conf(5) man page\n#\n# 设置域名为 \"elk.com\"\noption domain-name     \"elk.com\";\n\n# 设置域名服务器为 10.10.2.103\noption domain-name-servers    10.10.2.103;\n\n# Declare DHCP Server\n# 声明此 DHCP 服务器\nauthoritative;  \n\n# 设置默认租约时间为 600 秒\ndefault-lease-time 600;\n# 设置日志设施为 local4\nlog-facility local4;\n# 设置最大租约时间为 7200 秒\nmax-lease-time 7200;\n\n# Set Network address, subnet mask and gateway\nsubnet 10.10.2.0 netmask 255.255.255.0 {\n}\n\n# 设置 IP 地址范围为 10.10.2.202 到 10.10.2.205\nsubnet 10.10.2.0 netmask 255.255.255.0 {\n  range 10.10.2.202 10.10.2.205;\n  # 设置网关为 10.10.2.1\n  option routers 10.10.2.1;\n  # 设置子网掩码为 255.255.255.0\n  option subnet-mask 255.255.255.0;\n  # 设置广播地址为 10.10.2.255\n  option broadcast-address 10.10.2.255;\n  # 设置默认租约时间为 600 秒\n  default-lease-time 600;\n  # 设置最大租约时间为 7200 秒\n  max-lease-time 7200;\n  # Add other DHCP options here\n}\n-------------------dhcpd.conf-------------------\n```\n\n（3）在rsyslog.conf中配置dhcp日志路径\n\n```\n[root@dncp-103 ~]# vi /etc/rsyslog.conf\n-------------------rsyslog.conf-------------------\n#为排版方便此处省略部分提示信息\n# Don't log private authentication messages!\n#将 local4 设施的日志消息排除在 /var/log/messages 文件之外\n*.info;mail.none;authpriv.none;cron.none;local4.none           /var/log/messages\n# 来自 local4 设施的所有级别的日志消息记录到 /var/log/dhcp/dhcp.log 文件中\nlocal4.* /var/log/dhcp/dhcp.log\n#为排版方便此处省略部分提示信息\n-------------------rsyslog.conf-------------------\n```\n\n（4）启动dhcpd服务并重启rsyslog服务，使得配置生效\n\n```\n# 启动dhcpd服务\n[root@dncp-103 ~]# systemctl start dhcpd\n# 重启rsyslog服务\n[root@dncp-103 ~]# systemctl restart rsyslog\n```\n\n（5）查看dhcpd服务状态\n\n```\n[root@dncp-103 ~]# systemctl status dhcpd\n```\n\n（6）配置dhcpd服务为开机自启动\n\n```\n[root@dncp-103 ~]# systemctl enable dhcpd\n```\n\n（7）配置防火墙，重启服务，使得配置生效\n\n```\n# 配置dhcp服务访问策略，使其能够正常访问\n[root@dncp-103 ~]# firewall-cmd --add-service=dhcp --permanent\nsuccess\n# 重新加载防火墙策略\n[root@dncp-103 ~]# firewall-cmd --reload\nsuccess\n```\n\n## 客户机端\n\n（1）查看客户机原本ip\n\n```\n[root@dncp-113 ~]# ip a\n```\n\n（2）修改客户机ip为动态，验证DHCP服务器是否能动态的分配ip\n\n```\n# 修改网卡配置文件\n[root@dncp-113 ~]# vi /etc/NetworkManager/system-connections/ens32.nmconnection\n-------------------ens32.nmconnection-------------------\n[connection]\nid=ens32\nuuid=2bb0f492-19e2-37c0-b8fa-1781d3122a83\ntype=ethernet\nautoconnect-priority=-999\ninterface-name=ens32\ntimestamp=1713841073\n\n[ethernet]\n\n[ipv4]\n# 注释设置静态IP信息\n#address1=10.10.2.113/24,10.10.2.1\ndns=10.10.2.103;\n# 通过 DHCP动态获取 IP 地址\nmethod=auto\n\n[ipv6]\naddr-gen-mode=eui64\nmethod=auto\n\n[proxy]\n-------------------ens32.nmconnection-------------------\n```\n\n（3）重新加载 NetworkManager 的连接配置，查看分配的动态ip\n\n```\n# 重新加载 NetworkManager 的连接配置\n[root@dncp-113 ~]# nmcli c reload\n# 连接网络接口 ens32\n[root@dncp-113 ~]# nmcli d connect ens32\n设备 \"ens32\" 成功以 \"2bb0f492-19e2-37c0-b8fa-1781d3122a83\" 激活。\n# 查看分配的动态ip\n[root@dncp-113 ~]# ip a\n```","tags":["Linux运维","DHCP"],"categories":["Linux运维","网络服务搭建"]},{"title":"DNS服务器搭建","url":"/yyg/37824039/","content":"## 服务器端\n\n（1）通过在线方式安装bind9\n\n```\nyum install -y bind bind-utils\n```\n\n（2）启动服务\n\n```\nsystemctl start named\n```\n\n（3）查看服务状态\n\n```\nsystemctl status named\n```\n\n（4）配置服务为开机自启动\n\n```\nsystemctl enable named\n```\n\n（5）修改named服务配置文件，配置DNS日志路径\n\n```shell\nvi /etc/named.conf\n-------------------named.conf-------------------\noptions {\n        // 接受来自任何网络接口的DNS请求\n        listen-on port 53 { any; };\n        listen-on-v6 port 53 { ::1; };\n        directory       \"/var/named\";\n        dump-file       \"/var/named/data/cache_dump.db\";\n        statistics-file \"/var/named/data/named_stats.txt\";\n        memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n        secroots-file   \"/var/named/data/named.secroots\";\n        recursing-file  \"/var/named/data/named.recursing\";\n        // 允许访问的ip范围\n        allow-query     { 10.10.0.0/16; };\n\n# 为排版方便此处省略部分提示信息\n};\nlogging {\n    //    channel default_debug {\n    //            file \"data/named.run\";\n    //            severity dynamic;\n    //    };\n        \n        channel named_log {\n                // 指定日志文件的路径,设置日志文件的版本数量限制为5个，每个文件的最大大小为50m\n                file \"/var/log/dns/named.log\" versions 5 size 50m;\n                // 日志记录的时间格式采用ISO 8601标准\n                print-time iso8601;\n                // 打印日志条目的类别\n                print-category yes;\n               // 打印日志条目的严重性\n                print-severity yes;\n                severity info;\n        };\n        // 定义多个日志记录类别\n        category default   { named_log; };\n        category general   { named_log; };\n        category config   { named_log; };\n        category client   { named_log; };\n        category network   { named_log; };\n        category notify   { named_log; };\n        category queries   { named_log; };\n        category update   { named_log; };\n        category query-errors  { named_log; };\n        category resolver   { named_log; };\n        category xfer-in   { named_log; };\n        category xfer-out   { named_log; };\n        category dnssec   { named_log; };\n};\n# 为排版方便此处省略部分提示信息\n-------------------named.conf-------------------\n```\n\n（6）配置DNS区域，完成DNS服务器的搭建\n\n```\nvi /etc/named.rfc1912.zones\n-------------------named.rfc1912.zones-------------------\n// If private ranges should be forwarded, add\n// disable-empty-zone \".\"; into options\n//\n\nzone \"localhost.localdomain\" IN {\n        type master;\n        file \"named.localhost\";\n        allow-update { none; };\n};\n\nzone \"localhost\" IN {\n        type master;\n        file \"named.localhost\";\n        allow-update { none; };\n};\n\nzone \"1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa\" IN {\n        type master;\n        file \"named.loopback\";\n        allow-update { none; };\n};\n\nzone \"1.0.0.127.in-addr.arpa\" IN {\n        type master;\n        file \"named.loopback\";\n        allow-update { none; };\n};\n\nzone \"0.in-addr.arpa\" IN {\n        type master;\n        file \"named.empty\";\n        allow-update { none; };\n};\n// 自定义域名区域和DNS记录集合\nzone \"elk.com\" IN {\n        type master;\n        file \"/var/named/elk.com.zone\";\n        allow-update { none; };\n};\n-----------------------------------------------\n```\n\n（7）创建”elk.com”的区域数据文件，并定义DNS服务器的域名为“[dns.elk.com](https://dns.elk.com/)”和客户机的域名为“[test.elk.com](https://test.elk.com/)”。\n\n```\n# 创建\"elk.com\"的区域数据文件\nvi /var/named/elk.com.zone\n# 定义DNS服务器和客户机的域名\n-------------------named.rfc1912.zones-------------------\n$TTL 1D\n@       IN SOA  dns.elk.com. admin.elk.com. (\n                                        0       ; serial\n                                        1D      ; refresh\n                                        1H      ; retry\n                                        1W      ; expire\n                                        3H )    ; minimum\n@   IN  NS      dns.elk.com.\ndns     IN      A   10.10.2.103\ntest    IN      A   10.10.2.113\n-------------------------------------------------------\n```\n\n（8）设置日志服务权限，配置防火墙，重启服务使得配置生效\n\n```\n# 创建存放日志的目录并赋权\nmkdir /var/log/dns\nchmod -R 777 /var/log/dns/\n# 配置dns服务访问策略，使其能够正常访问\nfirewall-cmd --permanent --add-service=dns\n# 重新加载防火墙策略\nfirewall-cmd --reload\n# 重启named服务，让配置生效\n systemctl restart named\n```\n\n（9）修改DNS为“10.10.2.103”，并重启网络生效。\n\n```\n# 使用nmtui管理网络连接\nnmtui\n```\n\n选择“编辑连接”，回车，选择网卡“ens32”，在“编辑连接”界面中将DNS服务器修改为“10.10.2.103”，保存退出。如图6-1-1、6-1-2所示。\n\n[![img](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)](https://marsperl.github.io/yyg/37824039/image-20240422191810676.png)[![img](data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7)](https://marsperl.github.io/yyg/37824039/2.png)\n\n保存退出后，重启网络让DNS服务器配置生效。\n\n```\nsystemctl restart NetworkManager\n```\n\n（10）验证DNS服务器是否配置成功。\n\n```\nping www.baidu.com\nping dns.elk.com\nping test.elk.com\n```\n\n## 客户机端\n\n修改DNS配置为“10.10.2.103”，具体步骤参照《步骤1：虚拟机规划》的“（9）”。\n\n重启网络网络后，验证DNS配置是否生效。\n\n```\nping www.baidu.com\nping dns.elk.com\nping test.elk.com\n```","tags":["Linux运维","DNS"],"categories":["Linux运维","网络服务搭建"]},{"title":"NTP服务器搭建","url":"/yyg/29b27760/","content":"## 服务器端\n\n（1）通过在线方式安装chrony\n\n```\nyum install -y chrony\n```\n\n（2）启动chrony服务\n\n```\nsystemctl start chronyd\n```\n\n（3）查看chrony服务状态\n\n```\nsystemctl status chronyd\n```\n\n（4）配置chrony服务为开机自启动\n\n```\nsystemctl enable chronyd\n```\n\n（5）修改chronyd服务配置文件，完成NTP服务器的搭建\n\n```shell\nvi /etc/chrony.conf\n-------------------chrony.conf-------------------\npool 2.centos.pool.ntp.org iburst\nsourcedir /run/chrony-dhcp\n#配置的多个NTP服务器\nserver ftp.aliyun.com iburst\nserver time1.aliyun.com iburst\nserver time2.aliyun.com iburst\nserver time3.aliyun.com iburst\nserver time4.aliyun.com iburst\nserver time5.aliyun.com iburst\nserver 10.10.2.103 iburst\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\n#允许访问的网段\nallow 10.10.0.0/16\nkeyfile /etc/chrony.keys\nntsdumpdir /var/lib/chrony\nleapsectz right/UTC\n#日志存储路径\nlogdir /var/log/chrony\n#日志测量统计指标\nlog measurements statistics tracking\n-------------------chrony.conf-------------------\n```\n\n（6）配置防火墙，重启服务，使得配置生效\n\n```\n# 配置ntp服务访问策略，使其能够正常访问\nfirewall-cmd --permanent --add-service=ntp\n# 重新加载防火墙策略\nfirewall-cmd --reload\n# 重启chronyd服务\nsystemctl restart chronyd\n```\n\n（7）查看连接NTP服务的客户机\n\n```\nchronyc clients\n```\n\n## 客户机端\n\n（1）通过在线方式安装chrony\n\n```\nyum install -y chrony\n```\n\n（2）配置访问chrony服务器\n\n```\n# 修改chrony的配置文件\nvi /etc/chrony.conf\n-------------------chrony.conf-------------------\n# Use public servers from the pool.ntp.org project.\n# Please consider joining the pool (https://www.pool.ntp.org/join.html).\npool 2.centos.pool.ntp.org iburst\n\n# Use NTP servers from DHCP.\nsourcedir /run/chrony-dhcp\n\n# Record the rate at which the system clock gains/losses time.\ndriftfile /var/lib/chrony/drift\n# 配置ntp服务器地址\nserver 10.10.2.103 iburst\n#为排版方便此处省略部分提示信息\n-----------------------------------------------\n```\n\n（3）启动chronyd服务，连接NTP服务器\n\n```\n# 启动chrony服务\nsystemctl start chronyd\n# 查看chrony服务状态\nsystemctl status chronyd\n# 配置chrony服务为开机自启动\nsystemctl enable chronyd\n```\n\n（4）配置防火墙，使得配置生效\n\n```\n# 配置ntp服务访问策略，使其能够正常访问\nfirewall-cmd --permanent --add-service=ntp\n# 重新加载防火墙策略\nfirewall-cmd --reload\n```\n\n（5）验证NTP服务器的连接情况\n\n```\n# 查看源服务器状态\n chronyc sourcestats\n# 查看详细同步状态\nchronyc sources -v\n# 追踪同步情况\nchronyc tracking\n# 开启网络时间同步\ntimedatectl set-ntp true\n# 查看时间同步状态\ntimedatectl status\n```\n","tags":["Linux运维","NTP"],"categories":["Linux运维","网络服务搭建"]},{"title":"安装Winlogbeat日志采集器","url":"/yyg/f547e2a9/","content":"1. Winlogbeat安装程序可通过其官网（https://www.elastic.co/cn/beats/winlogbeat）下载。\n2. 双击启动安装程序，进入安装向导后勾选“I accept the terms in the License Agreement”，单击【Install】，如图所示。\n\n![img](安装Winlogbeat日志采集器/4.png)\n\n1. 按安装向导安装完成后，单击【Finish】，完成安装并打开安装目录。如图所示。\n\n![img](安装Winlogbeat日志采集器/3.png)\n\n![img](安装Winlogbeat日志采集器/2.png)\n\n1. 以管理员身份打开 PowerShell 提示符（右键单击 PowerShell 图标，然后选择以管理员身份运行），允许在本地主机上创建winlogbeat服务，创建成功后允许winlogbeat服务并验证。如图所示。\n\n   ```\n   PS C:\\Windows\\system32> cd \"C:\\Program Files\\Elastic\\Beats\\8.13.2\\winlogbeat\"\n   # 允许在本地计算机上编写的未签名脚本自由运行\n   PS C:\\Program Files\\Elastic\\Beats\\8.13.2\\winlogbeat> set-executionpolicy remotesigned\n   \n   执行策略更改\n   执行策略可帮助你防止执行不信任的脚本。更改执行策略可能会产生安全风险，如 https:/go.microsoft.com/fwlink/?LinkID=135170\n   中的 about_Execution_Policies 帮助主题所述。是否要更改执行策略?\n   [Y] 是(Y)  [A] 全是(A)  [N] 否(N)  [L] 全否(L)  [S] 暂停(S)  [?] 帮助 (默认值为“N”): y\n   # 允许脚本在系统上运行\n   PS C:\\Program Files\\Elastic\\Beats\\8.13.2\\winlogbeat> set-executionpolicy Bypass\n   \n   执行策略更改\n   执行策略可帮助你防止执行不信任的脚本。更改执行策略可能会产生安全风险，如 https:/go.microsoft.com/fwlink/?LinkID=135170\n   中的 about_Execution_Policies 帮助主题所述。是否要更改执行策略?\n   [Y] 是(Y)  [A] 全是(A)  [N] 否(N)  [L] 全否(L)  [S] 暂停(S)  [?] 帮助 (默认值为“N”): y\n   # 创建winlogbeat服务\n   PS C:\\Program Files\\Elastic\\Beats\\8.13.2\\winlogbeat> .\\install-service-winlogbeat.ps1\n   [SC] DeleteService 成功\n   \n   Status   Name               DisplayName\n   ------   ----               -----------\n   Stopped  winlogbeat         winlogbeat\n   \n   # 启动winlogbeat服务\n   PS C:\\Program Files\\Elastic\\Beats\\8.13.2\\winlogbeat> Start-Service winlogbeat\n   ```\n\n![img](安装Winlogbeat日志采集器/1.png)","tags":["日志分析","ELK","日志采集器","Winlogbeat"],"categories":["日志分析","ELK"]},{"title":"docker部署zabbix6.4指南","url":"/yyg/6d91d852/","content":"## 任务目标\n\n1. 在主机一上完成docker部署zabbix6.4\n2. 在主机二上安装zabbix-agent2\n3. 使用主机一上的zabbix容器监控主机二的容器等业务\n\n## 任务平台\n\n1. 物理设备--\n2. 操作系统：CentOS 7\n\n## 部署指南\n\n### 任务 一\n\n1. 安装docker（最新版）\n\n```Shell\n#下载并安装docker文件及依赖\nyum install -y yum-utils\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install -y docker-ce docker-ce-cli containerd.io\n#启动docker并设置开机自启\nsystemctl start docker  \nsystemctl enable docker\n#查看版本\ndocker -v\n```\n\n1. 配置防火墙所需的开放端口\n\n```Shell\n# 放行nginx端口\nfirewall-cmd --zone=public --add-port=80/tcp --permanent\n# 放行zabbix agent端口\nfirewall-cmd --zone=public --add-port=10050/tcp --permanent\n# 放行zabbix server端口\nfirewall-cmd --zone=public --add-port=10051/tcp --permanent\n# 放行zabbix java gateway端口\nfirewall-cmd --zone=public --add-port=10052/tcp --permanent\n# 放行mysql端口\nfirewall-cmd --zone=public --add-port=3306/tcp --permanent\n# 放行snmptraps端口\nfirewall-cmd --zone=public --add-port=162/udp --permanent\n# 重载防火墙\nfirewall-cmd --reload\n#查看80端口是否开放\nfirewall-cmd --query-port=80/tcp\n#查看所有放行的端口\nfirewall-cmd --zone=public --list-ports\n```\n\n1. 拉取zabbix所需镜像文件\n\n```Shell\n#拉取安装zabbix需要的docker镜像\ndocker pull mysql:8.0\ndocker pull zabbix/zabbix-java-gateway:alpine-6.4-latest\ndocker pull zabbix/zabbix-server-mysql:alpine-6.4-latest\ndocker pull zabbix/zabbix-web-nginx-mysql:alpine-6.4-latest\n```\n\n1. 创建专用于Zabbix组件容器网络与所需容器\n\n#### 方案一：\n\n```Shell\n#创建专用于Zabbix组件容器的网络\ndocker network create --subnet 172.20.0.0/16 --ip-range 172.20.240.0/20 zabbix-net\n\n#启动空的MySQ服务器实例\ndocker run --name mysql-server -t \\\n      -e MYSQL_DATABASE=\"zabbix\" \\\n      -e MYSQL_USER=\"zabbix\" \\\n      -e MYSQL_PASSWORD=\"123456\" \\\n      -e MYSQL_ROOT_PASSWORD=\"123456\" \\\n      --network=zabbix-net \\\n      --restart=always \\\n      -d mysql:8.0 \\\n      --character-set-server=UTF8MB4 --collation-server=UTF8MB4_bin \\\n      --authentication_policy=mysql_native_password\n\n#启动Zabbix Java网关实例      \ndocker run --name zabbix-java-gateway -t \\\n      --network=zabbix-net \\\n      --restart unless-stopped \\\n      -d zabbix/zabbix-java-gateway:alpine-6.4-latest\n\n#启动Zabbix服务器实例并将实例与创建的MySQL服务器实例链接      \ndocker run --name zabbix-server-mysql -t \\\n      -e DB_SERVER_HOST=\"mysql-server\" \\\n      -e MYSQL_DATABASE=\"zabbix\" \\\n      -e MYSQL_USER=\"zabbix\" \\\n      -e MYSQL_PASSWORD=\"123456\" \\\n      -e MYSQL_ROOT_PASSWORD=\"123456\" \\\n      -e ZBX_JAVAGATEWAY=\"zabbix-java-gateway\" \\\n      --network=zabbix-net \\\n      -p 10051:10051 \\\n      --restart unless-stopped \\\n      -d zabbix/zabbix-server-mysql:alpine-6.4-latest\n\n#启动Zabbix Web界面，并将实例与创建的MySQL服务器和Zabbix服务器实例链接\ndocker run --name zabbix-web-nginx-mysql -t \\\n      -e ZBX_SERVER_HOST=\"zabbix-server-mysql\" \\\n      -e DB_SERVER_HOST=\"mysql-server\" \\\n      -e MYSQL_DATABASE=\"zabbix\" \\\n      -e MYSQL_USER=\"zabbix\" \\\n      -e MYSQL_PASSWORD=\"123456\" \\\n      -e MYSQL_ROOT_PASSWORD=\"123456\" \\\n      --network=zabbix-net \\\n      -p 80:8080 \\\n      --restart unless-stopped \\\n      -d zabbix/zabbix-web-nginx-mysql:alpine-6.4-latest\n```\n\n#### 方案二：（推荐使用）\n\n```Shell\n#启动空的MySQ服务器实例\ndocker run --name mysql-server -t \\\n      -e MYSQL_DATABASE=\"zabbix\" \\\n      -e MYSQL_USER=\"zabbix\" \\\n      -e MYSQL_PASSWORD=\"123456\" \\\n      -e MYSQL_ROOT_PASSWORD=\"123456\" \\\n      --restart=always \\\n      -d mysql:8.0 \\\n      --character-set-server=UTF8MB4 --collation-server=UTF8MB4_bin \\\n      --authentication_policy=mysql_native_password\n\n#启动Zabbix Java网关实例      \ndocker run --name zabbix-java-gateway -t \\\n      --restart unless-stopped \\\n      -d zabbix/zabbix-java-gateway:alpine-6.4-latest\n\n#启动Zabbix服务器实例并将实例与创建的MySQL服务器实例链接      \ndocker run --name zabbix-server-mysql -t \\\n      -e DB_SERVER_HOST=\"mysql-server\" \\\n      -e MYSQL_DATABASE=\"zabbix\" \\\n      -e MYSQL_USER=\"zabbix\" \\\n      -e MYSQL_PASSWORD=\"123456\" \\\n      -e MYSQL_ROOT_PASSWORD=\"123456\" \\\n      -e ZBX_JAVAGATEWAY=\"zabbix-java-gateway\" \\\n      --link mysql-server:mysql \\\n      --link zabbix-java-gateway:zabbix-java-gateway \\\n      -p 10051:10051 \\\n      --restart unless-stopped \\\n      -d zabbix/zabbix-server-mysql:alpine-6.4-latest\n\n#启动Zabbix Web界面，并将实例与创建的MySQL服务器和Zabbix服务器实例链接\ndocker run --name zabbix-web-nginx-mysql -t \\\n      -e ZBX_SERVER_HOST=\"zabbix-server-mysql\" \\\n      -e DB_SERVER_HOST=\"mysql-server\" \\\n      -e MYSQL_DATABASE=\"zabbix\" \\\n      -e MYSQL_USER=\"zabbix\" \\\n      -e MYSQL_PASSWORD=\"123456\" \\\n      -e MYSQL_ROOT_PASSWORD=\"123456\" \\\n      --link mysql-server:mysql \\\n      --link zabbix-server-mysql:zabbix-server \\\n      -p 80:8080 \\\n      --restart unless-stopped \\\n      -d zabbix/zabbix-web-nginx-mysql:alpine-6.4-latest\n```\n\n1. 在其中一台的主机的/etc/docker/daemon.json文件中，加入如下内容：\n\n```Shell\n#打开daemon.json文件\nvi /etc/docker/daemon.json\n\n#添加的内容\n{\n \"bip\": \"172.16.200.1/24\"\n}\n\n#重启docker服务\nsystemctl restart docker \n```\n\n### 任务二\n\n在主机二上安装zabbix-agent2\n\n在要监控的主机上安装zabbix agent2\n\n步骤1：Zabbix agent2下载\n\n{% link Zabbix agent2下载, https://www.zabbix.com/documentation/6.4/manual/installation/install_from_packages %}\n\n```Shell\nrpm -Uvh https://repo.zabbix.com/zabbix/6.4/rhel/7/x86_64/zabbix-release-6.4-1.el7.noarch.rpm\nyum clean all\n```\n\n步骤2：b. 下载Zabbix agent2\n\n```Shell\nyum install zabbix-agent2 zabbix-agent2-plugin-* -y\n```\n\n步骤3：启动Zabbix agent2\n\nStart Zabbix agent2 process and make it start at system boot.\n\n```Shell\nsystemctl restart zabbix-agent2\nsystemctl enable zabbix-agent2\n```\n\n运行成功界面如下\n\n![img](docker部署zabbix6.4指南/1711208128881-1.png)\n\n![img](docker部署zabbix6.4指南/1711208138487-4-1711208142767-7.png)\n\n### 安装报错总结\n\n1. \n\n![img](docker部署zabbix6.4指南/1711208157423-9.png)\n\n进入mysql容器，输入以下命令.\n\n```Shell\n#进入sql界面\nmysql -u root -p\n#输入该命令，解决问题\ncreate database zabbix character set utf8 collate utf8_bin;\nSET GLOBAL log_bin_trust_function_creators = 1;\n```\n\n1. 安装zabbix出现下图情况解决方法\n\n![img](docker部署zabbix6.4指南/1711208164924-12.png)\n\n{% link 故障处理,https://blog.csdn.net/ly4983/article/details/111029506 %}\n\n### 扩展-docker-compose部署zabbix\n\n```yaml\nservices:\n  zabbix-db-mysql:\n    image: mysql:9.0.1\n    container_name: zabbix-db-mysql\n    restart: always\n    environment:\n      - MYSQL_DATABASE=zabbix\n      - MYSQL_USER=zabbix\n      - MYSQL_PASSWORD=zabbix@123\n      - MYSQL_ROOT_PASSWORD=zabbix@123\n      - TZ=Asia/Shanghai\n    command: [\"--character_set_server=utf8mb4\", \"--collation_server=utf8mb4_bin\"]\n    volumes:\n      - /data/database/zabbix-mysql:/var/lib/mysql\n    user: \"0\"\n    ports:\n      - 3306:3306\n    networks:\n      zabbix:\n        ipv4_address: 172.20.20.10\n\n  zabbix-java-gateway:\n    image: zabbix/zabbix-java-gateway:latest\n    container_name: zabbix-java-gateway\n    restart: always\n    environment:\n      - TZ=Asia/Shanghai\n    user: \"0\"\n    networks:\n      zabbix:\n        ipv4_address: 172.20.20.11\n\n  zabbix-server-mysql:\n    image:  zabbix/zabbix-server-mysql:latest\n    container_name: zabbix-server-mysql\n    restart: always\n    environment:\n      - DB_SERVER_HOST=zabbix-db-mysql\n      - MYSQL_DATABASE=zabbix\n      - MYSQL_USER=zabbix\n      - MYSQL_PASSWORD=zabbix@123\n      - MYSQL_ROOT_PASSWORD=zabbix@123\n      - ZBX_JAVAGATEWAY=zabbix-java-gateway\n      - TZ=Asia/Shanghai\n    user: \"0\"\n    ports:\n      - 10051:10051\n    networks:\n      zabbix:\n        ipv4_address: 172.20.20.12\n\n  zabbix-web-nginx-mysql:\n    image:  zabbix/zabbix-web-nginx-mysql:latest\n    container_name: zabbix-web-nginx-mysql\n    restart: always\n    environment:\n      - ZBX_SERVER_HOST=zabbix-server-mysql\n      - DB_SERVER_HOST=zabbix-db-mysql\n      - MYSQL_DATABASE=zabbix\n      - MYSQL_USER=zabbix\n      - MYSQL_PASSWORD=zabbix@123\n      - MYSQL_ROOT_PASSWORD=zabbix@123\n      - ZBX_JAVAGATEWAY_ENABLE=true\n      - ZBX_JAVAGATEWAYPORT=10052\n      - PHP_TZ=Asia/Shanghai\n    user: \"0\"\n    ports:\n      - 80:8080\n    networks:\n      zabbix:\n        ipv4_address: 172.20.20.13\n\n  zabbix-agent:\n    image: zabbix/zabbix-agent:latest\n    container_name: zabbix-agent\n    restart: always\n    environment:\n      - TZ=Asia/Shanghai\n      - ZBX_SERVER_HOST=172.20.20.12\n      - ZBX_HOSTNAME=Zabbix server\n    user: \"0\"\n    ports:\n      - 10050:10050\n    networks:\n      zabbix:\n        ipv4_address: 172.20.20.14\n\nnetworks:\n zabbix:\n  driver: bridge\n  ipam:\n   config:\n    - subnet: 172.20.20.0/24\n```\n\n","tags":["docker","zabbix"],"categories":["Linux运维","容器化","docker","监控"]},{"title":"Kubernetes知识梳理","url":"/yyg/f5a7a3df/","content":"\n### Kubernetes集群YAML文件详解\n\n使用 kubectl create命令生成yaml文件\n\n```\nkubectl create deployment web --image=nginx -o yaml --dry-run\n```\n\n输出到一个文件中\n\n```\nkubectl create deployment web --image=nginx -o yaml --dry-run > my.yaml\n```\n\n使用kubectl get命令导出yaml文件\n\n```\nkubectl get deploy nginx -o=yaml --export > nginx.yaml\n```\n\n### 升级 Kubernetes 集群\n\nmaster节点\n\n```Shell\n#查看kubeadm发行版本\nyum list --showduplicates kubeadm --disableexcludes=kubernetes\n#下载kubeadm最新版\nyum install -y kubeadm-1.28.0 --disableexcludes=kubernetes \n#验证升级计划\nkubeadm upgrade plan\n#拉取国内镜像\nkubeadm config images pull --image-repository registry.aliyuncs.com/google_containers\n#选择要升级到的目标版\nkubeadm upgrade apply v1.28.0\n#成功提示\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.28.0\". Enjoy!\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n\n#升级kubelet 和 kubectl\n#下载kubelet 和 kubectl最新版\nyum install -y kubelet-1.28.0 kubectl-1.28.0 --disableexcludes=kubernetes\n#重启 kubelet\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n\n#查看版本\n kubeadm version\n kubelet --version\n kubectl version\n```\n\nnode节点\n\n```Shell\n#下载kubeadm最新版\nyum install -y kubeadm-1.28.0 --disableexcludes=kubernetes\n#升级本地的 kubelet 配置\nkubeadm upgrade node\n[upgrade] The configuration for this node was successfully updated!\n[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.\n\n#升级kubelet 和 kubectl\n#下载kubelet 和 kubectl最新版\nyum install -y kubelet-1.28.0 kubectl-1.28.0 --disableexcludes=kubernetes\n#重启 kubelet\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n\n#查看版本\n kubeadm version\n kubelet --version\n kubectl version\n```\n\n### 定时更新Dashboard登录token\n\n```Shell\n#!/bin/bash\n#############描述#############\n:<<!\n定时生成Dashboard登录token，有效期24小时\n!\n#############描述#############\n\ntoken=$(/usr/bin/kubectl -n kubernetes-dashboard create token admin-user)\necho \"\n#######################################################\n$(date)生成新的Dashboard登录token，最新token如下：\n#######################################################\n$token\" > /root/dashboard/admin-user.token\n```\n\n### Dashboard配置ingress-nginx代理\n\n```Shell\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: k8s-dashboard\n  namespace: kubernetes-dashboard\n  labels:\n    ingress: k8s-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /  #重写路径\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"  #http自动转https\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\nspec:\n  ingressClassName: nginx \n  rules:\n    - host: k8s.yjs.51xueweb.cn\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard\n                port:\n                  number: 443\n```\n\n### node节点NotReady问题的处理\n\n![image-20240323232551958](Kubernetes知识梳理/image-20240323232551958-1711207555524-1.png)\n\n重新加入node即可\n\n```Shell\nkubeadm token create --print-join-command > join-command.txt\ncat join-command.txt\n```\n\n### ingress-nginx控制器安装\n\n```Shell\n#下载yaml文件\nwget https://gitcode.net/mirrors/kubernetes/ingress-nginx/-/blob/master/deploy/static/provider/baremetal/deploy.yaml\n\n#修改yaml文件中拉取镜像的地址\n#####################修改内容######################\nregistry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v20230407\nregistry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.8.1\n#####################修改内容######################\n#安装\nkubectl apply -f deploy.ymal\n#查看状态\nkubectl get pods -n ingress-nginx\n################状态##################\nNAME                                       READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-2lz4v       0/1     Completed   0          5m46s\ningress-nginx-admission-patch-c6896        0/1     Completed   0          5m46s\ningress-nginx-controller-7575fb546-q29qn   1/1     Running     0          5m46s\n\n#修改yaml文件\n在Deployment类中加入副本\n####################修改内容####################\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.8.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  replicas: 2\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/instance: ingress-nginx\n      app.kubernetes.io/name: ingress-nginx\n```\n\n### k8s 使用 nfs 网络存储挂载\n\n```Shell\n#在集群所有节点安装nfs\nyum -y install nfs-utils\n#创建 nfs 目录并修改权限\nmkdir /data\nchmod -R 777  /data\necho \"/data 10.12.10.32(insecure,rw,sync,no_root_squash) 10.12.10.33(insecure,rw,sync,no_root_squash) 10.12.10.34(insecure,rw,sync,no_root_squash) 10.12.10.35(insecure,rw,sync,no_root_squash)\" >> /etc/exports\n#使配置生效\nexportfs -r\nexportfs \n#启动rpcbind、nfs服务\nsystemctl restart rpcbind && systemctl enable rpcbind\nsystemctl restart nfs-server && systemctl enable nfs-server\n#查看 RPC 服务的注册状况\n rpcinfo -p localhost\n #放行防火墙端口\n  firewall-cmd --add-port=2049/tcp --permanent\n   firewall-cmd --add-port=2049/udp --permanent\n   firewall-cmd --reload\n   firewall-cmd --list-all \n 测试是否正常提供 nfs 目录\n showmount -e 创建目录的主机ip\n #将10.12.10.31的共享目录挂载到其他主机\n  mount -t nfs 10.12.10.31:/data data\n```\n\n创建PV\n\n```Shell\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv\n  namespace: nfs-test\n  labels:\n    pv: nfs-pv\nspec:\n  capacity:\n    storage: 5000Mi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs\n  nfs:\n    server: 10.12.10.31  #创建目录的主机ip\n    path: \"/data\"\n```\n\n创建pvc\n\n```Shell\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-pvc\n  namespace: nfs-test\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5000Mi  #容量\n  selector:\n    matchLabels:\n      pv: nfs-pv   #关联pv\n```\n\n###  k8s命令自动补全\n\n```Shell\nyum -y install bash-completion\nsource /usr/share/bash-completion/bash_completion\necho 'source <(kubectl completion bash)' >>  ~/.bashrc\n```","tags":["Linux","运维","k8s","知识梳理","总结"],"categories":["容器化","梳理总结","k8s"]},{"title":"docker知识梳理","url":"/yyg/5c56de6f/","content":"### 容器设置开机自启\n\n容器创建时未指定 --restart=always ,可通过update 命令设置\n\n```PowerShell\ndocker update --restart=always 容器ID\n```\n\n### 容器关闭自启动\n\n```Plaintext\ndocker update --restart=no 容器ID\n```\n\n### 获取容器/镜像的元数据\n\n```Plain\ndocker inspect id\n```\n\n### 容器网络\n\n创建容器网络\n\n```Shell\n#docker network create --subnet 子网范围 --ip-range 子网内可用的IP地址范围 Docker网络名\ndocker network create --subnet 172.20.0.0/16 --ip-range 172.20.1.102/29 zabbix-net\n```\n\n#### 删除创建的容器网络\n\n```Shell\n#查看容器网络\ndocker network ls\n#删除容器网络\ndocker network rm 容器id\n```\n\n### 查看docker所有容器ip\n\n```PowerShell\n docker inspect --format='{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $(docker ps -aq)\n```\n\n### 查看所有容器的名字\n\n```Shell\n#查看所有容器名字\ndocker ps -a --format {{.Names}}\n```\n\n### docker生成镜像（无目录映射）\n\n```PowerShell\ndocker commit id/容器名 镜像名\n\n# 生成带有提交信息与作者的镜像\ndocker commit -m \"提交信息\" -a \"作者\" id/容器名 镜像名\n#查看提交信息与作者\n docker inspect --format '提交信息:{{.Comment}},  作者:{{.Author}}' 镜像名\n```\n\n对有目录/文件映射的容器的处理\n\n针对映射了宿主机的文件/目录的容器，在打包成镜像时，映射文件/目录并不会被打包进去。\n\n针对这种情况提供两种解决方案：\n\n1.先生成镜像文件，再用新生成的镜像创建一个容器，将原来的映射目录/文件拷贝进容器后再打包，最后就会形成一个带有源目录镜像文件。\n\n```Shell\ndocker cp 要拷贝的文件或目录 容器ID或容器名称:要拷贝到的容器内部位置\n```\n\n2.对外提供镜像时，把映射文件一起提供出去。当运行镜像时，指定映射关系。\n\n### 导出镜像为压缩包\n\n```Shell\ndocker save -o /home/镜像名.tar 镜像名:latest\n```\n\n### 将镜像名.tar导入为镜像\n\n```Shell\n# 导入方法一\ndocker load --input /home/skj.tar\n# 导入方法二\n$ docker load -i /home/skj.tar\n# 导入方法三\n$ docker load < /home/skj.tar\n```\n\n### 运行docker compose文件，创建相关容器\n\n```PowerShell\ndocker compose -f docker-compose-XXXXXXX.yml up -d\n```\n\n创建docker compose文件时，要使用已经存在的网络，书写脚本网络内容的命令如下：\n\n```Shell\nnetworks:\n  网络名称:\n    external: true\n```\n\n### Docker 命令补全\n\n```Shell\n curl -L https://raw.githubusercontent.com/docker/composcompose > /etc/bash_completion.d/docker-compose\n #重新登录\n bash\n```\n\n### Docker 一键安装\n\nDocker 镜像源\n\n```Bash\n  \"registry-mirrors\": [\n    \"https://registry.docker-cn.com\",\n    \"http://hub-mirror.c.163.com\",\n    \"https://dockerhub.azk8s.cn\",\n    \"https://mirror.ccs.tencentyun.com\",\n    \"https://registry.cn-hangzhou.aliyuncs.com\",\n    \"https://docker.mirrors.ustc.edu.cn\",\n    \"https://docker.m.daocloud.io\",\n    \"https://noohub.ru\",\n    \"https://huecker.io\",\n    \"https://dockerhub.timeweb.cloud\"\n  ]\n```\n\n#### 方式一：centos\n\n```Shell\n#使用官方脚本自动安装,默认安装最新版\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\nsystemctl start docker\nsystemctl enable docker\n\n#设置存储库安装\nsudo yum install -y yum-utils\n#官方源\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n#阿里源（推荐）\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\nsudo yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n```\n\n#### 方式二：openEuler 22.03 (LTS-SP2)非管理员\n\n```Shell\n#!/bin/bash\n#############描述#############\n:<<!\n安装Docker CE，设置开机自启\n修改docker存储路径\n!\n#############描述#############\necho \"[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-stable-debuginfo]\nname=Docker CE Stable - Debuginfo $basearch\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/debug-$basearch/stable\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-stable-source]\nname=Docker CE Stable - Sources\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/source/stable\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-test]\nname=Docker CE Test - $basearch\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/$basearch/test\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-test-debuginfo]\nname=Docker CE Test - Debuginfo $basearch\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/debug-$basearch/test\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-test-source]\nname=Docker CE Test - Sources\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/source/test\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-nightly]\nname=Docker CE Nightly - $basearch\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/$basearch/nightly\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-nightly-debuginfo]\nname=Docker CE Nightly - Debuginfo $basearch\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/debug-$basearch/nightly\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\n[docker-ce-nightly-source]\nname=Docker CE Nightly - Sources\nbaseurl=https://repo.huaweicloud.com/docker-ce/linux/centos/7/source/nightly\nenabled=0\ngpgcheck=1\ngpgkey=https://repo.huaweicloud.com/docker-ce/linux/centos/gpg\n\" > /etc/yum.repos.d/docker-ce.repo\n\nyum install container-selinux\nyum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin --nogpgcheck --nobest\n#启动docker并设置开机自启\nsystemctl start docker\nsystemctl enable docker\n#查看版本\ndocker -v\ndocker compose version\necho \"***********************\ndocker 安装成功\n***********************\"\n\nmkdir -p /data/dockerData\n#/data/docker为存储新路径\necho '\n{\n  \"data-root\": \"/data/dockerData\"\n}' > /etc/docker/daemon.json\nsystemctl restart docker\necho \"docker的存储位置改为/data/dockerData\"\n```\n\n#### 方式三：openEuler 22.03 (LTS-SP2)管理员\n\n```Shell\n#!/bin/bash\n#############描述#############\n:<<!\n安装Docker CE，设置开机自启\n修改docker存储路径\n!\n#############描述#############\n\n#配置Docker CE的yum存储库\necho \"[docker-ce-stable]\nname=Docker CE Stable - \\$basearch\nbaseurl=https://download.docker.com/linux/centos/7/\\$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/centos/gpg\n\" > /etc/yum.repos.d/docker-ce.repo\n\nyum install container-selinux\nyum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin --nogpgcheck --nobest\n#启动docker并设置开机自启\nsystemctl start docker\nsystemctl enable docker\n#查看版本\ndocker -v\ndocker compose version\necho \"***********************\ndocker 安装成功\n***********************\"\n\nmkdir -p /data/dockerData\n#/data/docker为存储新路径\necho '\n{\n  \"data-root\": \"/data/dockerData\"\n}' > /etc/docker/daemon.json\nsystemctl restart docker\necho \"docker的存储位置改为/data/dockerData\"\n```\n\n#### 方式四：openEuler 20.03 (LTS)x86\n\n```Shell\n#安装openEuler镜像源\nwget -O /etc/yum.repos.d/openEulerOS.repo https://repo.huaweicloud.com/repository/conf/openeuler_x86_64.repo\n#安装依赖软件源\nwget -O /etc/yum.repos.d/CentOS-Base.repo https://repo.huaweicloud.com/repository/conf/CentOS-7-reg.repo \n#\nsed -i 's/\\$releasever/7/g' /etc/yum.repos.d/CentOS-Base.repo\nyum clean all\nyum makecache\n```\n\n### docker容器出现无法重启，报错内容如下\n\n> Error response from daemon: Cannot restart container 66dd752e0bd4: id already in use\n\n解决方法：\n\n```Bash\n#查看容器ID\ndocker ps -a\n#查看容器进程\nps -aux | grep 容器ID\n#杀掉进程\nkill -9 容器进程\n```\n\n再次启动，成功！\n\n#### 方式五：openEuler 22.03 \n\n```Bash\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\nsed -i 's/\\$releasever/7/g' /etc/yum.repos.d/docker-ce.repo\n```\n\n### docker容器内出现无法使用安装镜像源的问题，如下图\n\n![img](docker知识梳理/1711208367091-1-1711208371676-4.png)\n\n在/etc/docker/daemon.json文件中配置DNS即可解决\n\n```Shell\nvi /etc/docker/daemon.json\n{\n \"dns\": [\"8.8.8.8\", \"114.114.114.114\"]\n }\n```\n\n### 拉取`gcr.io` , `k8s.gcr.io` , `registry.k8s.io` , `quay.io`, `ghcr.io`官方镜像脚本\n\n方式一：\n\n> wget https://raw.githubusercontent.com/anjia0532/gcr.io_mirror/master/pull-k8s-image.sh chmod +x pull-k8s-image.sh\n\n```Shell\nvi pull-images.sh\n###########################################3\n#!/bin/sh\n\nk8s_img=$1\nmirror_img=$(echo ${k8s_img}|\n        sed 's/quay\\.io/anjia0532\\/quay/g;s/ghcr\\.io/anjia0532\\/ghcr/g;s/registry\\.k8s\\.io/anjia0532\\/google-containers/g;s/k8s\\.gcr\\.io/anjia0532\\/google-containers/g;s/gcr\\.io/anjia0532/g;s/\\//\\./g;s/ /\\n/g;s/anjia0532\\./anjia0532\\//g' |\n        uniq)\n\nif [ -x \"$(command -v docker)\" ]; then\n  sudo docker pull ${mirror_img}\n  sudo docker tag ${mirror_img} ${k8s_img}\n  exit 0\nfi\n\nif [ -x \"$(command -v ctr)\" ]; then\n  sudo ctr -n k8s.io image pull docker.io/${mirror_img}\n  sudo ctr -n k8s.io image tag docker.io/${mirror_img} ${k8s_img}\n  exit 0\nfi\n\necho \"command not found:docker or ctr\"\n###############################################\nchmod +x pull-images.sh\n#执行格式\n./pull-images.sh  镜像名\n```\n\n方式二：\n\nhttps://dockerproxy.com/\n\n### docker的 iptables 策略\n\n```Shell\n#显示DOCKER-USER的链中的规则信息\niptables -nL DOCKER-USER\n# 在主机10.10.3.117执行如下命令：(ens192 为本机网卡，以实际为准)\n#禁止所有IP访问docker的80端口\niptables -I DOCKER-USER -i ens192 -p tcp --dport 80 -j DROP\n#只允许10.10.3.122访问docker的80端口\niptables -I DOCKER-USER -i ens192 -s 10.10.3.122 -p tcp --dport 80 -j ACCEPT\n#删除DOCKER-USER的链中的默认规则\niptables -D DOCKER-USER -j RETURN\n#保存DOCKER-USER策略\nservice iptables save\n#设置iptables开机自启,使策略永久生效\nsystemctl enable iptables.service\n#或者\n/etc/rc.d/init.d/iptables save\n```\n\n### 容器中查看端口方法\n\n在容器中无需下载工具查看端口是否开放，如果连接成功，命令将无任何输出而结束。如果连接失败，将会输出错误消息。\n\n```\nprintf \"\" > /dev/tcp/IP地址/端口号\n```\n\n查看容器自己的3000端口号是否开启\n\nprintf \"\" > /dev/tcp/localhost/3000\n\n查看其他容器（172.20.101.23）的5030端口号是否开启\n\nprintf \"\" > /dev/tcp/172.20.101.23/5030\n\n> bash: connect: Connection refused\n>\n> bash: /dev/tcp/172.20.101.23/5030: Connection refused","tags":["Linux","docker","运维","知识梳理","总结"],"categories":["容器化","梳理总结","docker","知识梳理"]},{"title":"shell脚本","url":"/yyg/d0a69eae/","content":"\n## 私有镜像仓库相关脚本\n\n```Shell\n#签名环境准备\nyum install -y golang\nmkdir -p /data/cosign\ncd /data/cosign\n#x86\nwget https://github.com/sigstore/cosign/releases/download/v2.2.2/cosign-2.2.2-1.x86_64.rpm\nrpm -ivh cosign-2.2.2-1.x86_64.rpm\n#arm\nwget https://github.com/sigstore/cosign/releases/download/v2.2.2/cosign-2.2.2-1.aarch64.rpm\nrpm -ivh cosign-2.2.2-1.aarch64.rpm\n#验证安装\ncosign version\n#生成密钥，进行签名\ncosign generate-key-pair\n```\n\n更改标签、推送镜像并对镜像签名脚本\n\n```Shell\nvi hub-docker.sh && chmod +x hub-docker.sh\n#!/bin/bash\n#############描述#############\n:<<!\n1、修改镜像标签\n2、推送镜像到私有仓库\n3、给已推送的镜像签名\n!\n#############描述#############\nexport COSIGN_PASSWORD=私有仓库用户密码\n\n#登录\necho \"登录docker私有仓库，请输入密码：\"\ndocker login 私有仓库域名  -u 用户名\n\n\nread -p \"请输入要推送的项目库名称:\" pj_name\n#推送仓库地址\nhostname=私有仓库域名\n\necho \"\n*************************************************************\n输入要求：每行表示一个原镜像名与新镜像名，原镜像名与新镜像名之间以空格分隔\n示例格式：\nnginx:latest nginx:1.0\nhttpd:2.5 httpd:1.0.0\n请分别输入要推送的原镜像名与新镜像名,\n输入完成后先回车、再按Ctrl+D结束：\n*************************************************************\"\n#原镜像名数组\nlist_old_images=()\n#新镜像名数组\nlist_new_images=()\n#行数\nline_number=0\nwhile IFS= read -r line; do\n    # 拆分每行为两个元素\n    read -r old_images new_images <<< \"$line\"\n    list_old_images+=(\"$old_images\")\n    list_new_images+=(\"$new_images\")\n    ((line_number++))\ndone\nfor ((line_number_1=0; line_number_1 < line_number; line_number_1++)); do\n   #修改镜像标签\n   docker tag ${list_old_images[$line_number_1]} $hostname/$pj_name/${list_new_images[$line_number_1]}\n   #推送到私有仓库\n   docker push $hostname/$pj_name/${list_new_images[$line_number_1]}\n   #进行镜像签名\n   yes | cosign sign  --allow-insecure-registry --key /data/cosign/cosign.key $hostname/$pj_name/${list_new_images[$line_number_1]}\ndone\n#去除密钥密码\nunset COSIGN_PASSWORD\n#验证查看\nenv\n#选择是否登出私有仓库\nread -p \"是否要登出docker私有仓库【y(Y),n(N)】:\" choice \nif  [[ $choice  == \"y\" || $choice  == \"Y\" ]];then\n     docker logout hub-docker.tiliatech.net\nelif [[ $choice  == \"n\" || $choice  == \"N\" ]];then\n     echo \"注意：还未登出docker私有仓库\"\nelse\n     echo \"请输入正确的选项【y(Y),n(N)】\"\nfi\necho -e \"**************************\n\\033[35m镜像已推送完成\n共$line_number个\\033[m\n**************************\"\n```\n\n## 目录备份脚本\n\n### 需求一：\n\n> 写一个备份目录的脚本，需要的考虑的功能有哪些？\n>\n> 1.  指定目录的路径，作为变量dir进行定义\n> 2.  指定备份文件的文件名和路径，作为变量filename,file_path进行定义，使用时间戳作为标识。时间戳实现年月日时分秒毫秒            举例：filename-20231216150913678.tar.gz\n> 3. 将输入的变量dir、filename、file_path保存到dir_file.txt文件中（便于之后执行定时任务时，读取要备份的目录）\n> 4.  对目录进行打包和压缩，压缩使用gzip算法\n> 5.  备份完成后输出日志，日志路径：/var/log/back.log.0\n> 6.  备份完成后删除n天前的备份文件，n作为变量\n> 7. 执行写入定时任务，每天定时更新一次（默认2点）\n\n#### 方法一：\n\n```Shell\nvi bak.sh && chmod +x bak.sh\n#!/bin/bash\nconfig_file=\"/home/dir_file.txt\"\n#判断存储变量的txt是否存在\nif [ -e $config_file ]; then\n    echo \"/home/dir_file.txt存在\"\n    source /home/dir_file.txt\n    sudomkdir -p $dir\nfi\nline_number=0\necho \"$date输入备份文件的文件名与路径\" >> /var/log/back.log.0\nwhile IFS= read -r line; do\n    # 拆分每行为两个元素\n    read -r filename file_path <<< \"$line\"\n    list_filename+=(\"$filename\")\n    list_file_path+=(\"$file_path\")\n    ((line_number++))\ndone < /home/file.txt\necho \"$date备份文件的文件名与路径保存到/home/file.txt\" >> /var/log/back.log.0\n#定义时间格式\nDAY=$(date +\"%Y%m%d\")\nTIME=$(date +\"%H%M%S%N\")\n# 备份文件计数器\nfolder_count=0\nfor ((line_number_1=0; line_number_1 < line_number; line_number_1++)); do\n  sudo tar -zcvf $dir/${list_filename[$line_number_1]}-$DAY$TIME.tar.gz  ${list_file_path[$line_number_1]}  | tee -a  /var/log/back.log.0\n   ((folder_count++))\n  echo \"$date ${list_file_path[$line_number_1]}已完成压缩,\n        压缩文件的具体位置为:${list_filename[$line_number_1]}-$DAY$TIME.tar.gz\" >> /var/log/back.log.0\ndone\n#删除n天前的归档文件\ndel_day=$(date -d \"-$n day\" +%Y%m%d)\nsudo rm -f $dir/*-$del_day*.tar.gz\necho  \"$date删除del_day的备份文件\" >> /var/log/back.log.0\necho -e \"\n***********************\n\\033[35m$del_day文件已删除\n$DAY$TIME文件已备份\n$DAY$TIME已备份总数: $folder_count\\033[m\n***********************\" | tee -a  /var/log/back.log.0\n```\n\n在/home目录下创建两个txt文件，分别如下：\n\n```Shell\ntouch /home/file.txt /home/dir_file.txt\n```\n\n/home/file.txt\n\n```Shell\n#要备份文件的文件名与路径\nsvn-http /data/svn-http\ntest-svn /data/test-svn\n```\n\n/home/dir_file.txt\n\n```Shell\n#备份目录的路径\ndir=/home/bak\n#删除n天前的归档文件\nn=3\n```\n\n#### 方法二：\n\n```Shell\n#!/bin/bash\nconfig_file=\"/home/dir_file.txt\"\n#判断存储变量的txt是否存在\nif [ ! -e $config_file ]; then\n    touch /home/dir_file.txt /home/file.txt\n    read -p \"请输入备份目录的路径:\" dir\n       echo \"dir=$dir\" > /home/dir_file.txt\n    read -p \"请输入要删除备份文件具体天数（1表示:1天前）:\" n\n        echo \"n=$n\" >> /home/dir_file.txt\n    echo \"请分别输入要备份文件的文件名与路径:\n     *************************************************************\n      输入要求：每行表示一个备份文件的文件名与路径，备份文件的文件名与路径之间>以空格分隔\n      示例格式：\n          test /data/test\n          docker /data/docker\n\n      输入完成后先回车、再按Ctrl+D结束：\n      *************************************************************\"\n      #备份文件的文件名\n      list_filename=()\n      #路径\n      list_file_path=()\n    cat > /home/file.txt\nelse\n    echo \"/home/dir_file.txt存在\"\n    source $config_file\n    mkdir -p $dir\nfi\n\nline_number=0\necho \"$date输入备份文件的文件名与路径\" >> /var/log/back.log.0\nwhile IFS= read -r line; do\n    # 拆分每行为两个元素\n    read -r filename file_path <<< \"$line\"\n    list_filename+=(\"$filename\")\n    list_file_path+=(\"$file_path\")\n    ((line_number++))\ndone < /home/file.txt\necho \"$date备份文件的文件名与路径保存到/home/file.txt\" >> /var/log/back.log.0\n#定义时间格式\nDAY=$(date +\"%Y%m%d\")\nTIME=$(date +\"%H%M%S%N\")\n# 备份文件计数器\nfolder_count=0\nfor ((line_number_1=0; line_number_1 < line_number; line_number_1++)); do\n  tar -zcvf $dir/${list_filename[$line_number_1]}-$DAY$TIME.tar.gz  ${list_file_path[$line_number_1]}  | tee -a  /var/log/back.log.0\n   ((folder_count++))\n  echo \"$date ${list_file_path[$line_number_1]}已完成压缩,\n        压缩文件的具体位置为:${list_filename[$line_number_1]}-$DAY$TIME.tar.gz\" >> /var/log/back.log.0\ndone\n#删除n天前的归档文件\ndel_day=$(date -d \"-$n day\" +%Y%m%d)\nrm -f $dir/*-$del_day*.tar.gz\necho  \"$date删除del_day的备份文件\" >> /var/log/back.log.0\necho -e \"\n***********************\n\\033[35m$del_day文件已删除\n$DAY$TIME文件已备份\n$DAY$TIME已备份总数: $folder_count\\033[m\n***********************\" | tee -a  /var/log/back.log.0\n```\n\n### 需求二：\n\n> 使用脚本进行Linux 目录备份的基本要求\n>\n> 1. 命名规范：时间戳-单位名称-backup-IP地址/32-业务名称.tar.gz\n> 2. 备份周期：每周一早上1-3点，保留三个有效备份\n> 3. 备份前检测：同时出现下述情况，则跳过此次备份任务\n>\n> - du -s 备份目录取得的值和上次相同\n> - 目录下的文件最后一次更新时间、备份时间是上次执行时间\n> - 上次备份是成功的\n>\n> 1. 日志记录：下述情况记录日志、日志描述要规范\n>\n> - 备份完成\n> - 备份失败\n> - 备份任务跳过\n> - 删除备份文件\n> - 删除备份文件失败\n>\n> 1. 备份数据存档要求：\n>\n> - 存储三个备份文件\n> - 存储的备份文件要做哈希校验，恢复时进行验证，哈希校验值存备份日志\n\n```Shell\nvi backup.sh && chmod +x backup.sh\n#!/bin/bash\n#############描述#############\n:<<!\n备份目录脚本\n每周一早上1-3点执行，保留三个有效备份\n!\n#############描述#############\n#配置存储文本\nconfigFile=\"/home/dirFile.txt\"\n#备份目录与路径存储文本\nlistFile=\"/home/file.txt\"\n\n#判断存储变量的txt是否存在\nif [ ! -e \"$configFile\" ] || [ ! -e \"$listFile\" ] ; then\n    sudo touch $configFile $listFile\n    read -p \"请输入存放备份文件的路径:\" dir\n    echo \"#存放备份文件的路径\" > $configFile\n    echo \"dir=$dir\" >> $configFile\n    read -p \"请输入单位名称:\" unitName\n    echo \"#单位名称\" >> $configFile\n    echo \"unitName=$unitName\" >> $configFile\n    read -p \"请输入IP地址:\" ipAdd\n    echo \"#IP地址\" >> $configFile\n    echo \"ipAdd=$ipAdd\" >> $configFile\n    read -p \"请输入要保留备份文件周数（1表示:保留近1周）:\" n\n    echo \"#保留备份文件周数\" >> $configFile\n    echo \"n=$n\" >> $configFile\n    echo \"请分别输入要备份目录名与路径:\n     *************************************************************\n      输入要求：每行表示一个备份目录与路径，备份目录名与路径之间以空格分隔\n      示例格式：\n          test /data/test\n          docker /data/docker\n\n      输入完成后先回车、再按Ctrl+D结束：\n      *************************************************************\"\n      #备份文件的文件名\n      listFilename=()\n      #路径\n      listFilePath=()\n    cat > $listFile\nelse\n    source \"$configFile\"\nfi\n\n#统计备份目录的数量\nlineNumber=0\n#分别读取备份目录与路径的变量，并写入数组\nwhile IFS= read -r line; do\n    # 拆分每行为两个元素\n    read -r filename filePath <<< \"$line\"\n    listFilename+=(\"$filename\")\n    listFilePath+=(\"$filePath\")\n    ((lineNumber++))\ndone < $listFile\n\n#定义时间格式\nDATE=$(date +\"%Y%m%d%H%M%S%N\")\nlogDate=${DATE:0:8}\nlastlogDate=$(date -d \"$logDate 7 days ago\" +\"%Y%m%d\")\n#输出日志时间格式\nprintLogDate=$(date +\"%Y-%m-%d %T\")\n#定义日志路径\nlogDir=\"/var/log/backupDir\"\nlog=\"$logDir/$logDate-backup.log.0\"\nlastLog=\"$logDir/$lastlogDate-backup.log.0\"\n#计数器\nfolderCount=0\n\nif [ ! -d \"$logDir\" ]; then\n    sudo mkdir -p $logDir\n    echo \"$logDir目录已创建\"\nfi\n\nfor ((lineNumber1=0; lineNumber1 < lineNumber; lineNumber1++)); do\n  # 计算备份目录的大小\n  echo \"---------------------------------------------------------\" | sudo tee -a  \"$log\"\n  echo \"[$printLogDate] 备份目录${listFilePath[$lineNumber1]}的大小如下:\" >> \"$log\"\n  echo \"[$printLogDate] ${listFilePath[$lineNumber1]} = $(du -s \"${listFilePath[$lineNumber1]}\" | awk '{print $1}')\" >> \"$log\"\n  ###########################判断是否符合跳过备份条件：###########################\n  #1、备份目录的大小取值和上次备份相同\n  #2、目录下的文件最后一次更新时间是上次执行时间（文件的最新时间小于上次备份的时间）\n  #3、上次备份是成功的\n  ################################################################################\n  #上次备份目录的大小\n  lastDirSize=$(grep \"${listFilePath[$lineNumber1]} =\" \"$lastLog\" | awk '{print $5}')\n  #备份前读取的备份目录的大小\n  dirSize=$(du -s \"${listFilePath[$lineNumber1]}\" | awk '{print $1}')\n  #目录下的文件最后一次更新时间\n  lastUpdateTime=$(find \"${listFilePath[$lineNumber1]}\" -type f -printf '%TY%Tm%Td%TH%TM%.2TS %p\\n' | sort -r | head  -n 1 | awk '{print $1}')\n  #上次备份的时间\n  lastBackupTime=$(grep \"${listFilename[$lineNumber1]}.tar.gz.*Hash\" \"$lastLog\" | head -n 1 | awk '{print substr($3, 1, 14)}')\n  #比对上次备份成功的标志字符SUCCESS\n  lastSuccessFlag=$(grep \"backup ${listFilePath[$lineNumber1]}\" \"$lastLog\" | awk '{print $5}')\n  if [  \"$lastDirSize\" = \"$dirSize\"  ]  && [ \"$lastUpdateTime\" -le \"$lastBackupTime\" ]  && [ \"$lastSuccessFlag\" = \"SUCCESS\" ] ; then\n        echo \"[$printLogDate] ${listFilePath[$lineNumber1]}无数据变化,跳过此次备份任务\"  | sudo tee -a \"$log\"\n  else\n        echo \"[$printLogDate] 开始对${listFilePath[$lineNumber1]}目录进行备份...\"  | tee -a \"$log\"\n        tar -zcvf \"$dir/$DATE-$unitName-backup-$ipAdd-${listFilename[$lineNumber1]}.tar.gz\"  ${listFilePath[$lineNumber1]}\n        backupHash=$(md5sum \"$dir/$DATE-$unitName-backup-$ipAdd-${listFilename[$lineNumber1]}.tar.gz\" | awk '{print $1}')\n        echo \"[$printLogDate] $DATE-$unitName-backup-$ipAdd-${listFilename[$lineNumber1]}.tar.gz已生成\" >> \"$log\"\n        echo \"[$printLogDate] $DATE-$unitName-backup-$ipAdd-${listFilename[$lineNumber1]}.tar.gz的Hash值为：$backupHash\" >> \"$log\"\n        ((folderCount++))\n        echo \"[$printLogDate] 备份目录${listFilePath[$lineNumber1]}已完成压缩,压缩文件的具体位置为:$dir\" >> \"$log\"\n        echo \"[$printLogDate] backup ${listFilePath[$lineNumber1]} SUCCESS\"  | tee -a  \"$log\"\n  fi\n  #保留最近n次备份\n  if [ -n \"$( find \"$dir\" -maxdepth 1 -type f -name \"*${listFilename[$lineNumber1]}*.tar.gz\")\" ]\n  then\n      #按时间降序排列\n      files=$(find $dir -name \"*${listFilename[$lineNumber1]}*.tar.gz\" -printf \"%T@\\t%p\\n\" | sort -rn | cut -f 2-)\n      # 初始化计数器和数组\n      count=0\n      declare -a filesKeep\n      # 遍历文件并保留最近的n个\n      while IFS= read -r file; do\n          if [ $count -lt $n ]; then\n              filesKeep[$count]=$file\n              count=$((count+1))\n          else\n              rm -f \"$file\"\n              echo \"[$printLogDate] 删除无需保留的备份文件:$file\"  | sudo tee -a  \"$log\"\n          fi\n      done <<< \"$files\"\n  else\n      echo \"[$printLogDate] 删除失败，未找到${listFilename[$lineNumber1]}的备份文件\" | sudo tee -a  \"$log\"\n  fi\ndone\n\necho \"---------------------------------------------------------\" | sudo tee -a  \"$log\"\n\necho -e \"\\033[35m[$printLogDate] 所有目录备份完成,备份总数: $folderCount\\033[m\n---------------------------------------------------------\" | sudo tee -a  \"$log\"\n\n```\n\n```\n#写入定时任务\necho '\n0 1 * * 1 root bash /data/script/backup.sh >> /var/log/backup.log\n' > /etc/cron.d/backup.cron\n\ncat /etc/cron.d/backup.cron\n```\n\n### shell输出有颜色的提示信息\n\n```Shell\n#使用以下ANSI转义序列来设置不同的颜色和样式\n\\033[30m - \\033[37m：设置前景色为黑色至白色。\n\\033[40m - \\033[47m：设置背景色为黑色至白色。\n常用的ANSI转义序列示例：\n设置文本颜色：\n\\033[30m：设置文本颜色为黑色\n\\033[31m：设置文本颜色为红色\n\\033[32m：设置文本颜色为绿色\n\\033[33m：设置文本颜色为黄色\n\\033[34m：设置文本颜色为蓝色\n\\033[35m：设置文本颜色为洋红色\n\\033[36m：设置文本颜色为青色\n\\033[37m：设置文本颜色为白色\n设置背景颜色：\n\\033[40m：设置背景颜色为黑色\n\\033[41m：设置背景颜色为红色\n\\033[42m：设置背景颜色为绿色\n\\033[43m：设置背景颜色为黄色\n\\033[44m：设置背景颜色为蓝色\n\\033[45m：设置背景颜色为洋红色\n\\033[46m：设置背景颜色为青色\n\\033[47m：设置背景颜色为白色\n设置文本样式：\n\\033[0m：重置所有样式（也可以写作\\033[m）\n\\033[1m：设置粗体\n\\033[2m：设置模糊（不是所有终端都支持）\n\\033[3m：设置斜体（不是所有终端都支持）\n\\033[4m：设置下划线\n\\033[5m：设置闪烁（不是所有终端都支持）\n\\033[7m：设置反显\n\\033[8m：设置隐藏（不是所有终端都支持）\n#绿色\necho -e \"\\033[1;32m安装成功！\\033[m\"\nprintf \"\\n\\033[1;32m%s\\033[m\\n\" \"任务已完成!\"\n#紫色\nprintf \"\\033[1;35m%s\\033[m\\n\" \"错误：文件不存在。\"\necho -e \"\\033[1;35m安装失败！\\033[m\"\n```\n\n### shell脚本显示自定义进度条\n\n```Shell\n# 设置总进度时间，单位秒\ntotal_steps=5\n\necho \"加载进度如下:\"\n# 显示进度条函数\nshow_progress() {\n    local current_step=$1\n    local total_steps=$2\n    local progress=$((current_step * 100 / total_steps))\n    local bar_length=50\n    local completed_length=$((progress * bar_length / 100))\n    local remaining_length=$((bar_length - completed_length))\n\n    printf \"[%-${bar_length}s] %d%%\\r\" \\\n        \"$(printf  '*%.0s' $(seq 1 $completed_length))\" \\\n        \"$progress\"\n}\n\n# 循环展示进度条\nfor ((step=1; step<=total_steps; step++)); do\n    show_progress $step $total_steps\n    sleep 1  # 模拟任务执行时间\ndone\n```\n\n### 磁盘永久挂载脚本\n\n```Shell\nvi disk.sh && chmod +x disk.sh\n#!/bin/bash\n#############描述#############\n:<<!\n在根目录创建data文件夹，将磁盘挂载到/data\n!\n#############描述#############\nmkdir /data\n#查看磁盘分区情况，找到要挂载的磁盘名称\nfdisk -l\n#列出当前系统中所有挂载的文件系统\ndf -l\nread -p \"请输入需要格式化的磁盘名称（如：/dev/sdb）：\" disk\n#格式化磁盘\nmkfs.ext4 $disk\n#将磁盘挂载到/data\nmount $disk /data\n#修改文件$disk，设置为开机自动挂载\n#在文件中添加如下内容\necho \"$disk                /data                      ext4    defaults        0 0\" >>/etc/fstab\necho \"***********************\n磁盘永久挂载完成\n***********************\"\nlsblk\n```\n\n### 安装docker脚本（openEuler 22.03 LTS SP2）\n\n```Shell\nvi docker.sh && chmod +x docker.sh\n#!/bin/bash\n#############描述#############\n:<<!\n安装Docker CE，设置开机自启\n修改docker存储路径\n!\n#############描述#############\n\n#配置Docker CE的yum存储库\necho \"[docker-ce-stable]\nname=Docker CE Stable - \\$basearch\nbaseurl=https://download.docker.com/linux/centos/7/\\$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/centos/gpg\n\" > /etc/yum.repos.d/docker-ce.repo\n\nyum install -y container-selinux\nyum install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin \n#启动docker并设置开机自启\nsystemctl start docker\nsystemctl enable docker\n#查看版本\ndocker -v\ndocker compose version\necho \"***********************\ndocker 安装成功\n***********************\"\n\nmkdir -p /data/dockerData  \n#/data/dockerData为存储新路径\necho '\n{\n  \"data-root\": \"/data/dockerData\"\n}' > /etc/docker/daemon.json\nsystemctl restart docker\n#补全命令\n curl -L https://raw.githubusercontent.com/docker/composcompose > /etc/bash_completion.d/docker-compose\n #重新登录\n bash\necho \"docker的存储位置改为/data/dockerData\"\n```\n\n### 安装nginx脚本\n\n```Shell\nvi nginx.sh && chmod +x nginx.sh\n#!/bin/bash\n#############描述#############\n:<<!\n安装nginx，并设置开机自启\n!\n#############描述#############\n\n#下载nginx\n yum install -y nginx\n \n #启动Nginx服务、开机自动启动\n systemctl start nginx\n systemctl enable nginx\n \n #允许端口进出防火墙\n firewall-cmd --zone=public --add-port=80/tcp --permanent\n firewall-cmd --reload\n firewall-cmd --list-all\n echo \"***********************\n nginx安装成功\n ***********************\"\n```\n\n### 新建目录、更改docker存储位置脚本\n\n```Shell\n#!/bin/bash\n#############描述#############\n:<<!\n新建项目所需文件夹\n更改docker的存储位置\n!\n#############描述#############\nmkdir /data\ncd /data\n#新建code文件夹存放源代码、logs文件夹存放日志、script文件夹存放脚本\nmkdir code  logs  script\n\n#分别进入三个文件夹\n#创建MAPP文件夹存放微应用相关文件资源 \n#创建OS文件夹存放业务系统相关文件资源\n\nmkdir code/mapp code/os logs/mapp logs/os script/mapp script/os \necho \"***********************\n新建所需目录完成\n***********************\"\n\nmkdir -p /data/dockerData\n#/data/dockerData为存储新路径\necho '\n{\n  \"data-root\": \"/data/dockerData\"\n}' > /etc/docker/daemon.json \nsystemctl restart docker\necho \"docker的存储位置改为/data/dockerData\"\n```\n\n### 批量代码同步脚本\n\n```Shell\nvi gitee-sync.sh && chmod +x gitee-sync.sh\n#!/bin/bash\n#############描述#############\n:<<!\n同步源代码，并且记录同步时间和信息到日志文件\n并定时每分钟进行同步任务\n!\n#############描述#############\n# 指定目录路径\ndirectory=\"/data/code\"\n# 定义同步日志文件路径\nlog_file=\"/data/logs/gitee-sync.log\"\n# 文件夹计数器\nfolder_count=0\n# 自定义时间格式\nGIT_TIME=$(date +\"%Y-%m-%d %T\")\n# 遍历目录下的文件夹\nfor folder in \"$directory\"/*/; do\n  # 获取文件夹名称\n  folder_name=$(basename \"$folder\")\n  # 增加计数器\n  ((folder_count++))\n\n  cd /data/code/$folder_name\n  #代码同步\n  git_check_result=`git pull origin master`\n  if  [[ $git_check_result =~ \"Already up to date.\" || $git_check_result =~ \"已经是最新的。\" ]]\n  then\n    echo \"$GIT_TIME《$folder_name》skip\" | tee -a $log_file\n  else\n     echo -e \"$GIT_TIME \\033[1;32m《$folder_name》update\\033[m\"  | tee -a $log_file\n  fi\ndone\n# 记录同步时间和信息到日志文件\necho -e \"\n****************************************\n\\033[35m$GIT_TIME所有项目完成代码同步\n已同步项目总数: $folder_count\\033[m\n****************************************\n\" | tee -a $log_file\n```\n\n写入定时任务\n\n```Shell\necho '\n* * * * * root bash /data/script/gitee-sync.sh >> /var/log/nginx/sync-code.log\n' > /etc/cron.d/gitee-sync.cron\n\ncat /etc/cron.d/gitee-sync.cron\n```\n\n### 自定义定时关机脚本\n\n```Shell\n#!/bin/bash\n##########脚本描述##########\n:<<!\n此脚本为服务器定时关机脚本\n且每天在固定时间可自动执行\n!\n##########脚本描述##########\necho \"\n(*^_^*)欢迎使用定时关机脚本(*^_^*)\n脚本默认执行时间是10:00\n默认关机时间是22:30\n\"\necho \"脚本执行时间格式如下:(用空格隔开)\n1、若设置10:00执行，格式：10 0\n1、若设置10:08执行，格式：10 8\n2、若设置11:56执行，格式：11 56\"\nread -rp \"请输入脚本执行时间(回车使用默认时间)：\" execute_hours execute_minutes\necho \"\n关机时间格式如下:(符号为英文状态)\n若设置关机时间为10:00，格式：10:00\"\nread -p \"请输入关机时间(回车使用默认时间)：\" shutdown_time\necho \"\n定时关机脚本已执行\n\"\nif [ -z \"$execute_hours\" ] || [ -z \"$execute_hours\" ]; then\n  echo \"使用脚本默认执行时间，即10:00\"\n  # 将当前的 crontab 导出到 mycron 文件中\n  crontab -l > mycron\n  # 在 mycron 文件中添加一行新的定时任务\n  echo \"0 10 * * * /bin/bash /home/automatic-shutdown.sh\" > mycron\n  # 将修改后的 mycron 文件导入到 crontab 中\n  crontab mycron\n  # 删除临时的 mycron 文件\n  rm -f mycron\nelse\n  crontab -l > mycron\n  echo \"$execute_minutes $execute_hours * * * /bin/bash /home/automatic-shutdown.sh\" > mycron\n  crontab mycron\n  rm -f mycron\nfi\n\nif [ -z \"$shutdown_time\" ]; then\n  echo \"使用默认关机时间，即22:30\n  \"\n  /sbin/shutdown -h 22:30\nelse\n  /sbin/shutdown -h $shutdown_time\nfi\n\nif [ -z \"$execute_hours\" ] || [ -z \"$execute_hours\" ] || [ -z \"$shutdown_time\" ]; then\necho \"\n#############################\n脚本会在每天10:00自动执行\n服务器会在每天22:30自动关机\n#############################\n\"\nelse\necho \"\n#############################\n脚本会在每天$execute_hours:$execute_minutes自动执行\n服务器会在每天$shutdown_time自动关机\n#############################\n\"\nfi\n```\n\n### 拉取`gcr.io` , `k8s.gcr.io` , `registry.k8s.io` , `quay.io`, `ghcr.io`官方镜像脚本\n\n> wget https://raw.githubusercontent.com/anjia0532/gcr.io_mirror/master/pull-k8s-image.sh chmod +x pull-k8s-image.sh\n\n```Shell\nvi pull-images.sh\n###########################################3\n#!/bin/sh\n\nk8s_img=$1\nmirror_img=$(echo ${k8s_img}|\n        sed 's/quay\\.io/anjia0532\\/quay/g;s/ghcr\\.io/anjia0532\\/ghcr/g;s/registry\\.k8s\\.io/anjia0532\\/google-containers/g;s/k8s\\.gcr\\.io/anjia0532\\/google-containers/g;s/gcr\\.io/anjia0532/g;s/\\//\\./g;s/ /\\n/g;s/anjia0532\\./anjia0532\\//g' |\n        uniq)\n\nif [ -x \"$(command -v docker)\" ]; then\n  sudo docker pull ${mirror_img}\n  sudo docker tag ${mirror_img} ${k8s_img}\n  exit 0\nfi\n\nif [ -x \"$(command -v ctr)\" ]; then\n  sudo ctr -n k8s.io image pull docker.io/${mirror_img}\n  sudo ctr -n k8s.io image tag docker.io/${mirror_img} ${k8s_img}\n  exit 0\nfi\n\necho \"command not found:docker or ctr\"\n###############################################\nchmod +x pull-images.sh\n#执行格式\n./pull-images.sh  镜像名\n```\n","tags":["Linux","运维","总结","脚本","shell"],"categories":["Linux运维","脚本"]},{"title":"docker镜像仓库搭建(Harbor)","url":"/yyg/c3b2f9c5/","content":"\n## 任务目标\n\n1. 完成docker镜像仓库的搭建\n\n## 任务平台\n\n1. 物理设备--\n2. 操作系统：openEuler 22.03 LTS SP2\n\n## 部署指南\n\n### 任务一：环境需求\n\n1. 硬件配置\n\n下图列出了部署 Harbor 的最低硬件配置和推荐的硬件配置。\n\n![image-20240323215852040](docker镜像仓库搭建-Harbor/image-20240323215852040.png)\n\n1. 网络端口\n\nHarbor 要求在目标主机上打开以下端口\n\n| 端口 | 协议  | 备注                                            |\n| ---- | ----- | ----------------------------------------------- |\n| 443  | HTTPS | Harbor 门户和核心 API 接受此端口上的 HTTPS 请求 |\n| 4443 | HTTPS | 连接到适用于 Harbor 的 Docker 内容信任服务      |\n| 80   | HTTP  | Harbor 门户和核心 API 接受此端口上的 HTTP 请求  |\n\n### 任务二：基础环境准备\n\n### 任务三：部署\n\n1. #### 下载Harbor\n\n```Shell\nwget https://github.com/goharbor/harbor/releases/download/v2.9.1/harbor-offline-installer-v2.9.1.tgz\n#解压\ntar -vzxf harbor-offline-installer-v2.9.1.tgz\n```\n\n1. #### 安装https证书\n\n   1. #####  简单版\n\n   ```Shell\n   #创建证书目录，并赋予权限\n   mkdir -p /data/cert/\n   chmod  -R 777 /data/cert/\n   cd /data/cert\n   #创建私钥\n   openssl genrsa -des3 -out harbor.key 2048\n   #生成ca证书，ip为本机ip\n   openssl req -sha512 -new \\\n        -subj \"/C=CN/ST=hennan/L=zhengzhou/O=qishi/OU=qishi/CN=images.store.net.crt\" \\\n        -key harbor.key \\\n        -out harbor.csr\n    #备份证书\n    cp harbor.key  harbor.key.org\n    #转化为不带密码的私钥\n    openssl rsa -in harbor.key.org -out harbor.key\n    #使用证书进行签名\n     openssl x509 -req -days 100000  -in harbor.csr -signkey harbor.key -out harbor.crt\n   ```\n\n   1. #####  官方版\n\n      ```Shell\n      #创建存放目录\n      mkdir -p /data/harbor-ca\n      cd /data/harbor-ca\n      ```\n\n      - 生成证书颁发机构证书\n\n      1. 生成 CA 证书私钥\n\n      ```Shell\n      openssl genrsa -out ca.key 4096\n      ```\n\n      1. 生成 CA 证书\n\n      ```Shell\n      #CN后的内容为ip或者域名\n      openssl req -x509 -new -nodes -sha512 -days 3650 \\\n       -subj \"/C=CN/ST=henan/L=zhengzhou/O=qishi/OU=qishi/CN=hub-docker.xxx.net\" \\\n       -key ca.key \\\n       -out ca.crt\n      ```\n\n      - 生成服务器证书\n\n      1. 生成私钥\n\n      ```Shell\n      #域名或ip命名\n      openssl genrsa -out hub-docker.xxx.net.key 4096\n      ```\n\n      1. 生成证书签名请求 （CSR）\n\n      ```Shell\n      openssl req -sha512 -new \\\n          -subj \"/C=CN/ST=henan/L=zhengzhou/O=qishi/OU=qishi/CN=hub-docker.xxx.net\" \\\n          -key hub-docker.xxx.net.key \\\n          -out hub-docker.xxx.net.csr\n      ```\n\n      1. 生成 x509 v3 扩展文件\n\n      ```Shell\n      cat > v3.ext <<-EOF\n      authorityKeyIdentifier=keyid,issuer\n      basicConstraints=CA:FALSE\n      keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\n      extendedKeyUsage = serverAuth\n      subjectAltName = @alt_names\n      \n      [alt_names]\n      DNS.1=hub-docker.xxx.net\n      DNS.2=hub-docker.xxx\n      DNS.3=hub-docker\n      EOF\n      ```\n\n      1. 使用v3.ext生成证书\n\n      ```Shell\n      openssl x509 -req -sha512 -days 3650 \\\n          -extfile v3.ext \\\n          -CA ca.crt -CAkey ca.key -CAcreateserial \\\n          -in hub-docker.xxx.net.csr \\\n          -out hub-docker.xxx.net.crt\n      ```\n\n      - 向 Harbor 和 Docker 提供证书\n        - 将服务器证书和密钥复制到存放 harbor使用证书的目录\n      \n         ```shell\n         #创建存放harbor使用证书的目录\n         mkdir -p /data/cert/\n         #拷贝证书\n          cp hub-docker.xxx.net.crt /data/cert/\n          cp hub-docker.xxx.net.key /data/cert/\n          ```\n        \n        - 转换证书，供docker使用\n        \n          ```shell\n             openssl x509 -inform PEM -in hub-docker.xxx.net.crt -out hub-docker.xxx.net.cert\n             #创建存放docker使用证书目录，命名方式为ip:端口，或者域名\n             mkdir -p /etc/docker/certs.d/hub-docker.xxx.net/\n             #拷贝证书\n             cp hub-docker.xxx.net.cert /etc/docker/certs.d/hub-docker.xxx.net/\n             cp hub-docker.xxx.net.key /etc/docker/certs.d/hub-docker.xxx.net/\n             cp ca.crt /etc/docker/certs.d/hub-docker.xxx.net/\n             #重启docker\n             systemctl restart docker\n          ```\n        \n          \n        \n\n\n1. #### 安装Harbor\n\n修改配置文件\n\n```Shell\ncp harbor.yml.tmpl harbor.yml\nvi harbor.yml\n#################################\nhostname: hub-docker.xxx.net   #修改为本机ip,或者自定义域名\nhttp:\n   port: 80 #端口可自定义\nhttps:   \n   port: 443 #端口可自定义\ncertificate: /data/harbor/harbor-ca/hub-docker.xxx.net.crt #证书路径\nprivate_key: /data/harbor/harbor-ca/hub-docker.xxx.net.key #私钥路径\nharbor_admin_password: Harbor12345 #登录密码\ndata_volume: /data/harbor-data #数据存储目录\n#################################\n```\n\n执行安装\n\n```Shell\n./install.sh  --with-trivy\n```\n\n浏览器输入ip加端口访问(添加hosts后，域名访问)\n\n![image-20240323220017893](docker镜像仓库搭建-Harbor/image-20240323220017893.png)\n\n### 任务四：设置docker login 登录凭证加密（要拉取镜像的主机）\n\n1. 安装pass\n\n```Shell\n #基础环境\n yum install -y make\n#获取pass源码\nwget https://git.zx2c4.com/password-store/snapshot/password-store-1.7.4.tar.xz\n#解压到/usr/local/目录\ntar Jxf password-store-1.7.4.tar.xz -C /usr/local/\n#执行安装\ncd /usr/local/password-store-1.7.4\nmake install\n#验证\n pass version\n```\n\n1. 安装docker-credential-helpers\n\n```Shell\n#安装go基础环境\nyum -y install golang-1.17.3\n#获取docker-credential-helpers源码\nwget https://github.com/docker/docker-credential-helpers/archive/refs/tags/v0.8.0.tar.gz\ntar -xf v0.8.0.tar.gz\n mv docker-credential-helpers-0.8.0 docker-credential-helpers\n cd docker-credential-helpers/\n #使用pass方式安装\n make pass\n cp bin/build/docker-credential-pass /usr/bin/\n chmod +x /usr/bin/docker-credential-pass\n```\n\n1. 修改密码配置文件\n\n```Shell\n#生成密钥对\n gpg --full-generate-key\n #查看密钥\n gpg --list-keys\n ##################################################\n /root/.gnupg/pubring.kbx\n------------------------\npub   rsa3072 2023-11-30 [SC] [有效至：2025-11-29]\n      56CCF64EC289B13B1C0F14CCF2BB16136358AEA4\nuid             [ 绝对 ] qishi <qishi@qq.com>\nsub   rsa3072 2023-11-30 [E] [有效至：2025-11-29]\n##################################################\n #初始化\n pass init 56CCF64EC289B13B1C0F14CCF2BB16136358AEA4\n \n #登录docker仓库，查看登录凭证是否加密\n docker login hub-docker.xxx.net\n cat /root/.docker/config.json\n #######################################\n {\n        \"auths\": {\n                \"hub-docker.xxx.net\": {}\n        },\n        \"credsStore\": \"pass\"\n}\n #######################################\n \n  #创建密码本保存文件目录\n  pass insert docker-credential-helpers/docker-pass-initialized-check\n #查看密码本列表\n  docker-credential-pass list\n  #验证密码本\n  yum install tree -y\n  pass\n  #以实际路径为准\n  pass show docker-credential-helpers/aW1hZ2VzLnN0b3JlLm5ldA==/admin\n```\n\n### 任务五：上传下载镜像（要拉取镜像的主机）\n\n在需要拉取和上传镜像的主机上，修改daemon.json文件，再重启docker\n\n```Shell\nvi /etc/docker/daemon.json\n{\n  \"insecure-registries\": [\"hub-docker.xxx.net\"] #ip或者域名\n}\n\nsystemctl restart docker\n```\n\n拉取和上传镜像\n\n```Shell\n#拉取nginx镜像做测试\ndocker pull nginx\n#重新打上标签，其中test为harbor中存在的项目\ndocker tag nginx:latest 172.20.1.55/test/nginx-test:1.0\ndocker tag nginx:latest hub-docker.xxx.net/test/nginx-test:1.0\n#登录仓库，输入用户名和密码\ndocker login 172.20.1.55\ndocker login hub-docker.xxx.net\nsystemctl restart docker\n#将镜像推送到仓库\ndocker push 172.20.1.55/test/nginx-test:1.0\ndocker push hub-docker.xxx.net/test/nginx-test:1.0\n#拉取镜像\ndocker pull 172.20.1.55/test/nginx-test:1.0\ndocker pull hub-docker.xxx.net/test/nginx-test:1.0\n```\n\n### 任务六：cosign签名镜像（要拉取镜像的主机）\n\n```Shell\nmkdir -p /data/cosign\n#下载，安装cosign\nwget https://github.com/sigstore/cosign/releases/download/v2.2.2/cosign-2.2.2-1.x86_64.rpm\nrpm -ivh cosign-2.2.2-1.x86_64.rpm\n#arm\nwget https://github.com/sigstore/cosign/releases/download/v2.2.2/cosign-2.2.2-1.aarch64.rpm\nrpm -ivh cosign-2.2.2-1.aarch64.rpm\n#验证安装\ncosign version\n#生成密钥\ncosign generate-key-pair\n#对镜像进行加密签名\ncosign sign  --allow-insecure-registry --key /data/cosign/cosign.key images.store.net/test/test-nginx:1.0.0\n\n#解密验证\ncosign  verify  --allow-insecure-registry --key /data/cosign/cosign.pub images.store.net/test/test-nginx:1.0.0\n```\n","tags":["docker","Harbor","容器镜像仓库"],"categories":["Linux运维","容器化","docker","容器管理"]},{"title":"高可用k8s集群部署指南","url":"/yyg/612a796b/","content":"## 任务目标\n\n1. 完成高可用k8s集群安装部署\n\n## 任务平台\n\n1. 物理设备--\n2. 操作系统：openEuler 22.03 LTS SP2\n\n## 部署指南\n\n集群拓扑图\n\n![image-20240323222414360](高可用k8s集群部署指南/image-20240323222414360.png)\n\n### 任务一：配置准备\n\n1. 重命名hostname\n\n```Shell\n# 将10.10.3.121的主机名改为future-k8s-node0\nhostnamectl set-hostname future-k8s-node0 && bash\n# 将10.10.3.122的主机名改为future-k8s-node1\nhostnamectl set-hostname future-k8s-node1 && bash\n# 将10.10.3.123的主机名改为future-k8s-node2\nhostnamectl set-hostname future-k8s-node2 && bash\n# 将10.10.3.124的主机名改为future-k8s-node3\nhostnamectl set-hostname future-k8s-node3 && bash\n```\n\n1. 安装前的配置修改\n\n```Shell\n# 关闭防火墙\nsystemctl stop firewalld\nsystemctl disable firewalld\nfirewall-cmd --state\n \n# selinux永久关闭\nsetenforce 0\n sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\ncat /etc/selinux/config\n\n# swap永久关闭\nswapoff --all\nswapoff -a\nsed -ri 's/.*swap.*/#&/' /etc/fstab\ncat /etc/fstab\n\n# 添加hosts\ncat >> /etc/hosts << EOF\n10.10.3.121 future-k8s-node0\n10.10.3.122 future-k8s-node1\n10.10.3.123 future-k8s-node2\n10.10.3.124 future-k8s-node3\n10.10.3.125 future-k8s-vip\nEOF\n#查看\ncat /etc/hosts\n\n\n# 添加网桥过滤及内核转发配置文件\ncat > /etc/sysctl.d/k8s.conf << EOF\n net.bridge.bridge-nf-call-ip6tables = 1\n net.bridge.bridge-nf-call-iptables = 1\n net.ipv4.ip_forward = 1\nEOF\n# 查看\ncat /etc/sysctl.d/k8s.conf\n# 加载br_netfilter模块\nmodprobe br_netfilter\n# 查看是否加载\nlsmod | grep br_netfilter\n# 加载网桥过滤及内核转发配置文件\nsysctl -p /etc/sysctl.d/k8s.conf\n\n#同步时间\nyum install ntp -y\nsystemctl start ntpd\nsystemctl enable ntpd\nyum install chrony  -y\nsystemctl start chrony\nsystemctl enable chronyd\n#修改配置，添加内容\necho \"\nserver 10.10.3.70 iburst\nallow 10.10.3.0/24\n\" >> /etc/chrony.conf\ntimedatectl set-ntp true\nsystemctl restart chronyd\ntimedatectl status\ndate\n```\n\n1. 安装ipset及ipvsadm\n\n```Shell\n # 安装ipset及ipvsadm\n yum -y install ipset ipvsadm\n 配置ipvsadm模块加载方式\n # 添加需要加载的模块\necho ' #!/bin/bash\n modprobe -- ip_vs\n modprobe -- ip_vs_rr\n modprobe -- ip_vs_wrr\n modprobe -- ip_vs_sh\n modprobe -- nf_conntrack\n' > /etc/sysconfig/modules/ipvs.modules\n#查看\ncat /etc/sysconfig/modules/ipvs.modules\n # 授权、运行、检查是否加载\nchmod 755 /etc/sysconfig/modules/ipvs.modules \nbash /etc/sysconfig/modules/ipvs.modules \nlsmod | grep -e ip_vs -e nf_conntrack\n\n#重启\nreboot\n```\n\n配置准备完成后，所有节点都需重启\n\n### 任务二：安装docker\n\n1. 配置Docker CE的yum存储库。打开`docker-ce.repo`的文件，并将以下内容复制到文件中：\n\n```Shell\necho '\n[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/centos/7/$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/centos/gpg ' > /etc/yum.repos.d/docker-ce.repo\n```\n\n保存并退出文件。\n\n1. 安装Docker CE。运行以下命令来安装Docker CE：\n\n```Shell\n yum -y install docker-ce docker-ce-cli  containerd.io\n#启动docker并设置开机自启\nsystemctl start docker  \nsystemctl enable docker\n#查看版本\ndocker -v\ndocker compose version\n```\n\n1. Docker配置修改，设置cgroup驱动，使用systemd，配置修改为如下。\n\n```Shell\n#将配置写入daemon.json文件\necho '{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"data-root\": \"/data/docker\"\n} ' > /etc/docker/daemon.json\n#查看\ncat /etc/docker/daemon.json\nsystemctl daemon-reload\nsystemctl restart docker\ndocker info\n```\n\n1. 创建所需目录\n\n```Shell\nmkdir  cri-dockerd   calico dashboard  docker  metrics-server  script  ingress-nginx\n```\n\n### 任务三：安装cri-dockerd （k8s 1.24及以上版本）\n\n```Shell\ncd /data/cri-dockerd\n# 下载cri-dockerd安装包\nwget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.4/cri-dockerd-0.3.4-3.el8.x86_64.rpm\n# 安装cri-dockerd\nrpm -ivh cri-dockerd-0.3.4-3.el8.x86_64.rpm\ndocker pull registry.aliyuncs.com/google_containers/pause:3.9\n# 修改镜像地址为国内，否则kubelet拉取不了镜像导致启动失败\nsed -i.bak 's|ExecStart=.*$|ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9|g' /usr/lib/systemd/system/cri-docker.service\ncat /usr/lib/systemd/system/cri-docker.service\n# 启动cri-dockerd\nsystemctl daemon-reload \nsystemctl start cri-docker.service\nsystemctl enable cri-docker.service\n```\n\n### 任务四：安装高可用组件\n\n部署高可用集群需要安装**keepalived和haproxy，实现****master****节点高可用，****在各master节点操作**\n\n1. 安装keepalived与haproxy\n\n```Shell\nyum install keepalived haproxy -y\n```\n\n1. 备份keepalived与haproxy配置文件\n\n```Shell\ncp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\ncp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n```\n\n1. 修改各master节点的`/etc/keepalived/keepalived.conf`文件\n   1. future-k8s-node0\n   2. ```Shell\n      echo '\n      global_defs {\n         router_id k8s\n      }\n      \n      vrrp_script check_haproxy {\n          script \"killall -0 haproxy\"\n          interval 3\n          weight -2\n          fall 10\n          rise 2\n      }\n      \n      vrrp_instance VI_1 {\n          state MASTER  #主节点 则为MASTER ,其他则为 BACKUP\n          interface ens192  #网卡名称\n          virtual_router_id 51\n          priority 250   #优先级\n          nopreempt   #设置非抢占模式\n          advert_int 1\n          authentication {\n              auth_type PASS\n              auth_pass ceb1b3ec013d66163d6ab\n          }\n          virtual_ipaddress {\n              10.10.3.125/24   #虚拟ip\n          }\n          track_script {\n              check_haproxy\n          }\n      }    \n      ' > /etc/keepalived/keepalived.conf\n      ```\n\n   3. future-k8s-node1\n   4. ```Shell\n      echo '\n      global_defs {\n         router_id k8s\n      }\n      \n      vrrp_script check_haproxy {\n          script \"killall -0 haproxy\"\n          interval 3\n          weight -2\n          fall 10\n          rise 2\n      }\n      \n      vrrp_instance VI_1 {\n          state BACKUP  #主节点 则为MASTER ,其他则为 BACKUP\n          interface ens192  #网卡名称\n          virtual_router_id 51\n          priority 200   #优先级\n          nopreempt   #设置非抢占模式\n          advert_int 1\n          authentication {\n              auth_type PASS\n              auth_pass ceb1b3ec013d66163d6ab\n          }\n          virtual_ipaddress {\n              10.10.3.125/24   #虚拟ip\n          }\n          track_script {\n              check_haproxy\n          }\n      }    \n      ' > /etc/keepalived/keepalived.conf\n      ```\n\n   5. future-k8s-node2\n   6. ```Shell\n      echo '\n      global_defs {\n         router_id k8s\n      }\n      \n      vrrp_script check_haproxy {\n          script \"killall -0 haproxy\"\n          interval 3\n          weight -2\n          fall 10\n          rise 2\n      }\n      \n      vrrp_instance VI_1 {\n          state BACKUP  #主节点 则为MASTER ,其他则为 BACKUP\n          interface ens192  #网卡名称\n          virtual_router_id 51\n          priority 150   #优先级\n          nopreempt   #设置非抢占模式\n          advert_int 1\n          authentication {\n              auth_type PASS\n              auth_pass ceb1b3ec013d66163d6ab\n          }\n          virtual_ipaddress {\n              10.10.3.125/24   #虚拟ip\n          }\n          track_script {\n              check_haproxy\n          }\n      }    \n      ' > /etc/keepalived/keepalived.conf\n      ```\n2. 修改各master节点的`/etc/haproxy/haproxy.cfg`文件，（三个master节点的配置文件相同）\n\n```Shell\necho \"\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n#---------------------------------------------------------------------\n# kubernetes apiserver frontend which proxys to the backends\n#---------------------------------------------------------------------\nfrontend kubernetes-apiserver\n    mode                 tcp\n    bind                 *:16443 #高可用监控端口，初始化k8s集群时会用\n    option               tcplog\n    default_backend      kubernetes-apiserver\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\nbackend kubernetes-apiserver\n    mode        tcp\n    balance     roundrobin\n    server      future-k8s-node0   10.10.3.121:6443 check\n    server      future-k8s-node1   10.10.3.122:6443 check\n    server      future-k8s-node2   10.10.3.123:6443 check\n\n#---------------------------------------------------------------------\n# collection haproxy statistics message\n#---------------------------------------------------------------------\nlisten stats\n    bind                 *:1080\n    stats auth           admin:awesomePassword\n    stats refresh        5s\n    stats realm          HAProxy\\ Statistics\n    stats uri            /admin?stats\n\n\" > /etc/haproxy/haproxy.cfg\n```\n\n1. 启动（各master节点按顺序启动）\n\n```Shell\n#启动keepalived  \nsystemctl enable keepalived  && systemctl start keepalived  \n#启动haproxy \nsystemctl enable haproxy && systemctl start haproxy\nsystemctl status keepalived\nsystemctl status haproxy\n```\n\n1. 在future-k8s-node0查看绑定的vip地址  \n\n> ip add 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever 2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000    link/ether 00:50:56:9a:eb:48 brd ff:ff:ff:ff:ff:ff    inet 10.10.3.121/24 brd 10.10.3.255 scope global noprefixroute ens192       valid_lft forever preferred_lft forever    inet 10.10.3.125/24 scope global ens192       valid_lft forever preferred_lft forever    inet6 fe80::250:56ff:fe9a:eb48/64 scope link noprefixroute       valid_lft forever preferred_lft forever\n\n### 任务五：部署k8s集群\n\n1. #### 添加yum软件源\n\n```Shell\ncat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n1. #### 安装kubeadm，kubelet和kubectl\n\n```Shell\n# 安装kubelet、kubeadm、kubectl\nyum install -y kubelet-1.27.0 kubeadm-1.27.0 kubectl-1.27.0 --disableexcludes=kubernetes\n\n#将cgroup改为systemd\necho 'KUBELET_EXTRA_ARGS=\"--cgroup-driver=systemd\"' > /etc/sysconfig/kubelet\n# 查看\ncat /etc/sysconfig/kubelet\n# 设置开机启动\nsystemctl start kubelet.service\nsystemctl enable kubelet.service\nsystemctl status kubelet.service\n\n#查看版本\n kubeadm version\n kubelet --version\n kubectl version \n```\n\n1. #### 初始化k8s集群（future-k8s-node0节点）\n\n   #####  方式一：使用配置文件初始化\n\n   > 1. 导出默认配置文件 (可选)\n   >\n   > 暂时无法在飞书文档外展示此内容\n\n   1. 配置文件\n\n   ```Shell\n   echo '\n   apiVersion: kubeadm.k8s.io/v1beta3\n   kind: InitConfiguration\n   localAPIEndpoint:\n     advertiseAddress: 10.10.3.125  #虚拟ip\n     bindPort: 6443\n   nodeRegistration:\n     criSocket: unix:///var/run/cri-dockerd.sock\n   ---\n   apiServer:\n     certSANs:    #master节点与对应主机名\n       - future-k8s-node0\n       - future-k8s-node1\n       - future-k8s-node2\n       - future-k8s-vip\n       - 10.10.3.121\n       - 10.10.3.122\n       - 10.10.3.123\n       - 10.10.3.125\n       - 127.0.0.1\n     timeoutForControlPlane: 4m0s\n   apiVersion: kubeadm.k8s.io/v1beta3\n   certificatesDir: /etc/kubernetes/pki\n   clusterName: kubernetes\n   controlPlaneEndpoint: \"future-k8s-vip:16443\" #虚拟ip及高可用配置的端口号\n   controllerManager: {}\n   dns: {}\n   etcd:\n     local:\n       dataDir: /var/lib/etcd\n   imageRepository: registry.aliyuncs.com/google_containers\n   kind: ClusterConfiguration\n   kubernetesVersion: 1.28.0\n   networking:\n     dnsDomain: cluster.local\n     podSubnet: 10.244.0.0/16\n     serviceSubnet: 10.96.0.0/12\n   scheduler: {}\n   ' > /data/script/kubeadm-config.yaml\n   ```\n\n   1. 集群初始化\n\n   ```Shell\n   kubeadm init --config kubeadm-config.yaml --upload-certs\n   \n   mkdir -p $HOME/.kube\n   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n   sudo chown $(id -u):$(id -g) $HOME/.kube/config\n   ```\n\n   #####  方式二：使用命令初始化\n\n   1. 部署master节点，在10.10.3.121执行，初始化master节点\n\n   ```Shell\n   kubeadm init \\\n     --apiserver-advertise-address=10.10.3.121 \\\n     --image-repository registry.aliyuncs.com/google_containers \\\n     --kubernetes-version v1.27.0 \\\n     --control-plane-endpoint=future-k8s-vip:16443 \\  #虚拟ip（未定）\n     --control-plane-endpoint=future-k8s-vip \\  #虚拟ip（未定）\n     --service-cidr=10.96.0.0/12 \\\n     --pod-network-cidr=10.244.0.0/16 \\\n     --cri-socket=unix:///var/run/cri-dockerd.sock \\\n     --ignore-preflight-errors=all \n     \n     \n   mkdir -p $HOME/.kube\n   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n   sudo chown $(id -u):$(id -g) $HOME/.kube/config\n   ```\n\n   1. 配置ssh免密码\n\n   ```Shell\n   #在10.10.3.121上生成公钥，复制到其他master节点上\n   ssh-keygen -t rsa\n   ssh-copy-id 10.10.3.122\n   ssh-copy-id 10.10.3.123\n   ```\n\n   1. 将10.10.3.121上的证书拷贝到其他master节点\n\n   ```Shell\n   #在其他master节点创建证书存放目录\n   cd /root && mkdir -p /etc/kubernetes/pki/etcd &&mkdir -p ~/.kube/\n   \n   #将future-k8s-node0的证书复制到future-k8s-node1\n   scp /etc/kubernetes/pki/ca.crt 10.10.3.122:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/ca.key 10.10.3.122:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/sa.key 10.10.3.122:/etc/kubernetes/pki/\n   scp /etc/kubernetes/pki/sa.pub 10.10.3.122:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.crt 10.10.3.122:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.key 10.10.3.122:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/etcd/ca.crt 10.10.3.122:/etc/kubernetes/pki/etcd/\n   scp /etc/kubernetes/pki/etcd/ca.key 10.10.3.122:/etc/kubernetes/pki/etcd/\n   \n   #将future-k8s-node0的证书复制到future-k8s-node1\n   scp /etc/kubernetes/pki/ca.crt 10.10.3.123:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/ca.key 10.10.3.123:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/sa.key 10.10.3.123:/etc/kubernetes/pki/\n   scp /etc/kubernetes/pki/sa.pub 10.10.3.123:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.crt 10.10.3.123:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.key 10.10.3.123:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/etcd/ca.crt 10.10.3.123:/etc/kubernetes/pki/etcd/\n   scp /etc/kubernetes/pki/etcd/ca.key 10.10.3.123:/etc/kubernetes/pki/etcd/\n   ```\n\n1. #### 初始化其他master节点\n\n```Shell\n kubeadm join future-k8s-vip:16443 --token ysl0xr.knx79yu06cldwiy1         --discovery-token-ca-cert-hash sha256:5dd8de94e08a560c7c2424dde0719a9f4e6ac4e5e5fe538ebbab0cbc5866b000         --control-plane  --cri-socket=unix:///var/run/cri-dockerd.sock\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n1. #### 初始化node节点\n\n```Shell\n kubeadm join 10.10.3.121:6443 --token pzyo37.oaaqt1nrw3u7ijuj   --discovery-token-ca-cert-hash sha256:b8067f74af04b63399af1de28644223178e5d63e8258c25d465e78aca515e887 --cri-socket=unix:///var/run/cri-dockerd.sock\n```\n\n1. #### 设置master节点允许调度POD （可选）\n\n默认配置下Kubernetes不会将Pod调度到Master节点。如果希望将k8s-master也当作Node使用，需去除污点，开启调度。\n\n```Shell\n#查看默认配置的污点\nkubectl describe node future-k8s-node2 |grep Taints\n```\n\n> Taints:             node-role.kubernetes.io/control-plane:NoSchedule\n\n```Shell\n#去除污点\nkubectl taint nodes future-k8s-node2 node-role.kubernetes.io/control-plane-\n```\n\n添加woker标记\n\n```Shell\n#添加worker标记\nkubectl label nodes future-k8s-node2 node-role.kubernetes.io/worker=\n#删除worker标记\nkubectl label nodes future-k8s-node2 node-role.kubernetes.io/worker-\n```\n\n### 任务六：安装网络插件(master)\n\n安装calico\n\n```Shell\nmkdir /data/calico\ncd  /data/calico\nwget https://docs.tigera.io/archive/v3.25/manifests/calico.yaml\n#修改calico.yaml找到CALICO_IPV4POOL_CIDR\nvi calico.yaml\n##############修改内容###################\n value: \"10.244.0.0/16\"\n ##############修改内容###################\n #在master节点上安装calico\n kubectl apply -f calico.yaml\n```\n\n查看节点状态\n\n```Shell\n# 查看所有的节点\nkubectl get nodes\nkubectl get nodes -o wide\n#查看集群健康情况\n kubectl get cs\n```\n\n### 任务七：安装nginx进行测试\n\n```Shell\n# 创建Nginx程序\nkubectl create deployment nginx --image=nginx\n# 开放80端口\nkubectl expose deployment nginx --port=80 --type=NodePort\n# 查看pod状态\nkubectl get pod\n#查看service状态\nkubectl get service\n##########################################################################\nNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        5d1h\nnginx        NodePort    10.98.221.224   <none>        80:32743/TCP   23s\n##########################################################################\n# 访问网页测试(端口号以查看service状态得到的为准)\nhttp://10.10.3.121:32743/\n```\n\n### 任务八：安装**Dashboard界面**\n\n1. 下载yaml文件\n\n```Shell\n#创建存放目录\nmkdir dashboard\ncd dashboard/\n#2.7\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n```\n\n1. 修改yaml文件\n\n```Shell\nvi recommended.yaml\n#将副本设置为2\n#################修改内容#######################\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 32009   #添加这一行，注意缩进对齐\n  selector:\n    k8s-app: kubernetes-dashboard\n  type: NodePort          #添加这一行，注意缩进对齐\n  #################修改内容#######################\n```\n\n1. 应用安装，查看pod和svc\n\n```Shell\n#安装\nkubectl apply -f recommended.yaml\n#查看pod和svc\nkubectl get pod,svc -o wide -n kubernetes-dashboard\n#########################################################\nNAME                                             READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATES\npod/dashboard-metrics-scraper-5cb4f4bb9c-mg569   0/1     ContainerCreating   0          9s    <none>   node1   <none>           <none>\npod/kubernetes-dashboard-6967859bff-2968p        0/1     ContainerCreating   0          9s    <none>   node1   <none>           <none>\n\nNAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE   SELECTOR\nservice/dashboard-metrics-scraper   ClusterIP   10.100.129.191   <none>        8000/TCP        9s    k8s-app=dashboard-metrics-scraper\nservice/kubernetes-dashboard        NodePort    10.106.130.53    <none>        443:31283/TCP   9s    k8s-app=kubernetes-dashboard\n########################################################\n```\n\n使用所查看的svc，所提供的端口访问**Dashboard**\n\n1. 创建dashboard服务账户\n\n```Shell\n#创建一个admin-user的服务账户并与集群绑定\nvi dashboard-adminuser.yaml\n##################内容####################\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n  \n---\n# 创建密钥，获取服务帐户的长期持有者令牌\napiVersion: v1\nkind: Secret\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"admin-user\"\ntype: kubernetes.io/service-account-token\n  ##################内容####################\n \n  #执行生效\n  kubectl apply -f dashboard-adminuser.yaml\n```\n\n1. 登录方式\n\n方案一：获取长期可用token\n\n```Shell\n#将其保存在/data/dashboard/的admin-user.token文件中\ncd /data/dashboard/\nkubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d > admin-user.token \n```\n\n获取长期可用token脚本\n\n```Shell\n#!/bin/bash\n#作者：云\n#############描述#############\n:<<!\n获取长期可用token脚本\n将token存放在admin-user.token文件中\n!\n#############描述#############\nkubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d > admin-user.token\n\necho -e \"\\033[1;32m创建token成功，请在admin-user.token文件中查看\\033[m\"\n```\n\n方案二：使用使用 Kubeconfig 文件登录\n\n```Shell\n #定义 token 变量\n DASH_TOCKEN=$(kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d)\n #设置 kubeconfig 集群条目\n kubectl config set-cluster kubernetes --server=10.10.3.121:6433 --kubeconfig=/root/dashbord-admin.conf\n #设置 kubeconfig 用户条目\n kubectl config set-credentials admin-user --token=$DASH_TOCKEN --kubeconfig=/root/dashbord-admin.conf\n #设置 kubeconfig 上下文条目\n kubectl config set-context admin-user@kubernetes --cluster=kubernetes --user=admin-user --kubeconfig=/root/dashbord-admin.conf\n #设置 kubeconfig 当前上下文\n kubectl config use-context admin-user@kubernetes  --kubeconfig=/root/dashbord-admin.conf\n```\n\n将生成的dashbord-admin.conf文件放到本地主机上，登录时选择`Kubeconfig`选项，选择 kubeconfig 文件登录\n\n### 任务九：安装metrics-server\n\n下载部署文件\n\n```Shell\nwget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml -O metrics-server-components.yaml\n```\n\n修改yaml文件中的Deployment内容\n\n```Shell\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        k8s-app: metrics-server\n    spec:\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=4443\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls  #添加\n        image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.4 #修改\n        imagePullPolicy: IfNotPresent\n\n#安装\nkubectl apply -f metrics-server-components.yaml\n```\n\n查看metrics-server的pod状态\n\n```Shell\nkubectl get pods --all-namespaces | grep metrics\n```\n\n等待一些时间，查看查看各类监控图像已成功显示。\n\n![image-20240323222536472](高可用k8s集群部署指南/image-20240323222536472-1711203940377-3.png)\n\n### 任务十：kubectl命令自动补全\n\n```Shell\nyum -y install bash-completion\nsource /usr/share/bash-completion/bash_completion\necho 'source <(kubectl completion bash)' >>  ~/.bashrc\nbash\n```\n\n### 任务十一：ingress-nginx控制器安装\n\n```Shell\n#下载yaml文件\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml\n#修改yaml文件中拉取镜像的地址\n#####################修改内容######################\nwilldockerhub/ingress-nginx-controller:v1.0.0\nhzde0128/kube-webhook-certgen:v1.0\n#####################修改内容######################\n#修改Deployment修改成DaemonSet\n#修改网络模式为host network\n#####################修改内容######################\ntemplate:\n  spec:\n    hostNetwork: true\n    dnsPolicy: ClusterFirstWithHostNet\n    tolerations:  #使用亲和性配置可在所有节点部署\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n     nodeSelector:\n          kubernetes.io/os: linux\n          custem/ingress-controller-ready: 'true'\n      containers:\n        - name: controller\n#####################修改内容######################\n#为工作节点设置标签（必需）\nkubectl label nodes future-k8s-node0 custem/ingress-controller-ready=true\nkubectl label nodes future-k8s-node1 custem/ingress-controller-ready=true\nkubectl label nodes future-k8s-node2 custem/ingress-controller-ready=true\nkubectl label nodes future-k8s-node3 custem/ingress-controller-ready=true\n\n#安装\nkubectl apply -f deploy.yaml\n\n#查看状态\nkubectl get pods -n ingress-nginx\n################状态##################\nNAME                                       READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-2lz4v       0/1     Completed   0          5m46s\ningress-nginx-admission-patch-c6896        0/1     Completed   0          5m46s\ningress-nginx-controller-7575fb546-q29qn   1/1     Running     0          5m46s\n```\n\n### 任务十二：配置**Dashboard代理**\n\n```Shell\necho '\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: k8s-dashboard\n  namespace: kubernetes-dashboard\n  labels:\n    ingress: k8s-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /  #重写路径\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"  #http自动转https\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n\nspec:\n  ingressClassName: nginx \n  rules:\n    - host: k8s.yjs.51xueweb.cn\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard\n                port:\n                  number: 443\n' > /data/dashboard/dashboard-ingress.yaml\n```","tags":["docker","k8s","Kubeadm","k8s集群","高可用"],"categories":["容器化","k8s"]},{"title":"Kubeadm方式搭建K8S集群","url":"/yyg/12690776/","content":"### 环境准备\n\n在虚拟机里安装三台centos 7.x操作系统\n\n配置系统名称和ip地址(同一网段)如：\n\n| 角色   | IP             |\n| ------ | -------------- |\n| master | 192.168.66.100 |\n| node1  | 192.168.66.101 |\n| node2  | 192.168.66.102 |\n\n三台机器都要执行的命令:（使用xshell执行命令更方便）\n\n```\n# 关闭防火墙\nsystemctl stop firewalld\nsystemctl disable firewalld\n\n# selinux永久关闭\nsed -i 's/enforcing/disabled/' /etc/selinux/config\n\n# swap永久关闭\nsed -ri 's/.*swap.*/#&/' /etc/fstab\n\n# 添加hosts\ncat >> /etc/hosts << EOF\n192.168.66.100 master\n192.168.66.101 node1\n192.168.66.102 node2\nEOF\n\n# 将桥接的IPv4流量传递到iptables的链\ncat > /etc/sysctl.d/k8s.conf << EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\n# 生效\nsysctl --system  \n\n# 时间同步\nyum install ntpdate -y\nntpdate time.windows.com\n```\n\n![image-20230516173905682](Kubeadm方式搭建K8S集群/image-20230516173905682.png)\n\n### 安装Docker（三台机器都需要安装）\n\n配置Docker的阿里yum源\n\n```\ncat >/etc/yum.repos.d/docker.repo<<EOF\n[docker-ce-edge]\nname=Docker CE Edge - \\$basearch\nbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/\\$basearch/edge\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg\nEOF\n```\n\n以yum方式安装docker\n\n```\n# yum安装\nyum -y install docker-ce\n\n# 查看docker版本\ndocker --version  \n\n# 启动docker\nsystemctl enable docker\nsystemctl start docker\n```\n\n![image-20230516180634916](Kubeadm方式搭建K8S集群/image-20230516180634916.png)\n\n配置docker的镜像源\n\n```\ncat >> /etc/docker/daemon.json << EOF\n{\n  \"registry-mirrors\": [\"https://b9pmyelo.mirror.aliyuncs.com\"]\n}\nEOF\n```\n\n重启docker\n\n```\nsystemctl restart docker\n```\n\n###  添加kubernetes软件源\n\n配置yum的k8s软件源\n\n```\ncat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n### 安装kubeadm，kubelet和kubectl\n\n由于版本更新频繁，这里指定版本号部署：\n\n```\n# 安装kubelet、kubeadm、kubectl，同时指定版本\nyum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0\n# 设置开机启动\nsystemctl enable kubelet\n```\n\n### 部署master节点\n\n在master节点执行(此处的192.168.66.100为小编master机器的ip，需更换)\n\n```\nkubeadm init --apiserver-advertise-address=192.168.66.100 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.18.0 --service-cidr=10.96.0.0/12  --pod-network-cidr=10.244.0.0/16\n```\n\n![image-20230516181339201](Kubeadm方式搭建K8S集群/image-20230516181339201.png)\n\n等待其出现成功提示\n\n![image-20230516181608215](Kubeadm方式搭建K8S集群/image-20230516181608215.png)\n\n然后按提示在master节点上输入提示命令\n\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n![image-20230516181720076](Kubeadm方式搭建K8S集群/image-20230516181720076.png)\n\n按照master生成的代码向集群添加新节点(node1和node2上的操作)\n\n在node1和node2上执行在master上产生的kubeadm join命令：（根据实际而定，下面的是小编机器上的，仅供参考）\n\n![image-20230516182024786](Kubeadm方式搭建K8S集群/image-20230516182024786.png)\n\n```\nkubeadm join 192.168.66.100:6443 --token 7fqt6v.729wvdcjmgivns7y \\\n    --discovery-token-ca-cert-hash sha256:ef79029853fa3c5454cbfc5273a636c843db0ab96e4592467b8a1490b6b6d3c6 \n```\n\n我们就可以去Master节点 执行下面命令查看情况\n\n```\nkubectl get node\n```\n\n![image-20230516183426649](Kubeadm方式搭建K8S集群/image-20230516183426649.png)\n\n###  部署CNI网络插件\n\n上面的状态还是NotReady，下面我们需要网络插件，来进行联网访问\n\n```\n# 添加\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n![image-20230516183609956](Kubeadm方式搭建K8S集群/image-20230516183609956.png)\n\n```\n# 查看状态 \nkubectl get pods -n kube-system\n```\n\n![image-20230516183655302](Kubeadm方式搭建K8S集群/image-20230516183655302.png)\n\n此时还有未处于running状态的pod，再等些时间，再查看状态，全部变为running状态时，执行下面命令查看:\n\n```\nkubectl get node\n```\n\n![image-20230516184424536](Kubeadm方式搭建K8S集群/image-20230516184424536.png)\n\n运行完成后，此时可以发现，已经变成了Ready状态。若还存在NotReady状态的节点，再等些时间，然后执行`kubectl get pods -n kube-system`命令，若状态为running，再执行`kubectl get node`查看即可\n\n###  测试kubernetes集群\n\n在Kubernetes集群中创建一个pod，验证是否正常运行：\n\n```\n# 下载nginx \nkubectl create deployment nginx --image=nginx\n# 查看状态\nkubectl get pod\n```\n\n出现Running状态时，表示已经成功运行了\n\n![image-20230516185059052](Kubeadm方式搭建K8S集群/image-20230516185059052.png)\n\n下面我们就需要将端口暴露出去，让其它外界能够访问\n\n```\n# 暴露端口\nkubectl expose deployment nginx --port=80 --type=NodePort\n# 查看一下对外的端口\nkubectl get pod,svc\n```\n\n能够看到，我们已经成功暴露了 80端口 到 30374上\n\n![image-20230516185150167](Kubeadm方式搭建K8S集群/image-20230516185150167.png)\n\n在浏览器上，访问如下地址(任一一个节点的ip加上面显示的端口)\n\n```\nhttp://192.168.66.102:30374/\n```\n\n发现我们的nginx已经成功启动了\n\n![image-20201113204056851](Kubeadm方式搭建K8S集群/image-20201113204056851.png)","tags":["docker","k8s","Kubeadm","k8s集群"],"categories":["容器化","k8s"]},{"title":"ceph集群部署指南","url":"/yyg/3fc2a251/","content":"## 任务目标\n\n1. 完成ceph集群部署\n\n## 任务平台\n\n1. 物理设备--\n2. 操作系统：openEuler 22.03 LTS SP2\n\n## 部署指南\n\n### 任务一：配置准备\n\n1. 重命名hostname\n\n```Shell\n# 将10.10.3.117的主机名改为future-ceph-node0\nhostnamectl set-hostname future-ceph-node0 && bash\n# 将10.10.3.118的主机名改为future-ceph-node1\nhostnamectl set-hostname future-ceph-node1 && bash\n# 将10.10.3.119的主机名改为future-ceph-node2\nhostnamectl set-hostname future-ceph-node2 && bash\n# 将10.10.3.120的主机名改为future-ceph-node3\nhostnamectl set-hostname future-ceph-node3 && bash\n```\n\n1. 安装前的配置修改\n\n```Shell\n# 关闭防火墙\nsystemctl stop firewalld\nsystemctl disable firewalld\nfirewall-cmd --state\n \n# selinux永久关闭\nsetenforce 0\n sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\ncat /etc/selinux/config\n\n\n# 添加hosts\ncat >> /etc/hosts << EOF\n10.10.3.117 future-ceph-node0\n10.10.3.118 future-ceph-node1\n10.10.3.119 future-ceph-node2\n10.10.3.120 future-ceph-node3\nEOF\n#查看\ncat /etc/hosts\n\n\n#同步时间\nyum install ntp -y\nsystemctl start ntpd\nsystemctl enable ntpd\nyum install chrony  -y\nsystemctl start chronyd\nsystemctl enable chronyd\n#修改配置，添加内容\necho \"\nserver 10.10.3.70 iburst\nallow 10.10.3.0/24\n\" >> /etc/chrony.conf\ntimedatectl set-ntp true\nsystemctl restart chronyd\ntimedatectl status\ndate\n\n添加节点SSH互相通信\nssh-keygen -t rsa\nssh-copy-id 10.10.3.118\nssh-copy-id 10.10.3.119\nssh-copy-id 10.10.3.120\n```\n\n### 任务二：部署Ceph作为分布式存储\n\n配置下载源\n\n```Shell\necho \"[ceph]\nname=ceph\nbaseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/x86_64/\ngpgcheck=0\npriority=1\n\n[ceph-noarch]\nname=cephnoarch\nbaseurl=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch/\ngpgcheck=0\npriority=1\n\n[ceph-source]\nname=Ceph source packages\nbaseurl=http://mirrors.aliyun.com/ceph/rpm-mimic/el7/SRPMS/\nenabled=0\ngpgcheck=1\ntype=rpm-md\ngpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc\npriority=1\n\" >/etc/yum.repos.d/ceph.repo\n```\n\n下载python2\n\n1. 安装[zlib](https://so.csdn.net/so/search?q=zlib&spm=1001.2101.3001.7020)库，不然安装pip时会报错（还要重新编译python）\n\n```Plain\n yum -y install zlib*\n```\n\n1. 安装 GCC 包，如果没有安装 GCC，请使用以下命令进行安装\n\n```Shell\nyum -y install gcc openssl-devel bzip2-devel\n```\n\n1. 下载Python-2.7.18\n\n```Plain\n cd /usr/src\n yum -y install wget\n wget https://www.python.org/ftp/python/2.7.18/Python-2.7.18.tgz\n tar xzf Python-2.7.18.tgz\n```\n\n1. 在编译之前还需要在安装源文件中修改Modules/Setup.dist文件，将注释去掉\n\n```Shell\n#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz\n```\n\n1.  编译Python-2.7.18（`make altinstall`用于防止替换默认的 python 二进制文件 /usr/bin/python）\n\n```Shell\ncd /usr/src/Python-2.7.18\n./configure --enable-optimizations\nyum install -y make\nmake altinstall\n```\n\n不要覆盖或链接原始的 Python 二进制文件，这可能会损坏系统\n\n1. 设置环境变量\n\n```Shell\nvi /etc/profile\nexport PYTHON_HOME=/usr/local/\nPATH=$PATH:$PYTHON_HOME/bin\nsource /etc/profile\n```\n\n1. 下载安装pip的脚本\n\n```Shell\ncurl \"https://bootstrap.pypa.io/pip/2.7/get-pip.py\" -o \"get-pip.py\"\n```\n\n1. 运行下载安装脚本\n\n```Shell\npython2.7 get-pip.py \n```\n\n下载ceph\n\n```Shell\n#future-ceph-node0下载\npip2 install ceph-deploy\nyum install -y ceph ceph-radosgw\n#其他节点下载\nyum install -y ceph ceph-radosgw\n#检查安装包是否完整\nrpm -qa |egrep -i \"ceph|rados|rbd\"\n```\n\n#### 部署ceph集群\n\n##### 管理节点\n\n###### 部署Monitor\n\n1. 创建配置文件目录，并创建配置文件\n\n```Properties\nmkdir /etc/ceph/\ntouch /etc/ceph/ceph.conf\n```\n\n1. 为集群生成一个FSDI：\n\n```Ruby\nuuidgen\n38ac0084-40b7-4c93-b561-a16d6a6478c5\n```\n\n1. 集群创建一个钥匙串，为Monitor 服务创建一个密钥：\n\n```Ruby\nceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'\n```\n\n1. 创建一个管理员钥匙串，生成一个client.admin用户，并将此用户添加到钥匙串中：\n\n```Go\nceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'\n```\n\n1. 创建bootstrap-osd钥匙串，将client.bootstrap-osd 用户添加到此钥匙串中：\n\n```TypeScript\nceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd'\n```\n\n1. 将生成的key加入ceph.mon.keyring.\n\n```TypeScript\nceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring\n\nceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring\n```\n\n1. 使用主机名和IP地址以及FSID生成monitor map:\n\n```YAML\nmonmaptool --create --add future-ceph-node0 10.10.3.117 --fsid 79fd2206-39ca-4ec4-9cd2-96e065c6361e /tmp/monmap\n```\n\n1. 创建mon的目录,使用 `集群名称-主机名`的形式：\n\n```Shell\n mkdir  /var/lib/ceph/mon/ceph-future-ceph-node0\n```\n\n1. 填入第一个mon守护进程的信息:\n\n```TOML\nceph-mon --mkfs -i future-ceph-node0 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring\n```\n\n1. 配置/etc/ceph/ceph.conf文件：\n\n```Shell\ncat /etc/ceph/ceph.conf\n################################################\n[global]\nfsid = 79fd2206-39ca-4ec4-9cd2-96e065c6361e     # 生成的FSID\nmon initial members =future-ceph-node0\nmon host = 10.10.3.117\npublic network = 10.10.3.0/24\nauth cluster required = cephx\nauth service required = cephx\nauth client required = cephx\nosd journal size = 1024\nosd pool default size = 3\nosd pool default min size = 2\nosd pool default pg num = 333\nosd pool default pgp num = 333\nosd crush chooseleaf type = 1\n\n################################################\n```\n\n1. 由于我们使用使用root操作的，需要设置权限为ceph（也可以修改systemd的启动文件，将ceph用户改为root），并启动Monitor\n\n```Shell\nchown  -R ceph:ceph /var/lib/ceph\nsystemctl start ceph-mon@future-ceph-node0.service\nsystemctl enable ceph-mon@future-ceph-node0.service\n```\n\n1. 确认服务已经正常启动：\n\n```Shell\nceph -s\nyum install -y net-tools\nnetstat -lntp|grep ceph-mon\n```\n\n###### 部署Manager\n\n当我们配置好ceph-mon服务之后，就需要配置ceph-mgr服务。\n\n1. 生成一个认证密钥(ceph-mgr为自定义的名称)：\n\n```Shell\n#10.10.3.117\nceph auth get-or-create mgr.ceph-mgr mon 'allow profile mgr' osd 'allow *' mds 'allow *'\n[mgr.ceph-mgr]\n        key = AQBMNTZl5adxEhAAk6Jk/CKNWUyNb2DoKXUPvQ==\n        \n#10.10.3.118\nceph auth get-or-create mgr.ceph-mgr1 mon 'allow profile mgr' osd 'allow *' mds 'allow *'\n[mgr.ceph-mgr1]\n        key = AQDbRTZlgjXWBBAAGew4Xta+t9vgIWPCWC8EVg==\n```\n\n1. 创建存放此密钥的文件的目录\n\n```Shell\n#10.10.3.117\nsudo -u ceph mkdir /var/lib/ceph/mgr/ceph-ceph-mgr\n#将产生的密钥文件存入此目录下，并命名为keyring\nvi /var/lib/ceph/mgr/ceph-ceph-mgr/keyring \n[mgr.ceph-mgr]\n        key = AQBMNTZl5adxEhAAk6Jk/CKNWUyNb2DoKXUPvQ==\n        \n#10.10.3.118\nsudo -u ceph mkdir /var/lib/ceph/mgr/ceph-ceph-mgr1\n#将产生的密钥文件存入此目录下，并命名为keyring\nvi /var/lib/ceph/mgr/ceph-ceph-mgr1/keyring \n[mgr.ceph-mgr1]\n        key = AQDbRTZlgjXWBBAAGew4Xta+t9vgIWPCWC8EVg==\n```\n\n1. 启动ceph-mgr服务\n\n```Shell\nceph-mgr -i ceph-mgr\nceph-mgr -i ceph-mgr1\nsystemctl enable ceph-mgr@future-ceph-node0\nsystemctl enable ceph-mgr@future-ceph-node1\n#检查服务是否启动，查看ceph状态\nceph -s\n#查看当前mgr中可用的模块\nceph mgr module ls\n```\n\n###### 创建OSD\n\n```Shell\nceph-volume lvm create --data /dev/sdb\n#查看当前的lvm逻辑卷\nceph-volume lvm list\n#查看ceph状态\nceph -s\n```\n\n###### 安装配置Ceph-dashboard\n\n1. 开启dashboard功能\n\n```Shell\nceph mgr module enable dashboard\n```\n\n1. 创建证书\n\n```Shell\nceph dashboard create-self-signed-cert\n```\n\n1. 配置web登录的用户名和密码\n\n```Shell\n #创建/etc/ceph/dashboard.key，并将密码写入\n echo \"密码\" >/etc/ceph/dashboard.key\n ceph dashboard ac-user-create k8s administrator -i /etc/ceph/dashboard.key\n```\n\n1. 修改dashboard默认端口(可选)\n\n配置端口，默认端口是8443，修改为18443，修改后需重启mgr，修改端口才生效。\n\n```Shell\nceph config set mgr mgr/dashboard/server_port 18443\nsystemctl restart ceph-mgr.target\n```\n\n1. 查看发布服务地址并登录\n\n```Shell\nceph mgr services\n```\n\n> {\n>\n> ​    \"dashboard\": \"https://future-ceph-node0:8443/\"\n>\n> }\n>\n> ![image-20240323222001957](ceph集群部署指南/image-20240323222001957-1711203607034-1.png)\n\n##### node节点\n\n###### 扩展Monitor\n\n1. 修改master节点上的配置\n\n```Shell\nvi /etc/ceph/ceph.conf\n[global]\nfsid = 79fd2206-39ca-4ec4-9cd2-96e065c6361e     # 生成的FSID\nmon initial members =future-ceph-node0,future-ceph-node1,future-ceph-node2,future-ceph-node3            # 主机名\nmon host = 10.10.3.117,10.10.3.118,10.10.3.119,10.10.3.120                       # 对应的IP\npublic network = 10.10.3.0/24\nauth cluster required = cephx\nauth service required = cephx\nauth client required = cephx\nosd journal size = 1024\nosd pool default size = 3\nosd pool default min size = 2\nosd pool default pg num = 333\nosd pool default pgp num = 333\nosd crush chooseleaf type = 1\n[mon]\nmon allow pool delete = true\n\n[mds.future-ceph-node0]\nhost = future-ceph-node0\n```\n\n1. 将配置和密钥文件分发到其它的节点上（master节点）\n\n```Shell\n#生成公钥，复制到node节点主机上\nssh-keygen -t rsa\nssh-copy-id 10.10.3.118\nssh-copy-id 10.10.3.119\nssh-copy-id 10.10.3.120\n#复制认证密钥\nscp /etc/ceph/*  10.10.3.118:/etc/ceph/\nscp /etc/ceph/*  10.10.3.119:/etc/ceph/\nscp /etc/ceph/*  10.10.3.120:/etc/ceph/\n```\n\n1. 在node节点创建ceph相关目录，并添加权限：\n\n```Shell\nmkdir -p  /var/lib/ceph/{bootstrap-mds,bootstrap-mgr,bootstrap-osd,bootstrap-rbd,bootstrap-rgw,mds,mgr,mon,osd}\nchown  -R ceph:ceph /var/lib/ceph\n\nsudo -u ceph mkdir /var/lib/ceph/mon/ceph-future-ceph-node1\nsudo -u ceph mkdir /var/lib/ceph/mon/ceph-future-ceph-node2\nsudo -u ceph mkdir /var/lib/ceph/mon/ceph-future-ceph-node3\n```\n\n1. 修改node节点的配置文件,以node1为例（其他节点相似）\n\n```Shell\n[global]\nfsid = 79fd2206-39ca-4ec4-9cd2-96e065c6361e     # 生成的FSID\nmon initial members =future-ceph-node0,future-ceph-node1,future-ceph-node2,future-ceph-node3            # 主机名\nmon host = 10.10.3.117,10.10.3.118,10.10.3.119,10.10.3.120                       # 对应的IP\npublic network = 10.10.3.0/24\nauth cluster required = cephx\nauth service required = cephx\nauth client required = cephx\nosd journal size = 1024\nosd pool default size = 3\nosd pool default min size = 2\nosd pool default pg num = 333\nosd pool default pgp num = 333\nosd crush chooseleaf type = 1\n[mon]\nmon allow pool delete = true\n\n[mon.future-ceph-node1]\nmon_addr = 10.10.3.118:6789\nhost = future-ceph-node1\n```\n\n1. 获取集群中的密钥和map,以node1为例（其他节点相似）\n\n```Properties\nceph auth get mon. -o /tmp/monkeyring\nceph mon getmap -o /tmp/monmap\n```\n\n1. 使用已有的密钥和map添加一个新的Monitor,指定主机名,以node1为例（其他节点相似）\n\n```Perl\nsudo -u ceph ceph-mon --mkfs -i future-ceph-node1 --monmap /tmp/monmap --keyring /tmp/monkeyring\n```\n\n1. 启动服务,以node1为例（其他节点相似）\n\n```Nginx\nsystemctl start ceph-mon@future-ceph-node1\nsystemctl enable ceph-mon@future-ceph-node1\n#查看mon状态\nceph -s\nceph mon stat\n```\n\n###### 添加OSD\n\n从已经存在的osd的master节点上拷贝初始化的密钥文件\n\n```Shell\nscp -p  /var/lib/ceph/bootstrap-osd/ceph.keyring  10.10.3.118:/var/lib/ceph/bootstrap-osd/\nscp -p  /var/lib/ceph/bootstrap-osd/ceph.keyring  10.10.3.119:/var/lib/ceph/bootstrap-osd/\nscp -p  /var/lib/ceph/bootstrap-osd/ceph.keyring  10.10.3.120:/var/lib/ceph/bootstrap-osd/\n```\n\n在node节点添加osd\n\n```Shell\nceph-volume lvm create --data /dev/sdb\n\nsystemctl restart ceph-osd@future-ceph-node1\nsystemctl enable ceph-osd@future-ceph-node1\n#查看状态\nceph -s\n```\n\n#### 添加Mds\n\n```Shell\n#创建目录\nsudo -u ceph mkdir -p /var/lib/ceph/mds/ceph-future-ceph-node0\n#创建密钥\nceph-authtool --create-keyring /var/lib/ceph/mds/ceph-future-ceph-node0/keyring --gen-key -n mds.future-ceph-node0\n#导入密钥，并设置caps\nceph auth add mds.future-ceph-node0 osd \"allow rwx\" mds \"allow\" mon \"allow profile mds\" -i /var/lib/ceph/mds/ceph-future-ceph-node0/keyring\n#手动启动服务\nceph-mds --cluster ceph -i future-ceph-node0 -m future-ceph-node0:6789\n chown -R ceph:ceph /var/lib/ceph/mds/\n systemctl start ceph-mds@future-ceph-node0\n systemctl enable ceph-mds@future-ceph-node0\n #检查服务是否启动\n ps -ef|grep ceph-mds\n #检查ceph 集群状态\n ceph -s\n```\n\n#### 创建CephFS\n\n创建pools\n\n```Shell\n#存储数据\nceph osd pool create cephfs_data 64\n#存储元数据\nceph osd pool create cephfs_metadata 64\n#启用cephfs文件系统\nceph fs new cephfs cephfs_metadata cephfs_data\n#查看文件系统状态\nceph fs ls\nceph mds stat\n```\n\n1. 挂载cephfs\n\n内核驱动挂载\n\n```Shell\n#安装依赖\nyum install -y ceph-common\n#创建挂载点\nmkdir /data\n#获取存储密钥（管理节点）\ncat /etc/ceph/ceph.client.admin.keyring\n#将117中的/data 挂载到其他主机的/data\n mount -t ceph 10.10.3.117:6789:/ /data -o name=admin,secret=AQBLfS9l612IGhAAOF1iJqT6+rHJPCxqQegqCA==\n \n #卸载\numount /data  \n```\n\n用户控件挂载\n\n```Shell\nyum install ceph-fuse -y\n#挂载到其他主机\nceph-fuse -m  10.10.3.117:6789 /data\n#查看挂载配置\ndf -h |grep code\n\n#卸载\nfusermount -u /mnt/cephfs\n```\n\n### 创建rbd池\n\n```Shell\n#创建rbd池\nceph osd pool create rbd-k8s 64 64\n#启用 \nceph osd pool application enable rbd-k8s rbd\n#初始化\nrbd pool init rbd-k8s\n#查看\nceph osd lspools\n#查看映像\n rbd ls rbd-k8s\n #查看映像信息\n rbd info --pool rbd-k8s --image mars-c09bb992-963e-43dd-8e5c-e3a1d723b1c8 \n#映射镜像\n rbd map  --pool rbd-k8s --image mars-c09bb992-963e-43dd-8e5c-e3a1d723b1c8\n #格式化\n mkfs.ext4 /dev/rbd0\n #挂载到本地\n  mount /dev/rbd0 /data/code\n  #查看\n  lsblk\n  #取消挂载\n   umount /dev/rbd0\n   #取消映射\n   rbd unmap /dev/rbd0\n   \n```","tags":["ceph","共享存储","ceph集群"],"categories":["存储系统","ceph"]},{"title":"k8s集群对接ceph集群部署指南","url":"/yyg/57e2a956/","content":"## 任务目标\n\n1. 完成k8s集群对接ceph集群\n\n## 任务平台\n\n1. 物理设备--\n2. 操作系统：openEuler 22.03 LTS SP2\n\n## 部署指南\n\n采用动态挂载ceph RBD存储\n\n### 任务一：安装ceph客户端（ceph-common）\n\n在k8s集群的每个节点安装ceph-common\n\n```Shell\nyum install ceph-common -y\n```\n\n### 任务二：同步cpeh集群配置文件\n\n将 ceph 集群的 /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 文件同步到 k8s 所有节点上\n\n```Shell\n#配置ssh免密\nssh-keygen -t rsa\nssh-copy-id 10.10.3.121\nssh-copy-id 10.10.3.122\nssh-copy-id 10.10.3.123\nssh-copy-id 10.10.3.124\n\n#拷贝文件\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.3.121:/etc/ceph\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.3.122:/etc/ceph\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.3.123:/etc/ceph\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.3.124:/etc/ceph\n```\n\n### 任务三：部署ceph-csi（使用rbd）\n\n1. 下载ceph-csi组件\n\n```Shell\n#下载文件\nwget https://github.com/ceph/ceph-csi/archive/refs/tags/v3.9.0.tar.gz\n#解压\nmv v3.9.0.tar.gz ceph-csi-v3.9.0.tar.gz\ntar -xzf ceph-csi-v3.9.0.tar.gz\n#进入目录\ncd  ceph-csi-3.9.0/deploy/rbd/kubernetes\nmkdir /data/cephfs/csi\n#拷进csi中，共6六个文件\ncp * /data/cephfs\n```\n\n1. 拉取csi组件所需镜像\n\n```Shell\n#查看所需镜像\ngrep image csi-rbdplugin-provisioner.yaml\ngrep image csi-rbdplugin.yaml\n```\n\n在所有k8s节点上拉取所需的镜像\n\n```Shell\n./pull-images.sh registry.k8s.io/sig-storage/csi-provisioner:v3.5.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-resizer:v1.8.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2\ndocker pull quay.io/cephcsi/cephcsi:v3.9.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-attacher:v4.3.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0\n```\n\n1. 创建命名空间`cephfs`\n\n```Shell\necho '\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    kubernetes.io/metadata.name: cephfs\n  name: cephfs\n  ' > ceph-namespace.yaml\n  \n #执行\n  kubectl apply -f ceph-namespace.yaml \n```\n\n1. 创建连接 ceph 集群的秘钥文件csi-rbd-secret.yaml\n\n```Shell\necho '\napiVersion: v1\nkind: Secret\nmetadata:\n  name: csi-rbd-secret\n  namespace: cephfs\nstringData:\n  adminID: admin  #管理员名称\n  adminKey: AQCNNDZlPKI5KxAA/3gXnOmhHgyLU2qPzTaZ3A==   #ceph集群密钥\n  userID: admin   #用户名称 \n  userKey: AQCNNDZlPKI5KxAA/3gXnOmhHgyLU2qPzTaZ3A==  #ceph集群密钥\n  ' > csi-rbd-secret.yaml\n  \n   #执行\n    kubectl apply -f csi-rbd-secret.yaml\n```\n\n1. 创建ceph-config-map.yaml\n\n```Shell\necho '\napiVersion: v1\nkind: ConfigMap\ndata:\n  ceph.conf: |\n     [global]\n     fsid = 79fd2206-39ca-4ec4-9cd2-96e065c6361e     # 生成的FSID\n     mon initial members =future-ceph-node0,future-ceph-node1,future-ceph-node2,future-ceph-node3            # 主机名\n     mon host = 10.10.3.117,10.10.3.118,10.10.3.119,10.10.3.120                       # 对应的IP\n     public network = 10.10.3.0/24\n     auth cluster required = cephx\n     auth service required = cephx\n     auth client required = cephx\n     osd journal size = 1024\n     osd pool default size = 3\n     osd pool default min size = 2\n     osd pool default pg num = 333\n     osd pool default pgp num = 333\n     osd crush chooseleaf type = 1\n     [mon]\n     mon allow pool delete = true\n\n     [mds.future-ceph-node0]\n     host = future-ceph-node0\n  keyring: |\nmetadata:\n  name: ceph-config\n  namespace: cephfs\n' > ceph-config-map.yaml\n\n #执行\n kubectl apply -f ceph-config-map.yaml  \n```\n\n1. 修改csi-config-map.yaml，配置连接 ceph 集群的信息\n\n```Shell\necho '\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ceph-csi-config\n  namespace: cephfs\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\ndata:\n  config.json: |-\n    [{\"clusterID\":\"79fd2206-39ca-4ec4-9cd2-96e065c6361e\",\"monitors\":[\"10.10.3.117:6789\",\"10.10.3.118:6789\",\"10.10.3.119:6789\",\"10.10.3.120:6789\"]}]\n' > csi-config-map.yaml\n```\n\n1. 修改csi组件配置文件\n\n   1. 拷贝进`/data/cephfs/csi`目录中的所有yaml文件中的命名空间由`default`改为`cephfs`\n\n    ```Shell\n      cd /data/cephfs/csi\n      sed -i \"s/namespace: default/namespace: cephfs/g\" $(grep -rl \"namespace: default\" ./)\n      sed -i '/^kind: ServiceAccount/a\\  namespace: cephfs' $(grep -rl \"^kind: ServiceAccount\" ./)\n    ```\n\n   将`csi-rbdplugin-provisioner.yaml` 和 `csi-rbdplugin.yaml` 中的 kms 部分配置注释掉\n\n    >    \\# - name: KMS_CONFIGMAP_NAME\n      >\n      > ​            \\#   value: encryptionConfig\n      >\n      > \n      >\n      > \\#- name: ceph-csi-encryption-kms-config\n      >\n      > ​        \\#  configMap:\n      >\n      > ​        \\#    name: ceph-csi-encryption-kms-config\n      >\n      > \n\n```Shell\n #执行，安装csi组件\n kubectl apply -f csi-config-map.yaml\n kubectl apply -f csi-nodeplugin-rbac.yaml\n kubectl apply -f csidriver.yaml\n kubectl apply -f csi-provisioner-rbac.yaml\n kubectl apply -f csi-rbdplugin-provisioner.yaml\n kubectl apply -f csi-rbdplugin.yaml\n```\n\n### 任务四：创建storageclass\n\n```Shell\necho '\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    k8s.kuboard.cn/storageType: cephfs_provisioner\n  name: csi-rbd-sc\nprovisioner: rbd.csi.ceph.com\nparameters:\n # fsName: cephfs #ceph集群cephfs文件系统名称\n  clusterID: 79fd2206-39ca-4ec4-9cd2-96e065c6361e #ceph集群的fsid \n  pool: rbd-k8s #ceph中创建的存储池\n  imageFeatures: layering\n  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/provisioner-secret-namespace: cephfs\n  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/controller-expand-secret-namespace: cephfs\n  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/node-stage-secret-namespace: cephfs\n  csi.storage.k8s.io/fstype: xfs\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n#allowVolumeExpansion: true\n#mountOptions:\n # - discard\n ' > storageclass.yaml\n \n #执行\n  kubectl apply -f storageclass.yaml\n```\n\n### 任务五：创建PVC\n\n```Shell\necho '\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rbd-pvc #PVC的名称\n  namespace: cephfs\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi  #设置容量\n  storageClassName: csi-rbd-sc\n  ' > pvc.yaml\n  \n#执行\n kubectl apply -f pvc.yaml\n#查看PVC是否创建成功\nkubectl get pvc -n cephfs\n#查看PV是否创建成功\nkubectl get pv -n cephfs\n\n#查看ceph集群中的cephfs_data存储池中是否创建了image\n rbd ls -p cephfs_data\n```\n\n### 任务六：创建pod，进行测试验证\n\n```Shell\necho '\napiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-rbd-demo-pod\n  namespace: cephfs\nspec:\n  containers:\n    - name: web-server\n      image: nginx:latest\n      volumeMounts:\n        - name: mypvc\n          mountPath: /var/lib/www/html\n  volumes:\n    - name: mypvc\n      persistentVolumeClaim:\n        claimName: rbd-pvc  #对应的PVC名称\n        readOnly: false\n' > pod.yaml\n\n#执行\n kubectl apply -f pod.yaml\n #进入容器查看挂载信息\n kubectl exec -it csi-rbd-demo-pod bash\n lsblk -l|grep rbd\n```","tags":["docker","k8s","ceph","共享存储","ceph集群","容器云","k8s对接ceph"],"categories":["容器化","k8s","存储系统","ceph"]},{"title":"高可用容器云建设（k8s集群+ceph集群）","url":"/yyg/57e2a956/","content":"# 任务目标\n\n1. 完成高可用k8s集群安装部署\n\n# 任务平台\n\n1. 物理设备--\n2. 操作系统：openEuler 22.03 LTS SP2\n\n# 部署指南\n\n集群拓扑图\n\n![image-20240323222825713](高可用容器云建设/image-20240323222825713-1711204108611-3.png)\n\n## 一：部署ceph集群\n\n### 任务一：配置准备\n\n1. 重命名hostname\n\n```Shell\n# 将10.10.1.80的主机名改为future-k8s-master0\nhostnamectl set-hostname future-k8s-master0 && bash\n# 将10.10.1.81的主机名改为future-k8s-master1\nhostnamectl set-hostname future-k8s-master1 && bash\n# 将10.10.1.82的主机名改为future-k8s-master2\nhostnamectl set-hostname future-k8s-master2 && bash\n# 将10.10.1.16的主机名改为k8s-ceph-node0\nhostnamectl set-hostname k8s-ceph-node0 && bash\n# 将10.10.1.17的主机名改为k8s-ceph-node1\nhostnamectl set-hostname k8s-ceph-node1 && bash\n# 将10.10.1.18的主机名改为k8s-ceph-node2\nhostnamectl set-hostname k8s-ceph-node2 && bash\n# 将10.10.1.15的主机名改为k8s-ceph-node2\nhostnamectl set-hostname k8s-ceph-node3 && bash\n```\n\n1. 安装前的配置修改\n\n```Shell\n# 关闭防火墙\nsystemctl stop firewalld\nsystemctl disable firewalld\nfirewall-cmd --state\n \n# selinux永久关闭\nsetenforce 0\n sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\ncat /etc/selinux/config\n\n# swap永久关闭\nswapoff --all\nswapoff -a\nsed -ri 's/.*swap.*/#&/' /etc/fstab\ncat /etc/fstab\n\n# 添加hosts\ncat >> /etc/hosts << EOF\n10.10.1.80 future-k8s-master0\n10.10.1.81 future-k8s-master1\n10.10.1.82 future-k8s-master2\n10.10.1.16 k8s-ceph-node0\n10.10.1.17 k8s-ceph-node1\n10.10.1.18 k8s-ceph-node2\n10.10.1.15 k8s-ceph-node3\n10.10.1.83 future-k8s-vip\nEOF\n#查看\ncat /etc/hosts\n\n\n# 添加网桥过滤及内核转发配置文件\ncat > /etc/sysctl.d/k8s.conf << EOF\n net.bridge.bridge-nf-call-ip6tables = 1\n net.bridge.bridge-nf-call-iptables = 1\n net.ipv4.ip_forward = 1\nEOF\n# 查看\ncat /etc/sysctl.d/k8s.conf\n# 加载br_netfilter模块\nmodprobe br_netfilter\n# 查看是否加载\nlsmod | grep br_netfilter\n# 加载网桥过滤及内核转发配置文件\nsysctl -p /etc/sysctl.d/k8s.conf\n\n#同步时间\nyum install ntp -y\nsystemctl start ntpd\nsystemctl enable ntpd\nyum install chrony  -y\nsystemctl start chronyd\nsystemctl enable chronyd\n#修改配置，添加内容\necho \"\nserver 10.10.3.70 iburst\nallow 10.10.3.0/24\nallow 10.10.1.0/24\n\" >> /etc/chrony.conf\ntimedatectl set-ntp true\nsystemctl restart chronyd\ntimedatectl status\ndate\n```\n\n1. 安装ipset及ipvsadm\n\n```Shell\n # 安装ipset及ipvsadm\n yum -y install ipset ipvsadm\n 配置ipvsadm模块加载方式\n # 添加需要加载的模块\necho ' #!/bin/bash\n modprobe -- ip_vs\n modprobe -- ip_vs_rr\n modprobe -- ip_vs_wrr\n modprobe -- ip_vs_sh\n modprobe -- nf_conntrack\n' > /etc/sysconfig/modules/ipvs.modules\n#查看\ncat /etc/sysconfig/modules/ipvs.modules\n # 授权、运行、检查是否加载\nchmod 755 /etc/sysconfig/modules/ipvs.modules \nbash /etc/sysconfig/modules/ipvs.modules \nlsmod | grep -e ip_vs -e nf_conntrack\n\n#重启\nreboot\n```\n\n配置准备完成后，所有节点都需重启\n\n### 任务二：配置python环境\n\n下载python2\n\n1. 安装[zlib](https://so.csdn.net/so/search?q=zlib&spm=1001.2101.3001.7020)库，不然安装pip时会报错（还要重新编译python）\n\n```Plain\n yum -y install zlib*\n```\n\n1. 安装 GCC 包，如果没有安装 GCC，请使用以下命令进行安装\n\n```Shell\nyum -y install gcc openssl-devel bzip2-devel\n```\n\n1. 下载Python-2.7.18\n\n```Plain\n cd /usr/src\n yum -y install wget tar\n wget https://www.python.org/ftp/python/2.7.18/Python-2.7.18.tgz\n tar xzf Python-2.7.18.tgz\n```\n\n1. 在编译之前还需要在安装源文件中修改Modules/Setup.dist文件，将注释去掉\n\n```Shell\nsed -i 's/#zlib zlibmodule.c -I$(prefix)/zlib zlibmodule.c -I$(prefix)/'  Python-2.7.18/Modules/Setup.dist\n```\n\n1.  编译Python-2.7.18（`make altinstall`用于防止替换默认的 python 二进制文件 /usr/bin/python）\n\n```Shell\ncd /usr/src/Python-2.7.18\n./configure --enable-optimizations\nyum install -y make\nmake altinstall\n```\n\n不要覆盖或链接原始的 Python 二进制文件，这可能会损坏系统\n\n1. 设置环境变量\n\n```Shell\necho \"\nexport PYTHON_HOME=/usr/local/\nPATH=\\$PATH:\\$PYTHON_HOME/bin\n\" >> /etc/profile\ncat /etc/profile\nsource /etc/profile\n```\n\n1. 方法一：\n\n```Shell\ncurl \"https://bootstrap.pypa.io/pip/2.7/get-pip.py\" -o \"get-pip.py\"\npython2.7 get-pip.py \n```\n\n下载ceph\n\n```Shell\n#k8s-ceph-node0下载\n#方法一：使用pip下载\npip2 install ceph-deploy\nyum install -y ceph ceph-radosgw\n#其他节点下载\nyum install -y ceph ceph-radosgw\n#检查安装包是否完整\nrpm -qa |egrep -i \"ceph|rados|rbd\"\n```\n\n### 任务三：部署ceph集群\n\n1. #### admin节点\n\n1. ##### 部署Monitor\n\n1. 创建配置文件目录，并创建配置文件\n\n```Properties\nmkdir /etc/ceph/\ntouch /etc/ceph/ceph.conf\n```\n\n1. 为集群生成一个FSDI：\n\n```Ruby\nuuidgen\n30912204-0c26-413f-8e00-6d55c9c0af03\n```\n\n1. 集群创建一个钥匙串，为Monitor 服务创建一个密钥：\n\n```Ruby\nceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'\n```\n\n1. 创建一个管理员钥匙串，生成一个client.admin用户，并将此用户添加到钥匙串中：\n\n```Go\nceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'\n```\n\n1. 创建bootstrap-osd钥匙串，将client.bootstrap-osd 用户添加到此钥匙串中：\n\n```TypeScript\nceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd'\n```\n\n1. 将生成的key加入ceph.mon.keyring.\n\n```TypeScript\nceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring\n\nceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring\n```\n\n1. 使用主机名和IP地址以及FSID生成monitor map:\n\n```YAML\nmonmaptool --create --add k8s-ceph-node0 10.10.1.16 --fsid 30912204-0c26-413f-8e00-6d55c9c0af03 /tmp/monmap\n```\n\n1. 创建mon的目录,使用 `集群名称-主机名`的形式：\n\n```Shell\n mkdir  /var/lib/ceph/mon/ceph-k8s-ceph-node0\n```\n\n1. 填入第一个mon守护进程的信息:\n\n```TOML\nceph-mon --mkfs -i k8s-ceph-node0 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring\n```\n\n1. 配置/etc/ceph/ceph.conf文件：\n\n```Shell\ncat /etc/ceph/ceph.conf\n################################################\n[global]\nfsid = 30912204-0c26-413f-8e00-6d55c9c0af03     # 生成的FSID\nmon initial members =k8s-ceph-node0\nmon host = 10.10.1.16\npublic network = 10.10.1.0/24\nauth cluster required = cephx\nauth service required = cephx\nauth client required = cephx\nosd journal size = 1024\nosd pool default size = 3\nosd pool default min size = 2\nosd pool default pg num = 333\nosd pool default pgp num = 333\nosd crush chooseleaf type = 1\n\n################################################\n```\n\n1. 由于我们使用使用root操作的，需要设置权限为ceph（也可以修改systemd的启动文件，将ceph用户改为root），并启动Monitor\n\n```Shell\nchown  -R ceph:ceph /var/lib/ceph\nsystemctl start ceph-mon@k8s-ceph-node0.service\nsystemctl enable ceph-mon@k8s-ceph-node0.service\n```\n\n1. 确认服务已经正常启动：\n\n```Shell\nceph -s\nyum install -y net-tools\nnetstat -lntp|grep ceph-mon\n```\n\n1. ##### 部署Manager\n\n当我们配置好ceph-mon服务之后，就需要配置ceph-mgr服务。\n\n1. 生成一个认证密钥(ceph-mgr为自定义的名称)：\n\n```Shell\n#10.10.1.16\nceph auth get-or-create mgr.ceph-mgr mon 'allow profile mgr' osd 'allow *' mds 'allow *'\n[mgr.ceph-mgr]\n        key = AQANDD9lfWg2LBAAHY0mprdbuKFBPJDkE7/I5Q==\n        \n#10.10.1.17\nceph auth get-or-create mgr.ceph-mgr1 mon 'allow profile mgr' osd 'allow *' mds 'allow *'\n[mgr.ceph-mgr1]\n        key = AQDbRTZlgjXWBBAAGew4Xta+t9vgIWPCWC8EVg==\n```\n\n1. 创建存放此密钥的文件的目录\n\n```Shell\n#10.10.1.16\nsudo -u ceph mkdir /var/lib/ceph/mgr/ceph-ceph-mgr\n#将产生的密钥文件存入此目录下，并命名为keyring\nvi /var/lib/ceph/mgr/ceph-ceph-mgr/keyring \n[mgr.ceph-mgr]\n        key = AQANDD9lfWg2LBAAHY0mprdbuKFBPJDkE7/I5Q==\n        \n#10.10.1.17\nsudo -u ceph mkdir /var/lib/ceph/mgr/ceph-ceph-mgr1\n#将产生的密钥文件存入此目录下，并命名为keyring\nvi /var/lib/ceph/mgr/ceph-ceph-mgr1/keyring \n[mgr.ceph-mgr1]\n        key = AQDbRTZlgjXWBBAAGew4Xta+t9vgIWPCWC8EVg==\n```\n\n1. 启动ceph-mgr服务\n\n```Shell\nceph-mgr -i ceph-mgr\nceph-mgr -i ceph-mgr1\nsystemctl enable ceph-mgr@k8s-ceph-node0\nsystemctl enable ceph-mgr@k8s-ceph-node1\n#检查服务是否启动，查看ceph状态\nceph -s\n#查看当前mgr中可用的模块\nceph mgr module ls\n```\n\n1. ##### 创建OSD\n\n```Shell\nceph-volume lvm create --data /dev/sda8\n#查看当前的lvm逻辑卷\nceph-volume lvm list\n#查看ceph状态\nceph -s\n```\n\n1. ##### 安装配置Ceph-dashboard\n\n1. 开启dashboard功能\n\n```Shell\nceph mgr module enable dashboard\n```\n\n1. 创建证书\n\n```Shell\nceph dashboard create-self-signed-cert\n```\n\n1. 配置web登录的用户名和密码\n\n```Shell\n #创建/etc/ceph/dashboard.key，并将密码写入\n echo \"qishi#09319\" >/etc/ceph/dashboard.key\n ceph dashboard ac-user-create k8s administrator -i /etc/ceph/dashboard.key\n```\n\n1. 修改dashboard默认端口(可选)\n\n配置端口，默认端口是8443，修改为18443，修改后需重启mgr，修改端口才生效。\n\n```Shell\nceph config set mgr mgr/dashboard/server_port 18443\nsystemctl restart ceph-mgr.target\n```\n\n1. 查看发布服务地址并登录\n\n```Shell\nceph mgr services\n```\n\n> {\n>\n> ​    \"dashboard\": \"https://k8s-ceph-node0:8443/\"\n>\n> }\n\n![image-20240323222910679](高可用容器云建设/image-20240323222910679-1711204151978-5.png)\n\n1. #### node节点\n\n1. ##### 扩展Monitor\n\n1. 修改master节点上的配置\n\n```Shell\nvi /etc/ceph/ceph.conf\n[global]\nfsid = 30912204-0c26-413f-8e00-6d55c9c0af03     # 生成的FSID\nmon initial members =k8s-ceph-node0,k8s-ceph-node1,k8s-ceph-node2,k8s-ceph-node3            # 主机名\nmon host = 10.10.1.16,10.10.1.17,10.10.1.18,10.10.1.15                       # 对应的IP\npublic network = 10.10.1.0/24\nauth cluster required = cephx\nauth service required = cephx\nauth client required = cephx\nosd journal size = 1024\nosd pool default size = 3\nosd pool default min size = 2\nosd pool default pg num = 333\nosd pool default pgp num = 333\nosd crush chooseleaf type = 1\n[mon]\nmon allow pool delete = true\n\n[mds.k8s-ceph-node0]\nhost = k8s-ceph-node0\n```\n\n1. 将配置和密钥文件分发到其它的节点上（master节点）\n\n```Shell\n#生成公钥，复制到node节点主机上\nssh-keygen -t rsa\nssh-copy-id 10.10.1.17\nssh-copy-id 10.10.1.18\nssh-copy-id 10.10.1.15\n#复制认证密钥\nscp /etc/ceph/*  10.10.1.17:/etc/ceph/\nscp /etc/ceph/*  10.10.1.18:/etc/ceph/\nscp /etc/ceph/*  10.10.1.15:/etc/ceph/\n```\n\n1. 在node节点创建ceph相关目录，并添加权限：\n\n```Shell\nmkdir -p  /var/lib/ceph/{bootstrap-mds,bootstrap-mgr,bootstrap-osd,bootstrap-rbd,bootstrap-rgw,mds,mgr,mon,osd}\nchown  -R ceph:ceph /var/lib/ceph\n\nsudo -u ceph mkdir /var/lib/ceph/mon/ceph-k8s-ceph-node1\nsudo -u ceph mkdir /var/lib/ceph/mon/ceph-k8s-ceph-node2\n```\n\n1. 修改node节点的配置文件,以node1为例（其他节点相似）\n\n```Shell\n[global]\nfsid = 30912204-0c26-413f-8e00-6d55c9c0af03     # 生成的FSID\nmon initial members =k8s-ceph-node0,k8s-ceph-node1,k8s-ceph-node2,k8s-ceph-node3           # 主机名\nmon host = 10.10.1.16,10.10.1.17,10.10.1.18,10.10.1.15                       # 对应的IP\npublic network = 10.10.1.0/24\nauth cluster required = cephx\nauth service required = cephx\nauth client required = cephx\nosd journal size = 1024\nosd pool default size = 3\nosd pool default min size = 2\nosd pool default pg num = 333\nosd pool default pgp num = 333\nosd crush chooseleaf type = 1\n[mon]\nmon allow pool delete = true\n\n[mon.k8s-ceph-node1]\nmon_addr = 10.10.1.17:6789\nhost = k8s-ceph-node1\n```\n\n1. 获取集群中的密钥和map,以node1为例（其他节点相似）\n\n```Properties\nceph auth get mon. -o /tmp/monkeyring\nceph mon getmap -o /tmp/monmap\n```\n\n1. 使用已有的密钥和map添加一个新的Monitor,指定主机名,以node1为例（其他节点相似）\n\n```Perl\nsudo -u ceph ceph-mon --mkfs -i k8s-ceph-node1 --monmap /tmp/monmap --keyring /tmp/monkeyring\n```\n\n1. 启动服务,以node1为例（其他节点相似）\n\n```Nginx\nsystemctl start ceph-mon@k8s-ceph-node1\nsystemctl enable ceph-mon@k8s-ceph-node1\n#查看mon状态\nceph -s\nceph mon stat\n```\n\n1. ##### 添加OSD\n\n从已经存在的osd的master节点上拷贝初始化的密钥文件\n\n```Shell\nscp -p  /var/lib/ceph/bootstrap-osd/ceph.keyring  10.10.1.17:/var/lib/ceph/bootstrap-osd/\nscp -p  /var/lib/ceph/bootstrap-osd/ceph.keyring  10.10.1.18:/var/lib/ceph/bootstrap-osd/\nscp -p  /var/lib/ceph/bootstrap-osd/ceph.keyring  10.10.1.15:/var/lib/ceph/bootstrap-osd/\n```\n\n在node节点添加osd\n\n```Shell\nceph-volume lvm create --data /dev/sdb\n\nsystemctl enable ceph-osd@k8s-ceph-node1\n#查看状态\nceph -s\n```\n\n1. #### 添加Mds（以node0为例）\n\n```Shell\n#创建目录\nsudo -u ceph mkdir -p /var/lib/ceph/mds/ceph-k8s-ceph-node0\n#创建密钥\nceph-authtool --create-keyring /var/lib/ceph/mds/ceph-k8s-ceph-node0/keyring --gen-key -n mds.k8s-ceph-node0\n#导入密钥，并设置caps\nceph auth add mds.k8s-ceph-node0 osd \"allow rwx\" mds \"allow\" mon \"allow profile mds\" -i /var/lib/ceph/mds/ceph-k8s-ceph-node0/keyring\n#手动启动服务\nceph-mds --cluster ceph -i k8s-ceph-node0 -m k8s-ceph-node0:6789\n chown -R ceph:ceph /var/lib/ceph/mds/\n systemctl start ceph-mds@k8s-ceph-node0\n systemctl enable ceph-mds@k8s-ceph-node0\n #检查服务是否启动\n ps -ef|grep ceph-mds\n #检查ceph 集群状态\n ceph -s\n```\n\n1. #### 创建CephFS\n\n创建pools\n\n```Shell\n#存储数据\nceph osd pool create cephfs_data 64\n#存储元数据\nceph osd pool create cephfs_metadata 64\n#启用cephfs文件系统\nceph fs new cephfs cephfs_metadata cephfs_data\n#查看文件系统状态\nceph fs ls\nceph mds stat\n```\n\n1. #### 创建rbd池\n\n```Shell\n#创建rbd池\nceph osd pool create rbd-k8s 64 64\n#启用 \nceph osd pool application enable rbd-k8s rbd\n#初始化\nrbd pool init rbd-k8s\n#查看\nceph osd lspools\n```\n\n## 二：部署高可用k8s集群\n\n### 任务一：配置准备（与ceph集群一样）\n\n### 任务二：安装docker\n\n1. 配置Docker CE的yum存储库。打开`docker-ce.repo`的文件，并将以下内容复制到文件中：\n\n```Shell\necho '\n[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/centos/7/$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/centos/gpg ' > /etc/yum.repos.d/docker-ce.repo\n```\n\n保存并退出文件。\n\n1. 安装Docker CE。运行以下命令来安装Docker CE：\n\n```Shell\n yum -y install docker-ce docker-ce-cli  containerd.io\n#启动docker并设置开机自启\nsystemctl start docker  \nsystemctl enable docker\n#查看版本\ndocker -v\ndocker compose version\n```\n\n1. Docker配置修改，设置cgroup驱动，使用systemd，配置修改为如下。\n\n```Shell\n#将配置写入daemon.json文件\necho '{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"data-root\": \"/data/docker\"\n} ' > /etc/docker/daemon.json\n#查看\ncat /etc/docker/daemon.json\nsystemctl daemon-reload\nsystemctl restart docker\ndocker info\n```\n\n1. 创建所需目录\n\n```Shell\ncd /data\nmkdir  cri-dockerd calico    dashboard   metrics-server  script  ingress-nginx\n```\n\n### 任务三：安装cri-dockerd （k8s 1.24及以上版本）\n\n```Shell\ncd /data/cri-dockerd\n# 下载cri-dockerd安装包\nwget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.4/cri-dockerd-0.3.4-3.el8.x86_64.rpm\n# 安装cri-dockerd\nrpm -ivh cri-dockerd-0.3.4-3.el8.x86_64.rpm\ndocker pull registry.aliyuncs.com/google_containers/pause:3.9\n# 修改镜像地址为国内，否则kubelet拉取不了镜像导致启动失败\nsed -i.bak 's|ExecStart=.*$|ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9|g' /usr/lib/systemd/system/cri-docker.service\ncat /usr/lib/systemd/system/cri-docker.service\n# 启动cri-dockerd\nsystemctl daemon-reload \nsystemctl start cri-docker.service\nsystemctl enable cri-docker.service\n```\n\n### 任务四：安装高可用组件\n\n部署高可用集群需要安装**keepalived和haproxy，实现****master****节点高可用，****在各master节点操作**\n\n1. 安装keepalived与haproxy\n\n```Shell\nyum install keepalived haproxy -y\n```\n\n1. 备份keepalived与haproxy配置文件\n\n```Shell\ncp /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak\ncp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bak\n```\n\n1. 修改各master节点的`/etc/keepalived/keepalived.conf`文件\n   1. future-k8s-master0\n   2. ```Shell\n      echo '\n      global_defs {\n         router_id k8s\n      }\n      \n      vrrp_script check_haproxy {\n          script \"killall -0 haproxy\"\n          interval 3\n          weight -2\n          fall 10\n          rise 2\n      }\n      \n      vrrp_instance VI_1 {\n          state MASTER  #主节点 则为MASTER ,其他则为 BACKUP\n          interface ens192  #网卡名称\n          virtual_router_id 51\n          priority 250   #优先级\n          nopreempt   #设置非抢占模式\n          advert_int 1\n          authentication {\n              auth_type PASS\n              auth_pass ceb1b3ec013d66163d6ab\n          }\n          virtual_ipaddress {\n              10.10.1.83/24   #虚拟ip\n          }\n          track_script {\n              check_haproxy\n          }\n      }    \n      ' > /etc/keepalived/keepalived.conf\n      cat /etc/keepalived/keepalived.conf\n      ```\n\n   3. future-k8s-master1\n   4. ```Shell\n      echo '\n      global_defs {\n         router_id k8s\n      }\n      \n      vrrp_script check_haproxy {\n          script \"killall -0 haproxy\"\n          interval 3\n          weight -2\n          fall 10\n          rise 2\n      }\n      \n      vrrp_instance VI_1 {\n          state BACKUP  #主节点 则为MASTER ,其他则为 BACKUP\n          interface ens192  #网卡名称\n          virtual_router_id 51\n          priority 200   #优先级\n          nopreempt   #设置非抢占模式\n          advert_int 1\n          authentication {\n              auth_type PASS\n              auth_pass ceb1b3ec013d66163d6ab\n          }\n          virtual_ipaddress {\n              10.10.1.83/24   #虚拟ip\n          }\n          track_script {\n              check_haproxy\n          }\n      }    \n      ' > /etc/keepalived/keepalived.conf\n      cat  /etc/keepalived/keepalived.conf\n      ```\n\n   5. future-k8s-master2\n   6. ```Shell\n      echo '\n      global_defs {\n         router_id k8s\n      }\n      \n      vrrp_script check_haproxy {\n          script \"killall -0 haproxy\"\n          interval 3\n          weight -2\n          fall 10\n          rise 2\n      }\n      \n      vrrp_instance VI_1 {\n          state BACKUP  #主节点 则为MASTER ,其他则为 BACKUP\n          interface ens192  #网卡名称\n          virtual_router_id 51\n          priority 150   #优先级\n          nopreempt   #设置非抢占模式\n          advert_int 1\n          authentication {\n              auth_type PASS\n              auth_pass ceb1b3ec013d66163d6ab\n          }\n          virtual_ipaddress {\n              10.10.1.83/24   #虚拟ip\n          }\n          track_script {\n              check_haproxy\n          }\n      }    \n      ' > /etc/keepalived/keepalived.conf\n      cat  /etc/keepalived/keepalived.conf\n      ```\n2. 修改各master节点的`/etc/haproxy/haproxy.cfg`文件，（三个master节点的配置文件相同）\n\n```Shell\necho \"\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n#---------------------------------------------------------------------\n# kubernetes apiserver frontend which proxys to the backends\n#---------------------------------------------------------------------\nfrontend kubernetes-apiserver\n    mode                 tcp\n    bind                 *:16443 #高可用监控端口，初始化k8s集群时会用\n    option               tcplog\n    default_backend      kubernetes-apiserver\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\nbackend kubernetes-apiserver\n    mode        tcp\n    balance     roundrobin\n    server      future-k8s-master0   10.10.1.80:6443 check\n    server      future-k8s-master1   10.10.1.81:6443 check\n    server      future-k8s-master2   10.10.1.82:6443 check\n\n#---------------------------------------------------------------------\n# collection haproxy statistics message\n#---------------------------------------------------------------------\nlisten stats\n    bind                 *:1080\n    stats auth           admin:awesomePassword\n    stats refresh        5s\n    stats realm          HAProxy\\ Statistics\n    stats uri            /admin?stats\n\n\" > /etc/haproxy/haproxy.cfg\n\ncat /etc/haproxy/haproxy.cfg\n```\n\n1. 启动（各master节点按顺序启动）\n\n```Shell\n#启动keepalived  \nsystemctl enable keepalived  && systemctl start keepalived  \n#启动haproxy \nsystemctl enable haproxy && systemctl start haproxy\nsystemctl status keepalived\nsystemctl status haproxy\n```\n\n1. 在future-k8s-master0查看绑定的vip地址  \n\n> ip add 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever 2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000    link/ether 00:50:56:9a:eb:48 brd ff:ff:ff:ff:ff:ff    inet 10.10.1.80/24 brd 10.10.3.255 scope global noprefixroute ens192       valid_lft forever preferred_lft forever    inet 10.10.1.83/24 scope global ens192       valid_lft forever preferred_lft forever    inet6 fe80::250:56ff:fe9a:eb48/64 scope link noprefixroute       valid_lft forever preferred_lft forever\n\n### 任务五：部署k8s集群\n\n1. #### 添加yum软件源\n\n```Shell\ncat > /etc/yum.repos.d/kubernetes.repo << EOF\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n1. #### 安装kubeadm，kubelet和kubectl\n\n```Shell\n# 安装kubelet、kubeadm、kubectl\nyum install -y kubelet-1.28.0 kubeadm-1.28.0 kubectl-1.28.0 --disableexcludes=kubernetes\n\n#将cgroup改为systemd\necho 'KUBELET_EXTRA_ARGS=\"--cgroup-driver=systemd\"' > /etc/sysconfig/kubelet\n# 查看\ncat /etc/sysconfig/kubelet\n# 设置开机启动\nsystemctl start kubelet.service\nsystemctl enable kubelet.service\n\n#查看版本\n kubeadm version\n kubelet --version\n kubectl version \n```\n\n1. #### 初始化k8s集群（future-k8s-master0节点）\n\n   #####  方式一：使用配置文件初始化\n\n   > 1. 导出默认配置文件 (可选)\n   >\n   > ```Shell\n   > kubeadm config print init-defaults > kubeadm-config.yaml\n   > ```\n\n   1. 配置文件\n\n   ```Shell\n   echo '\n   apiVersion: kubeadm.k8s.io/v1beta3\n   kind: InitConfiguration\n   localAPIEndpoint:\n     advertiseAddress: 10.10.1.83  #虚拟ip\n     bindPort: 6443\n   nodeRegistration:\n     criSocket: unix:///var/run/cri-dockerd.sock\n   ---\n   apiServer:\n     certSANs:    #master节点与对应主机名\n       - future-k8s-master0\n       - future-k8s-master1\n       - future-k8s-master2\n       - future-k8s-vip\n       - 10.10.1.80\n       - 10.10.1.81\n       - 10.10.1.82\n       - 10.10.1.83\n       - 127.0.0.1\n     timeoutForControlPlane: 4m0s\n   apiVersion: kubeadm.k8s.io/v1beta3\n   certificatesDir: /etc/kubernetes/pki\n   clusterName: kubernetes\n   controlPlaneEndpoint: \"future-k8s-vip:16443\" #虚拟ip及高可用配置的端口号\n   controllerManager: {}\n   dns: {}\n   etcd:\n     local:\n       dataDir: /var/lib/etcd\n   imageRepository: registry.aliyuncs.com/google_containers\n   kind: ClusterConfiguration\n   kubernetesVersion: 1.28.0\n   networking:\n     dnsDomain: cluster.local\n     podSubnet: 10.244.0.0/16\n     serviceSubnet: 10.96.0.0/12\n   scheduler: {}\n   ' > /data/script/kubeadm-config.yaml\n   cat /data/script/kubeadm-config.yaml\n   ```\n\n   1. 集群初始化\n\n   ```Shell\n   kubeadm init --config kubeadm-config.yaml --upload-certs\n   \n   mkdir -p $HOME/.kube\n   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n   sudo chown $(id -u):$(id -g) $HOME/.kube/config\n   ```\n\n   #####  方式二：使用命令初始化\n\n   1. 部署master节点，在10.10.1.80执行，初始化master节点\n\n   ```Shell\n   kubeadm init \\\n     --apiserver-advertise-address=10.10.1.80 \\\n     --image-repository registry.aliyuncs.com/google_containers \\\n     --kubernetes-version v1.28.0 \\\n     --control-plane-endpoint=future-k8s-vip:16443 \\  #虚拟ip（未定）\n     --control-plane-endpoint=future-k8s-vip \\  #虚拟ip（未定）\n     --service-cidr=10.96.0.0/12 \\\n     --pod-network-cidr=10.244.0.0/16 \\\n     --cri-socket=unix:///var/run/cri-dockerd.sock \\\n     --ignore-preflight-errors=all \n     \n     \n   mkdir -p $HOME/.kube\n   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n   sudo chown $(id -u):$(id -g) $HOME/.kube/config\n   ```\n\n   1. 配置ssh免密码\n\n   ```Shell\n   #在10.10.1.80上生成公钥，复制到其他master节点上\n   ssh-keygen -t rsa\n   ssh-copy-id 10.10.1.81\n   ssh-copy-id 10.10.1.82\n   ```\n\n   1. 将10.10.1.80上的证书拷贝到其他master节点\n\n   ```Shell\n   #在其他master节点创建证书存放目录\n   cd /root && mkdir -p /etc/kubernetes/pki/etcd &&mkdir -p ~/.kube/\n   \n   #将future-k8s-master0的证书复制到future-k8s-master1\n   scp /etc/kubernetes/pki/ca.crt 10.10.1.81:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/ca.key 10.10.1.81:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/sa.key 10.10.1.81:/etc/kubernetes/pki/\n   scp /etc/kubernetes/pki/sa.pub 10.10.1.81:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.crt 10.10.1.81:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.key 10.10.1.81:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/etcd/ca.crt 10.10.1.81:/etc/kubernetes/pki/etcd/\n   scp /etc/kubernetes/pki/etcd/ca.key 10.10.1.81:/etc/kubernetes/pki/etcd/\n   \n   #将future-k8s-master0的证书复制到future-k8s-master2\n   scp /etc/kubernetes/pki/ca.crt 10.10.1.82:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/ca.key 10.10.1.82:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/sa.key 10.10.1.82:/etc/kubernetes/pki/\n   scp /etc/kubernetes/pki/sa.pub 10.10.1.82:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.crt 10.10.1.82:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/front-proxy-ca.key 10.10.1.82:/etc/kubernetes/pki/ \n   scp /etc/kubernetes/pki/etcd/ca.crt 10.10.1.82:/etc/kubernetes/pki/etcd/\n   scp /etc/kubernetes/pki/etcd/ca.key 10.10.1.82:/etc/kubernetes/pki/etcd/\n   ```\n\n1. #### 初始化其他master节点\n\n```Shell\n kubeadm join future-k8s-vip:16443 --token yjphdh.guefcomqw3am4ask \\\n        --discovery-token-ca-cert-hash sha256:ed44c7deada0ea0fe5a54212ab4e5aa6fc34672ffe2a2c87a31ba73306e75c21 \\\n        --control-plane --certificate-key 4929b83577eafcd5933fc0b6506cb6d82e7bc481751e442888c4c2b32b5d0c9c  --cri-socket=unix:///var/run/cri-dockerd.sock\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n1. #### 初始化node节点\n\n```Shell\nkubeadm join future-k8s-vip:16443 --token yjphdh.guefcomqw3am4ask \\\n        --discovery-token-ca-cert-hash sha256:ed44c7deada0ea0fe5a54212ab4e5aa6fc34672ffe2a2c87a31ba73306e75c21 --cri-socket=unix:///var/run/cri-dockerd.sock\n```\n\n1. #### 设置master节点允许调度POD （可选）\n\n默认配置下Kubernetes不会将Pod调度到Master节点。如果希望将k8s-master也当作Node使用，需去除污点，开启调度。\n\n```Shell\n#查看默认配置的污点\nkubectl describe node future-k8s-master2 |grep Taints\n```\n\n> Taints:             node-role.kubernetes.io/control-plane:NoSchedule\n\n```Shell\n#去除污点\nkubectl taint nodes future-k8s-master2 node-role.kubernetes.io/control-plane-\n```\n\n添加woker标记\n\n```Shell\n#添加worker标记\nkubectl label nodes future-k8s-master2 node-role.kubernetes.io/worker=\n#删除worker标记\nkubectl label nodes future-k8s-master2 node-role.kubernetes.io/worker-\n```\n\n### 任务六：安装网络插件(master)\n\n安装calico\n\n```Shell\nmkdir /data/calico\nwget https://docs.tigera.io/archive/v3.25/manifests/calico.yaml\n#修改calico.yaml找到CALICO_IPV4POOL_CIDR\nvi calico.yaml\n##############修改内容###################\n value: \"10.244.0.0/16\"\n ##############修改内容###################\n #在master节点上安装calico\n kubectl apply -f calico.yaml\n```\n\n查看节点状态\n\n```Shell\n# 查看所有的节点\nkubectl get nodes\nkubectl get nodes -o wide\n#查看集群健康情况\n kubectl get cs\n```\n\n### 任务七：安装nginx进行测试\n\n```Shell\n# 创建Nginx程序\nkubectl create deployment nginx --image=nginx\n# 开放80端口\nkubectl expose deployment nginx --port=80 --type=NodePort\n# 查看pod状态\nkubectl get pod\n#查看service状态\nkubectl get service\n##########################################################################\nNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        5d1h\nnginx        NodePort    10.98.221.224   <none>        80:32743/TCP   23s\n##########################################################################\n# 访问网页测试(端口号以查看service状态得到的为准)\nhttp://10.10.1.80:32743/\n```\n\n### 任务八：安装**Dashboard界面**\n\n1. 下载yaml文件\n\n```Shell\n#创建存放目录\nmkdir dashboard\ncd dashboard/\n#2.7\nwget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n```\n\n1. 修改yaml文件\n\n```Shell\nvi recommended.yaml\n#将副本设置为2\n#################修改内容#######################\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 32009   #添加这一行，注意缩进对齐\n  selector:\n    k8s-app: kubernetes-dashboard\n  type: NodePort          #添加这一行，注意缩进对齐\n  #################修改内容#######################\n```\n\n1. 应用安装，查看pod和svc\n\n```Shell\n#安装\nkubectl apply -f recommended.yaml\n#查看pod和svc\nkubectl get pod,svc -o wide -n kubernetes-dashboard\n#########################################################\nNAME                                             READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATES\npod/dashboard-metrics-scraper-5cb4f4bb9c-mg569   0/1     ContainerCreating   0          9s    <none>   node1   <none>           <none>\npod/kubernetes-dashboard-6967859bff-2968p        0/1     ContainerCreating   0          9s    <none>   node1   <none>           <none>\n\nNAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE   SELECTOR\nservice/dashboard-metrics-scraper   ClusterIP   10.100.129.191   <none>        8000/TCP        9s    k8s-app=dashboard-metrics-scraper\nservice/kubernetes-dashboard        NodePort    10.106.130.53    <none>        443:31283/TCP   9s    k8s-app=kubernetes-dashboard\n########################################################\n```\n\n使用所查看的svc，所提供的端口访问**Dashboard**\n\n1. 创建dashboard服务账户\n\n```Shell\n#创建一个admin-user的服务账户并与集群绑定\nvi dashboard-adminuser.yaml\n##################内容####################\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n  \n---\n# 创建密钥，获取服务帐户的长期持有者令牌\napiVersion: v1\nkind: Secret\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"admin-user\"\ntype: kubernetes.io/service-account-token\n  ##################内容####################\n \n  #执行生效\n  kubectl apply -f dashboard-adminuser.yaml\n```\n\n1. 登录方式\n\n方案一：获取长期可用token\n\n```Shell\n#将其保存在/data/dashboard/的admin-user.token文件中\ncd /data/dashboard/\nkubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d > admin-user.token \n```\n\n获取长期可用token脚本\n\n```Shell\n#!/bin/bash\n#作者：云\n#############描述#############\n:<<!\n获取长期可用token脚本\n将token存放在admin-user.token文件中\n!\n#############描述#############\nkubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d > admin-user.token\n\necho -e \"\\033[1;32m创建token成功，请在admin-user.token文件中查看\\033[m\"\n```\n\n方案二：使用使用 Kubeconfig 文件登录\n\n```Shell\n #定义 token 变量\n DASH_TOCKEN=$(kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d)\n #设置 kubeconfig 集群条目\n kubectl config set-cluster kubernetes --server=10.10.1.80:6433 --kubeconfig=/root/dashbord-admin.conf\n #设置 kubeconfig 用户条目\n kubectl config set-credentials admin-user --token=$DASH_TOCKEN --kubeconfig=/root/dashbord-admin.conf\n #设置 kubeconfig 上下文条目\n kubectl config set-context admin-user@kubernetes --cluster=kubernetes --user=admin-user --kubeconfig=/root/dashbord-admin.conf\n #设置 kubeconfig 当前上下文\n kubectl config use-context admin-user@kubernetes  --kubeconfig=/root/dashbord-admin.conf\n```\n\n将生成的dashbord-admin.conf文件放到本地主机上，登录时选择`Kubeconfig`选项，选择 kubeconfig 文件登录\n\n### 任务九：安装metrics-server\n\n下载部署文件\n\n```Shell\nwget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml -O metrics-server-components.yaml\n```\n\n修改yaml文件中的Deployment内容\n\n```Shell\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        k8s-app: metrics-server\n    spec:\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=4443\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls  #添加\n        image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.4 #修改\n        imagePullPolicy: IfNotPresent\n\n#安装\nkubectl apply -f metrics-server-components.yaml\n```\n\n查看metrics-server的pod状态\n\n```Shell\nkubectl get pods --all-namespaces | grep metrics\n```\n\n等待一些时间，查看查看各类监控图像已成功显示。\n\n![image-20240323222707075](高可用容器云建设/image-20240323222707075-1711204031457-1.png)\n\n### 任务十：kubectl命令自动补全\n\n```Shell\nyum -y install bash-completion\nsource /usr/share/bash-completion/bash_completion\necho 'source <(kubectl completion bash)' >>  ~/.bashrc\nbash\n```\n\n### 任务十一：ingress-nginx控制器安装\n\n```Shell\n#下载yaml文件\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml\n#修改yaml文件中拉取镜像的地址\n#####################修改内容######################\nwilldockerhub/ingress-nginx-controller:v1.0.0\nhzde0128/kube-webhook-certgen:v1.0\n#####################修改内容######################\n#修改Deployment修改成DaemonSet\n#修改网络模式为host network\n#####################修改内容######################\ntemplate:\n  spec:\n    hostNetwork: true\n    dnsPolicy: ClusterFirstWithHostNet\n    tolerations:  #使用亲和性配置可在所有节点部署\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n     nodeSelector:\n          kubernetes.io/os: linux\n          custem/ingress-controller-ready: 'true'\n      containers:\n        - name: controller\n#####################修改内容######################\n#为工作节点设置标签（必需）\nkubectl label nodes future-k8s-master0 custem/ingress-controller-ready=true\nkubectl label nodes future-k8s-master1 custem/ingress-controller-ready=true\nkubectl label nodes future-k8s-master2 custem/ingress-controller-ready=true\nkubectl label nodes future-k8s-node3 custem/ingress-controller-ready=true\n\n#安装\nkubectl apply -f deploy.yaml\n\n#查看状态\nkubectl get pods -n ingress-nginx\n################状态##################\nNAME                                       READY   STATUS      RESTARTS   AGE\ningress-nginx-admission-create-2lz4v       0/1     Completed   0          5m46s\ningress-nginx-admission-patch-c6896        0/1     Completed   0          5m46s\ningress-nginx-controller-7575fb546-q29qn   1/1     Running     0          5m46s\n```\n\n### 任务十二：配置**Dashboard代理**\n\n```Shell\necho '\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: k8s-dashboard\n  namespace: kubernetes-dashboard\n  labels:\n    ingress: k8s-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /  #重写路径\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"   #http自动转https\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\nspec:\n  ingressClassName: nginx \n  rules:\n    - host: k8s.yjs.51xueweb.cn\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard\n                port:\n                  number: 443\n' > /data/dashboard/dashboard-ingress.yaml\n```\n\n## 三：对接k8s集群与ceph集群\n\n### 任务一：安装ceph客户端（ceph-common）\n\n在k8s集群的每个节点安装ceph-common\n\n```Shell\nyum install ceph-common -y\n```\n\n### 任务二：同步cpeh集群配置文件\n\n将 ceph 集群的 /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 文件同步到 k8s 所有节点上\n\n```Shell\n#配置ssh免密\nssh-keygen -t rsa\nssh-copy-id 10.10.1.80\nssh-copy-id 10.10.1.81\nssh-copy-id 10.10.1.82\n\n#拷贝文件\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.1.80:/etc/ceph\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.1.81:/etc/ceph\nscp -r /etc/ceph/{ceph.conf,ceph.client.admin.keyring} 10.10.1.82:/etc/ceph\n```\n\n### 任务三：部署ceph-csi（使用rbd）\n\n1. 下载ceph-csi组件(k8s中的一个master节点)\n\n```Shell\n#下载文件\nwget https://github.com/ceph/ceph-csi/archive/refs/tags/v3.9.0.tar.gz\n#解压\nmv v3.9.0.tar.gz ceph-csi-v3.9.0.tar.gz\ntar -xzf ceph-csi-v3.9.0.tar.gz\n#进入目录\ncd  ceph-csi-3.9.0/deploy/rbd/kubernetes\nmkdir /data/cephfs/csi\n#拷进csi中，共6六个文件\ncp * /data/cephfs/csi\n```\n\n1. 拉取csi组件所需镜像\n\n```Shell\n#查看所需镜像\ngrep image csi-rbdplugin-provisioner.yaml\ngrep image csi-rbdplugin.yaml\n```\n\n在所有k8s节点上拉取所需的镜像\n\n```Shell\ncd /data/script\n./pull-images.sh registry.k8s.io/sig-storage/csi-provisioner:v3.5.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-resizer:v1.8.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2\ndocker pull  quay.io/cephcsi/cephcsi:v3.9.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-attacher:v4.3.0\n./pull-images.sh registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0\n```\n\n1. 创建命名空间`cephfs`\n\n```Shell\necho '\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    kubernetes.io/metadata.name: cephfs\n  name: cephfs\n  ' > ceph-namespace.yaml\n  \n #执行\n  kubectl apply -f ceph-namespace.yaml \n```\n\n1. 创建连接 ceph 集群的秘钥文件csi-rbd-secret.yaml\n\n```Shell\necho '\napiVersion: v1\nkind: Secret\nmetadata:\n  name: csi-rbd-secret\n  namespace: cephfs\nstringData:\n  adminID: admin \n  adminKey: AQANDD9lfWg2LBAAHY0mprdbuKFBPJDkE7/I5Q==\n  userID: admin  \n  userKey: AQANDD9lfWg2LBAAHY0mprdbuKFBPJDkE7/I5Q==\n  ' > csi-rbd-secret.yaml\n  \n   #执行\n    kubectl apply -f csi-rbd-secret.yaml\n```\n\n1. 创建ceph-config-map.yaml \n\n```Shell\necho '\napiVersion: v1\nkind: ConfigMap\ndata:\n  ceph.conf: |\n     [global]\n     fsid = 30912204-0c26-413f-8e00-6d55c9c0af03     # 生成的FSID\n     mon initial members =k8s-ceph-node0,k8s-ceph-node1,k8s-ceph-node2            # 主机名\n     mon host = 10.10.1.16,10.10.1.17,10.10.1.18                       # 对应的IP\n     public network = 10.10.1.0/24\n     auth cluster required = cephx\n     auth service required = cephx\n     auth client required = cephx\n     osd journal size = 1024\n     osd pool default size = 3\n     osd pool default min size = 2\n     osd pool default pg num = 333\n     osd pool default pgp num = 333\n     osd crush chooseleaf type = 1\n     [mon]\n     mon allow pool delete = true\n\n     [mds.k8s-ceph-node0]    \n     host = k8s-ceph-node0\n  keyring: |\nmetadata:\n  name: ceph-config\n  namespace: cephfs\n' > ceph-config-map.yaml\n\n #执行\n kubectl apply -f ceph-config-map.yaml  \n```\n\n1. 修改csi-config-map.yaml，配置连接 ceph 集群的信息\n\n```Shell\necho '\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ceph-csi-config\n  namespace: cephfs\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\ndata:\n  config.json: |-\n    [{\"clusterID\":\"30912204-0c26-413f-8e00-6d55c9c0af03\",\"monitors\":[\"10.10.1.16:6789\",\"10.10.1.17:6789\",\"10.10.1.18:6789\"]}]\n' > csi-config-map.yaml\n```\n\n1. 修改csi组件配置文件\n\n   1. 拷贝进`/data/cephfs/csi`目录中的所有yaml文件中的命名空间由`default`改为`cephfs`\n\n   2. ```Shell\n      cd /data/cephfs/csi\n      sed -i \"s/namespace: default/namespace: cephfs/g\" $(grep -rl \"namespace: default\" ./)\n      sed -i -e \"/^kind: ServiceAccount/{N;N;a\\  namespace: cephfs}\" $(egrep -rl \"^kind: ServiceAccount\" ./)\n      ```\n\n   3. 将`csi-rbdplugin-provisioner.yaml` 和 `csi-rbdplugin.yaml` 中的 kms 部分配置注释掉\n\n   4. >    \\# - name: KMS_CONFIGMAP_NAME\n      >\n      > ​            \\#   value: encryptionConfig\n      >\n      > \n      >\n      > \\#- name: ceph-csi-encryption-kms-config\n      >\n      > ​        \\#  configMap:\n      >\n      > ​        \\#    name: ceph-csi-encryption-kms-config\n      >\n      > \n\n```Shell\n #执行，安装csi组件\n kubectl apply -f csi-config-map.yaml\n kubectl apply -f csi-nodeplugin-rbac.yaml\n kubectl apply -f csidriver.yaml\n kubectl apply -f csi-provisioner-rbac.yaml\n kubectl apply -f csi-rbdplugin-provisioner.yaml\n kubectl apply -f csi-rbdplugin.yaml\n```\n\n### 任务四：创建storageclass\n\n```Shell\necho '\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    k8s.kuboard.cn/storageType: cephfs_provisioner\n  name: csi-rbd-sc\nprovisioner: rbd.csi.ceph.com\nparameters:\n  # fsName: cephfs  （cephfs方式使用）\n  clusterID: 30912204-0c26-413f-8e00-6d55c9c0af03 \n  pool: rbd-k8s \n  imageFeatures: layering\n  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/provisioner-secret-namespace: cephfs\n  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/controller-expand-secret-namespace: cephfs\n  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret\n  csi.storage.k8s.io/node-stage-secret-namespace: cephfs\n  csi.storage.k8s.io/fstype: xfs\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\nmountOptions:\n  - discard\n ' > storageclass.yaml\n \n #执行\n  kubectl apply -f storageclass.yaml\n```\n\n### 任务五：创建PVC\n\n```Shell\necho '\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: rbd-pvc \n  namespace: cephfs\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi \n  storageClassName: csi-rbd-sc\n  ' > pvc.yaml\n  \n#执行\n kubectl apply -f pvc.yaml\n#查看PVC是否创建成功\nkubectl get pvc -n cephfs\n#查看PV是否创建成功\nkubectl get pv -n cephfs\n\n#查看ceph集群中的cephfs_data存储池中是否创建了image\n rbd ls -p rbd-k8s\n```\n\n### 任务六：创建pod，进行测试验证\n\n```Shell\necho '\napiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-rbd-demo-pod\n  namespace: cephfs\nspec:\n  containers:\n    - name: web-server\n      image: nginx:latest\n      volumeMounts:\n        - name: mypvc\n          mountPath: /var/lib/www/html\n  volumes:\n    - name: mypvc\n      persistentVolumeClaim:\n        claimName: rbd-pvc \n        readOnly: false\n' > pod.yaml\n\n#执行\n kubectl apply -f pod.yaml\n #进入容器查看挂载信息\nkubectl exec -it csi-rbd-demo-pod -n cephfs -- bash\n lsblk -l|grep rbd\n```\n\n","tags":["docker","k8s","ceph","共享存储","ceph集群","容器云"],"categories":["容器化","k8s","存储系统","ceph"]},{"title":"使用cephadm方式部署ceph集群","url":"/yyg/89bdf9b5/","content":"## 任务目标\n\n1. 完成使用cephadm方式部署ceph集群\n\n## 任务平台\n\n1. 物理设备--\n2. 操作系统：openEuler 22.03 LTS SP2\n\n## 部署指南\n\n### 任务一：配置准备\n\n1. 重命名hostname\n\n```Shell\n# 将10.10.3.117的主机名改为future-ceph-node0\nhostnamectl set-hostname future-ceph-node0 && bash\n# 将10.10.3.118的主机名改为future-ceph-node1\nhostnamectl set-hostname future-ceph-node1 && bash\n# 将10.10.3.119的主机名改为future-ceph-node2\nhostnamectl set-hostname future-ceph-node2 && bash\n# 将10.10.3.120的主机名改为future-ceph-node3\nhostnamectl set-hostname future-ceph-node3 && bash\n```\n\n1. 安装前的配置修改\n\n```Shell\n# 关闭防火墙\nsystemctl stop firewalld\nsystemctl disable firewalld\nfirewall-cmd --state\n \n# selinux永久关闭\nsetenforce 0\n sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\ncat /etc/selinux/config\n\n\n# 添加hosts\ncat >> /etc/hosts << EOF\n10.10.3.117 future-ceph-node0\n10.10.3.118 future-ceph-node1\n10.10.3.119 future-ceph-node2\n10.10.3.120 future-ceph-node3\nEOF\n#查看\ncat /etc/hosts\n\n\n#同步时间\nyum install ntp -y\nsystemctl start ntpd\nsystemctl enable ntpd\nyum install chrony  -y\nsystemctl start chronyd\nsystemctl enable chronyd\n#修改配置，添加内容\necho \"\nserver 10.10.3.70 iburst\nallow 10.10.3.0/24\n\" >> /etc/chrony.conf\ntimedatectl set-ntp true\nsystemctl restart chronyd\ntimedatectl status\ndate\n```\n\n1. 添加SSH免密通信\n\n```Shell\nssh-keygen -t rsa\nssh-copy-id 10.10.3.118\nssh-copy-id 10.10.3.119\nssh-copy-id 10.10.3.120\n```\n\n### 任务二：安装docker\n\n1. 配置Docker CE的yum存储库。打开`docker-ce.repo`的文件，并将以下内容复制到文件中：\n\n```Shell\necho '\n[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/centos/7/$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/centos/gpg ' > /etc/yum.repos.d/docker-ce.repo\n```\n\n保存并退出文件。\n\n1. 安装Docker CE。运行以下命令来安装Docker CE：\n\n```Shell\n yum -y install docker-ce docker-ce-cli  containerd.io\n#启动docker并设置开机自启\nsystemctl start docker  \nsystemctl enable docker\n#查看版本\ndocker -v\ndocker compose version\n```\n\n1. Docker配置修改存储路径，配置修改为如下。\n\n```Shell\n#将配置写入daemon.json文件\necho '{\n  \"data-root\": \"/data/docker\"\n} ' > /etc/docker/daemon.json\n#查看\ncat /etc/docker/daemon.json\nsystemctl daemon-reload\nsystemctl restart docker\ndocker info\n```\n\n### 任务三：使用cephadm建设集群\n\n1. #### 安装ceph软件\n\n在所有主机上安装ceph软件\n\n```Shell\nyum install -y cephadm ceph-common\n#查看版本\nceph -v\n```\n\n1. #### 启用admin节点\n\n在10.10.3.117上启用集群\n\n```Shell\ncephadm bootstrap --mon-ip 10.10.3.117\n```\n\n访问dashboard修改默认密码\n\n> Fetching dashboard port number...\n>\n> Ceph Dashboard is now available at:\n>\n> \n>\n> ​             URL: https://future-ceph-node0:8443/\n>\n> ​            User: admin\n>\n> ​        Password: p4csdavtmr\n>\n> \n>\n> Enabling client.admin keyring and conf on hosts with \"admin\" label\n>\n> You can access the Ceph CLI with:\n>\n> \n>\n> ​        sudo /usr/sbin/cephadm shell --fsid 5f344e64-85e4-11ee-9181-0050569a1378 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring\n\n1. #### 添加节点\n\n```Shell\n#启用ceph命令\ncephadm shell\n#添加ceph的免密访问\nceph cephadm get-pub-key > ~/ceph.pub\nssh-copy-id -f -i ~/ceph.pub root@future-ceph-node1\nssh-copy-id -f -i ~/ceph.pub root@future-ceph-node2\nssh-copy-id -f -i ~/ceph.pub root@future-ceph-node3\n#添加ceph节点\nceph orch host add future-ceph-node1 10.10.3.118\nceph orch host add future-ceph-node2 10.10.3.119\nceph orch host add future-ceph-node3 10.10.3.120\n#查看节点\nceph orch host ls\n#添加_admin标签允许该节点运行ceph相关命令\nceph orch host label add future-ceph-node1  _admin\nceph orch host label add future-ceph-node2  _admin\nceph orch host label add future-ceph-node3  _admin\n\n#将配置文件拷贝到其他节点\nscp /etc/ceph/ceph.conf future-ceph-node1:/etc/ceph/\nscp /etc/ceph/ceph.conf future-ceph-node2:/etc/ceph/\nscp /etc/ceph/ceph.conf future-ceph-node3:/etc/ceph/\nscp /etc/ceph/ceph.client.admin.keyring future-ceph-node1:/etc/ceph/\nscp /etc/ceph/ceph.client.admin.keyring future-ceph-node2:/etc/ceph/\nscp /etc/ceph/ceph.client.admin.keyring future-ceph-node3:/etc/ceph/\n```\n\n1. #### 设置mon节点（监控）\n\n```Shell\nceph orch apply mon --placement=\"4 future-ceph-node0 future-ceph-node1 future-ceph-node2 future-ceph-node3\"\n#查看mon的详细信息\nceph mon dump\n```\n\n1. #### 设置mgr节点（管理）\n\n```Shell\n ceph orch apply mgr --placement=\"2 future-ceph-node0 future-ceph-node1 \"\n #查看详情\n  ceph orch ls\n  #打上标签\n  ceph orch host label add future-ceph-node0 master\n  ceph orch host label add future-ceph-node1 master\n```\n\n1. #### 添加osd（存储数据）\n\n```Shell\n#查看要挂载的磁盘\nlsblk\n#所有节点\nceph orch daemon add osd future-ceph-node0:/dev/sdb\nceph orch daemon add osd future-ceph-node1:/dev/sdb\nceph orch daemon add osd future-ceph-node2:/dev/sdb\nceph orch daemon add osd future-ceph-node3:/dev/sdb\n```\n\n1. #### 创建mds（元数据）\n\n```Shell\n#创建cephfs文件系统\nceph osd pool create cephfs_data\nceph osd pool create cephfs_metadata\nceph fs new cephfs cephfs_metadata cephfs_data\n#设置mds组件\n ceph orch apply mds cephfs --placement=\"4 future-ceph-node0 future-ceph-node1 future-ceph-node2 future-ceph-node3\"\n #查看mds状态\n ceph orch ps --daemon-type mds\n```\n\n1. #### 配置rgw（对象网关）\n\n```Shell\n#创建名为myorg的realm\nradosgw-admin realm create --rgw-realm=myorg \n#创建名为default的区域组(设置为主区域组)\nradosgw-admin zonegroup create --rgw-zonegroup=default --master\n#创建名为cn-east-1的区域（设置为主区域）\n radosgw-admin zone create --rgw-zonegroup=default --rgw-zone=cn-east-1 --master\n #为realm与区域配置radosgw\n ceph orch apply rgw myorg cn-east-1 --placement=\"4 future-ceph-node0 future-ceph-node1 future-ceph-node2 future-ceph-node3\"\n #验证\n ceph orch ps --daemon-type rgw\n ceph -s\n```\n\n1. #### 访问测试\n\n```Shell\n#ceph-Dashboard\nhttps://10.10.3.117:8443\n#Prometheus\nhttp://10.10.3.117:9095/\n#Grafana\nhttps://10.10.3.117:3000/\n```","tags":["ceph","共享存储","ceph集群","cephadm"],"categories":["存储系统","ceph"]},{"title":"Docker Swarm集群部署","url":"/yyg/71468332/","content":"**任务平台**\n\n3台虚拟机，一台作为manager 节点，另两台作为work节点。\n\n**部署指南**\n\n[TOC]\n\n### 安装docker\n\n```shell\n#下载并安装docker文件及依赖\nyum install -y yum-utils\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum install -y docker-ce docker-ce-cli containerd.io\n#启动docker并设置开机自启\nsystemctl start docker  \nsystemctl enable docker\n#查看版本\ndocker -v\n```\n\n### 配置防火墙开放端口\n\nTCP协议端口 2377 ：集群管理端口\nTCP协议端口 7946 ：节点之间通讯端口（不开放则会负载均衡失效）\nUDP协议端口 4789 ：overlay网络通讯端口\n\n```shell\nfirewall-cmd --zone=public --add-port=2377/tcp --permanent\nfirewall-cmd --zone=public --add-port=7946/tcp --permanent\nfirewall-cmd --zone=public --add-port=7946/udp --permanent\nfirewall-cmd --zone=public --add-port=4789/tcp --permanent\n#重载防火墙\nfirewall-cmd --reload\n#查看80端口是否开放\nfirewall-cmd --query-port=80/tcp\n#查看所有放行的端口\nfirewall-cmd --zone=public --list-ports\n```\n\n### 在 manager 节点创建 Swarm 集群\n\n```Shell\ndocker swarm init --advertise-addr=本机ip:2377 --listen-addr=本机ip:2377\n```\n\n> Swarm initialized: current node (608u180nsa654xbxdthdhl0f6) is now a manager.\n>\n> To add a worker to this swarm, run the following command:\n>\n> ​    `docker swarm join --token SWMTKN-1-13dv43qm3tdux7243z3c0najcetizjpgly1urd4uchtcooxe87-4eh15dbayxttxipm34s5tod6t 172.20.1.51:2377`\n>\n> To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n\n将执行后产生的命令（如上标红的），分别加入到其他主机中，执行后产生如下结果。即为成功加入。\n\n或者执行下面命令产生令牌，并且是定期轮换计划（推荐）\n\n```Shell\ndocker swarm join-token  --rotate worker\n```\n\n> This node joined a swarm as a worker.\n\n（扩展）加入管理节点的命令\n\n```Shell\ndocker swarm join-token --rotate manager\n```\n\n在manager节点上输入`docker node ls` 可查看所有节点。\n\n![image-20240323221029238](Docker Swarm集群部署/image-20240323221029238-1711203035847-1.png)\n\n### 创建用于swarm服务的自定义的overlay网络\n\n```Shell\n#方式一\n#--attachable选项表示该网络是可附加的，即其他容器可以连接到该网络  \ndocker network create -d overlay --attachable my-overlay\n\n#方式二\n# --subnet:子网 --gateway:网关\ndocker network create --driver overlay --subnet 10.0.9.0/24 --gateway 10.0.9.99 my-overlay\n  \n #查看docker网络\n docker network ls\n```\n\n### 测试跨主机容器通信\n\n1. 在三台主机上各部署一个容器，进行相互ping，结果如下图。\n\n```Shell\n#主机一\ndocker run -it --name master --net=my-overlay ubuntu:latest\n#主机二\ndocker run -it --name node1 --net=my-overlay ubuntu:latest\n#主机三\ndocker run -it --name node2 --net=my-overlay ubuntu:latest\n```\n\n2. 安装测试工具\n\n```Shell\napt-get update\n#安装ping工具\napt-get install inetutils-ping -y\n#安装ip查看工具\napt-get install net-tools -y\n```\n\n![image-20240323221151231](Docker Swarm集群部署/image-20240323221151231-1711203113839-3.png)\n","tags":["docker","docker Swarm"],"categories":["容器化","docker"]},{"title":"docker 安装 redis","url":"/yyg/6fffbcc4/","content":"首先输入命令`docker pull redis` ，将最新版redis安装到本地\n\n输入以下命令,后台启动redis(注：**该命令是一条命令**)\n\n其中`--name`后面的是redis容器的名称，可自行更改\n\n> docker run -p 6379:6379 --name mars1 --privileged=true\n>\n>  -v /app/redis/redis.conf:/etc/redis/redis.conf \n>\n> -v /app/redis/data:/data \n>\n> -d redis:latest redis-server /etc/redis/redis.conf\n\n此命令也可解决在删除redis容器后，而出现的数据丢失的问题。若要找回数据，则只需重新运行命令即可。\n\n![image-20230411164804210](docker 安装 redis/image-20230411164804210.png)\n\n运行`docker ps`命令，可查看到redis正在运行\n\n![image-20230411163752381](docker 安装 redis/image-20230411163752381.png)\n\n然后执行`docker exec -it mars1 /bin/bash`命令，redis成功运行\n\n![image-20230411180817939](docker 安装 redis/image-20230411180817939-1711202816360-2.png)","tags":["docker","redis"],"categories":["容器化","docker"]},{"title":"docker安装mysql","url":"/yyg/25ae9841/","content":"> `docker pull mysql` #拉取mysql镜像(最新版)\n\n若安装特定版本如6.0，则命令如下：\n\n`docker pull mysql:6.0`\n\n运行mysql，并分配3306端口，该命令如下：(**注：下面的是一条命令**)\n\n此命令中的`jiamian`为小编的liunx系统的用户名，用时需替换为自己的用户名\n\n`MYSQL_ROOT_PASSWORD`后填的是mysql的密码，可自行替换\n\n`--name`后填的mysql-1是运行的mysql容器的名称，也可自行替换\n\n> docker run -d -p 3306:3306 --privileged=true -v /jiamian/mysql/log:/var/log/mysql\n>\n> -v /jiamian/mysql/data:/var/lib/mysql -v /jiamian/mysql/conf:/etc/mysql/conf.d \n>\n> -e MYSQL_ROOT_PASSWORD=123456 --name mysql-1 mysql\n\n此命令也可解决在删除mysql容器后，而出现的数据库数据丢失的问题。若要找回数据，则只需重新运行命令即可。\n\n![image-20230410201124021](docker安装mysql/image-20230410201124021.png)\n\n运行`docker ps`命令，可查看到mysql正在运行\n\n![image-20230410201052873](docker安装mysql/image-20230410201052873.png)\n\n## 解决mysql插入中文乱码的情况\n\n然后输入命令`cd /jiamian/mysql/conf`。进入该目录后，输入命令`vim my.cnf`，新建my.cnf文件，\n\n并输入以下代码\n\n> [client]\n> default_character_set=utf8\n> [mysqld]\n> collation_server = utf8_general_ci\n> character_set_server = utf8\n\n保存退出后，输入 `docker restart mysql-1`，重启mysql。\n\n再输入`docker exec -it mysql-1 /bin/bash`进入mysql环境，此时汉字可正常显示\n\n![image-20230410202120798](docker安装mysql/image-20230410202120798.png)\n\n\n\n","tags":["docker","mysql"],"categories":["容器化","docker"]},{"title":"docker基础知识","url":"/yyg/7b36b89c/","content":"\n`docker search 镜像` #搜索要下载的镜像\n\n`docker pull 镜像` #将镜像拉到本地\n\n`docker images` #查看已拉取到本地的镜像\n\n`docker rmi 镜像名或ID` #删除镜像\n\n`docker rm 容器ID` #删除已停止容器\n\n> `docker rm -f 容器ID` #强制删除容器\n\n一次性删除多个容器\n\n> `docker rm -f $(docker ps -a -q)`\n>\n> `docker ps -a -q |xargs docker rm`\n\n删除虚悬镜像\n\n> `docker image ls -f dangling=true` #显示所有的虚悬镜像\n>\n> `docker image prune` #删除虚悬镜像\n\n`docker ps` #查看正在运行的容器\n\n> `docker ps -a` #列出所有正在运行和曾运行过的容器\n>\n> `docker ps -l` #显示最近创建的容器\n>\n> `docker ps -n` #显示最近创建的n个容器\n>\n> `docker ps -q` #只显示容器编号\n\n`docker stop  容器名或ID` #结束容器运行\n\n`docker kill  容器名或ID` #强制停止容器运行\n\n`docker start 容器名或ID` #启动已停止的容器\n\n`docker restart 容器名或ID` #重启容器\n\n`docker run -it 镜像名或ID` #启动镜像(-it指交互式启动)\n\n> `docker run -d 容器名或ID` #容器后台运行\n>\n> `docker run -it -p 端口号:端口号 容器名或ID` #以特定端口号运行容器\n>\n> `docker run -it -P 容器名或ID` #随机分配端口号运行容器\n>\n> 启动镜像后，按下`ctrl+p+q`，容器后台运行 \n\n重新进入后台运行的容器\n\n> `docker exec -it  容器ID` #在容器中打开新的终端，并且可以启动新的进程，用exit退出，不会导致容器的停止。(推荐使用)\n>\n> `docker attach 容器ID`  #直接进入容器启动命令的终端，不会启动新的进程，用exit退出，会导致容器的停止。\n\n`docker inspect 容器ID`#显示容器内的详细信息\n\n`docker logs  容器ID` #查看容器日志\n\n`docker top  容器ID` 查看容器内进程\n\n`docker cp  容器ID:容器内路径 目的主机路径` #将文件从容器拷贝到主机\n\n容器的导入导出\n\n> `docker export  容器ID >文件名.tar` #将容器及内容导出为tar包\n>\n> `cat 文件名.tar | docker import -镜像用户/镜像名:镜像版本号` #从tar包中创建一个新文件系统再导入镜像\n\n`docker run -it --privileged=true -v /宿主机目录:/容器内目录 镜像名`#宿主机与容器之间映射添加容器卷\n\n`docker run -it --privileged=true -v /宿主机目录:/容器内目录:rw 镜像名`#宿主机与容器之间映射添加容器卷,容器内文件可读可写\n\n`docker run -it --privileged=true -v /宿主机目录:/容器内目录:ro 镜像名`#宿主机与容器之间映射添加容器卷,容器内文件只可读\n\n`docker run -it --privileged=true -volumes-from 父类 -name 容器2名称 镜像名` #容器2继承容器1的卷规则\n\n![image-20240323215245387](docker基础知识/image-20240323215245387.png)","tags":["docker","docker基础"],"categories":["容器化","梳理总结","docker"]},{"title":"实现Docker 容器跨主机通讯","url":"/yyg/60ed202a/","content":"\n[TOC]\n\n### 方案一：docker swarm集群\n\nhttps://blog.csdn.net/AMCUL/article/details/132913280\n\n### 方案二：直接路由法\n\n在部署指南中提到的方法的实现分固定网段、路由持久化、IP转发三部分组成。\n\n#### 方法1：\n\n##### 修改并固定docker0的默认网段\n\n1. 修改docker0网段ip，在主机的/etc/docker/daemon.json文件中，加入如下内容：\n\n```Shell\n#打开daemon.json文件\nvi /etc/docker/daemon.json\n\n#主机一中添加的内容\n#172.16.200.1 为主机一的docker网段ip\n{\n \"bip\": \"172.16.200.1/24\"\n}\n\n#主机二中添加的内容\n#172.16.210.1 为主机二的docker网段ip\n{\n \"bip\": \"172.16.210.1/24\"\n}\n\n#重启docker服务\nsystemctl restart docker \n```\n\n1.  添加路由规则，主机间互相添加\n\n```Shell\n#主机1上添加路由规则（添加主机2的ip和网段）\nip route add 172.16.210.0/24 via 172.20.1.52\n\n#主机2上添加路由规则（添加主机1的ip和网段）\nip route add 172.16.200.0/24 via 172.20.1.51\n\n#方式一：\n#开启路由转发\niptables -P FORWARD ACCEPT\n\n#方式二：\n#配置iptables规则（本机的网段）\n#主机一\niptables -t nat -F POSTROUTING\niptables -t nat -A POSTROUTING -s 172.16.200.0/24 ! -d 172.16.0.0/16 -j MASQUERADE\n\n#主机二\niptables -t nat -F POSTROUTING\niptables -t nat -A POSTROUTING -s 172.16.210.0/24 ! -d 172.16.0.0/16 -j MASQUERADE\n```\n\n1. 启动容器并测试\n\n```Shell\n# 主机1上启动ubuntu容器\ndocker run -it --name node1 ubuntu:latest /bin/bash\n\n# 主机2上启动ubuntu容器\ndocker run -it --name node2 ubuntu:latest /bin/bash\n\n#安装测试工具\napt-get update\n#安装ping工具\napt-get install inetutils-ping -y\n#安装ip查看工具\napt-get install net-tools -y\n```\n\n##### 路由持久化（防止主机重启路由丢失）\n\n###### 法一：（推荐）\n\n在`/etc/sysconfig/network-scripts/`目录下创建名为`route-enp0s3`(enp0s3为主机网卡的名字，以实际为准)的文件 \n\n推荐此方法，注意创建时`route-`后跟的是本机主机的网卡名\n\n```Shell\nvi /etc/sysconfig/network-scripts/route-enp0s3\n\n#在此文件添加如下格式的内容\n#主机一中添加\n172.16.230.0/24 via 172.20.1.52\n#主机二中添加\n172.16.220.0/24 via 172.20.1.51\n\n#重启网络验证\nservice network restart\n\n\n#查看路由表中是否有刚才添加的路由\nip route show | column -t\n```\n\n###### 法二：\n\n在`/etc/sysconfig/static-routes` 文件里添加如下内容 (没有static-routes需手动建立)\n\n```Shell\n#主机一中添加\nany net 172.16.230.0/24 via 172.20.1.52\n#主机二中添加\nany net 172.16.220.0/24 via 172.20.1.51\n\n#重启网络服务\nsystemctl restart network\n\n#查看路由表中是否有刚才添加的路由\nip route show | column -t\n```\n\n##### 开启IP转发\n\n```Shell\n#永久开启\nvi /etc/sysctl.conf\n#修改\nnet.ipv4.ip_forward=1\n```\n\n#### 方法2：（适用于使用固定ip的容器）\n\n##### 创建docker网络\n\n1. docker桥接类型网络创建\n\n分别对两台服务器中的docker创建名字为test的网络，指定子网范围为 172.16.220.0/24 、172.16.230.0/24\n\n```Shell\n#主机一\ndocker network create test --driver bridge --ipam-driver default --subnet 172.16.220.0/24\n\n#主机二\ndocker network create test --driver bridge --ipam-driver default --subnet 172.16.230.0/24\n#查看网络\ndocker network ls\n```\n\n1. 设置静态路由\n\n```Shell\n#主机1上添加路由规则（添加主机2的ip和网段）\nip route add 172.16.230.0/24 via 172.20.1.52\n\n#主机2上添加路由规则（添加主机1的ip和网段）\nip route add 172.16.220.0/24 via 172.20.1.51\n\n#开启路由转发\niptables -P FORWARD ACCEPT\n```\n\n1. 启动容器并测试\n\n```Shell\n# 主机1上启动ubuntu容器(加上创建的桥接网络)\ndocker run -it --name node1 --net=test ubuntu:latest /bin/bash\n\n# 主机2上启动ubuntu容器(加上创建的桥接网络)\ndocker run -it --name node2 --net=test ubuntu:latest /bin/bash\n\n#安装测试工具\napt-get update\n#安装ping工具\napt-get install inetutils-ping -y\n#安装ip查看工具\napt-get install net-tools -y\n```\n\n##### 路由持久化（防止主机重启路由丢失）\n\n使用**nmtui图形化界面添加**\n\n```Shell\n #下载nmtui\n yum install net-tools -y\n #显示nmtui图形化界面\n nmtui\n```\n\n在主机一网卡中配置主机二的docker网络的ip段和ip，在主机二网卡中配置主机一的docker网络的ip段和ip\n\n```Shell\n#主机一中添加\n172.16.230.0/24  172.20.1.52\n#主机二中添加\n172.16.220.0/24  172.20.1.51\n```\n\n配置完成会在/etc/sysconfig/network-scripts文件夹下看到\n\n![image-20240323221412716](实现Docker 容器跨主机通讯/image-20240323221412716-1711203257505-1.png)\n\n![image-20240323221442754](实现Docker 容器跨主机通讯/image-20240323221442754-1711203286475-3.png)\n\n启动对应主机的新创建的网卡（网络重启或系统重启后会失效）\n\n```Shell\n#主机一\nnmcli c up br-aed92de88760\n#主机二\nnmcli c up br-0ab3914edle2\n```\n\n要使新网卡保持启动则需将对应新创建的网卡文件中的`ONBOOT=no`改为`ONBOOT=yes`\n\n```Shell\n#主机一\nvi ifcfg-br-aed92de88760\n#ONBOOT=no改为ONBOOT=yes\nONBOOT=yes\n#主机二\nvi ifcfg-br-0ab3914edle2\n#ONBOOT=no改为ONBOOT=yes\nONBOOT=yes\n\n#重启网络服务\nsystemctl restart network\n```\n\n![image-20240323221537519](实现Docker 容器跨主机通讯/image-20240323221537519-1711203339288-5.png)\n\n##### 开启IP转发\n\n```Shell\n#永久开启\nvi /etc/sysctl.conf\n#修改\nnet.ipv4.ip_forward=1\n```\n\n此时容器不受网络重启影响，可实现跨主机通讯。\n\n但系统重启后，所创建的docker网络和容器均失效。\n\n需要重新创建docker网络和容器，此后docker网络和容器均不受系统重启和网络重启的影响，成功实现docker容器的跨主机通讯。\n\n![image-20240323221547069](实现Docker 容器跨主机通讯/image-20240323221547069-1711203348713-7.png)","tags":["docker","容器通信"],"categories":["容器化","梳理总结","docker"]}]